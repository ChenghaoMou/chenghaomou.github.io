---
aliases:
  - Simplifying BERT-based Models to Increase Efficiency, Capacity
doc_type: hypothesis-highlights
linter-yaml-title-alias: Simplifying BERT-based Models to Increase Efficiency, Capacity
order: -1
tags: [processed]
title: Simplifying BERT-based Models to Increase Efficiency, Capacity
url: >-
---

# Simplifying BERT-based Models to Increase Efficiency, Capacity

## Metadata

- Author: [amazon.science]()
- Title: Simplifying BERT-based models to increase efficiency, capacity
- Reference: https://www.amazon.science/blog/simplifying-bert-based-models-to-increase-efficiency-capacity
- Category: #article

## Page Notes

## Highlights

- To make BERT-based models more efficient, we progressively eliminate redundant individual-word embeddings in intermediate layers of the network, while trying to minimize the effect on the complete-sentence embeddings. — [Updated on 2022-07-02 15:55:45](https://hyp.is/G6CoOvpaEeylUHO3HvP_Rg/www.amazon.science/blog/simplifying-bert-based-models-to-increase-efficiency-capacity) — Group: #Personal

- The basic idea is that, in each of the network’s encoders, we preserve the embedding of the CLS token but select a representative subset — a core set — of the other tokens’ embeddings. — [Updated on 2022-07-02 16:00:05](https://hyp.is/tjCrUvpaEeylcr-vGPf9cw/www.amazon.science/blog/simplifying-bert-based-models-to-increase-efficiency-capacity) — Group: #Personal

