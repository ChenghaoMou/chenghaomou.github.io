---
aliases:
  - On the Paradox of Learning to Reason From Data
linter-yaml-title-alias: On the Paradox of Learning to Reason From Data
order: -20220605111631
tags: [natural_language_processing, natural_language_processing/reasoning, machine_learning/spurious_features]
title: On the Paradox of Learning to Reason From Data
---

# On the Paradox of Learning to Reason From Data

(6/5/2022, 11:16:31 AM)

“instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems” ([Zhang et al., 2022, p. 1](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=1&annotation=ZLE9HQVL))

“the models attaining near-perfect accuracy on one data distribution do not generalize to other distributions within the same problem space.” ([Zhang et al., 2022, p. 2](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=2&annotation=3NJ6HTZ4))

“In fact, the model has learned to use statistical features in logical reasoning problems to make predictions rather than to emulate the correct reasoning function.” ([Zhang et al., 2022, p. 2](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=2&annotation=KKZ5SXVE))

“statistical features inherently exist in reasoning problems and are not specific to certain data distributions.” ([Zhang et al., 2022, p. 2](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=2&annotation=2WC6EFS3))

“moreover, we argue that there are potentially countless statistical features and it is computationally expensive to jointly remove them from training distributions.” ([Zhang et al., 2022, p. 2](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=2&annotation=ZR9XAWNS))

“However, for logical reasoning, even though numerous statistical features inherently exist, models should not be utilizing them to make predictions.” ([Zhang et al., 2022, p. 2](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=2&annotation=UHCK5DSB))

“When BERT is trained on data with statistical features, it tends to make predictions based on such features rather than learning to emulate the correct reasoning function; thus, BERT fails to generalize to the whole problem space. However, unlike the shallow shortcuts found in other typical NLP tasks, such statistical features can be countless and extremely complicated, and thus very difficult to be removed from training data.” ([Zhang et al., 2022, p. 5](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=5&annotation=SNP7MN4R))

“(1) when statistical features are presented in training distributions, BERT tends to utilize them to make predictions; (2) after removing one statistical feature from training data, the model generalizes better.” ([Zhang et al., 2022, p. 7](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=7&annotation=NUB9Z68R))

“it is computationally infeasible to jointly remove multiple statistical features.” ([Zhang et al., 2022, p. 8](zotero://select/library/items/AKZXH8LK)) ([pdf](zotero://open-pdf/library/items/NC8TRG2L?page=8&annotation=SH6GIKN7))

***
- @zhang_2022