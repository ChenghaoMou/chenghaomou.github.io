@article{lhoest_2021,
  title = {Datasets: {{A Community Library}} for {{Natural Language Processing}}},
  shorttitle = {Datasets},
  author = {Lhoest, Quentin and {del Moral}, Albert Villanova and Jernite, Yacine and Thakur, Abhishek and {von Platen}, Patrick and Patil, Suraj and Chaumond, Julien and Drame, Mariama and Plu, Julien and Tunstall, Lewis and Davison, Joe and {\v S}a{\v s}ko, Mario and Chhablani, Gunjan and Malik, Bhavitvya and Brandeis, Simon and Scao, Teven Le and Sanh, Victor and Xu, Canwen and Patry, Nicolas and {McMillan-Major}, Angelina and Schmid, Philipp and Gugger, Sylvain and Delangue, Cl{\'e}ment and Matussi{\`e}re, Th{\'e}o and Debut, Lysandre and Bekman, Stas and Cistac, Pierric and Goehringer, Thibault and Mustar, Victor and Lagunas, Fran{\c c}ois and Rush, Alexander M. and Wolf, Thomas},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.02846 [cs]},
  eprint = {2109.02846},
  primaryclass = {cs},
  urldate = {2021-09-08},
  abstract = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,datasets,huggingface,nlp},
  annotation = {31 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{rae_2021,
  title = {Scaling {{Language Models}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Training Gopher}}},
  shorttitle = {Scaling {{Language Models}}},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and {d'Autume}, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.11446 [cs]},
  eprint = {2112.11446},
  primaryclass = {cs},
  urldate = {2021-12-24},
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,data,lm,nlp,scaling},
  annotation = {38 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{lin_2021a,
  title = {Few-Shot {{Learning}} with {{Multilingual Language Models}}},
  author = {Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and Pasunuru, Ramakanth and Shleifer, Sam and Koura, Punit Singh and Chaudhary, Vishrav and O'Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Kozareva, Zornitsa and Diab, Mona and Stoyanov, Veselin and Li, Xian},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.10668 [cs]},
  eprint = {2112.10668},
  primaryclass = {cs},
  urldate = {2021-12-24},
  abstract = {Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4\% absolute accuracy improvement in 0-shot settings and +9.4\% in 4-shot settings) and natural language inference (+5.4\% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model succeeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,data,few-shot learning,lm,multilingual,nlp},
  annotation = {7 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{du_2021,
  title = {{{GLaM}}: {{Efficient Scaling}} of {{Language Models}} with {{Mixture-of-Experts}}},
  shorttitle = {{{GLaM}}},
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and {Meier-Hellstern}, Kathy and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.06905 [cs]},
  eprint = {2112.06905},
  primaryclass = {cs},
  urldate = {2021-12-16},
  abstract = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,moe,nlp,scaling},
  annotation = {20 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{gao_2021,
  title = {An {{Empirical Exploration}} in {{Quality Filtering}} of {{Text Data}}},
  author = {Gao, Leo},
  year = {2021},
  month = oct,
  journal = {arXiv:2109.00698 [cs]},
  eprint = {2109.00698},
  primaryclass = {cs},
  urldate = {2021-11-28},
  abstract = {While conventional wisdom suggests that more aggressively filtering data from low-quality sources like Common Crawl always monotonically improves the quality of training data, we find that aggressive filtering can in fact lead to a decrease in model quality on a wide array of downstream tasks for a GPT-like language model. We speculate that this is because optimizing sufficiently strongly for a proxy metric harms performance on the true objective, suggesting a need for more robust filtering objectives when attempting to filter more aggressively. We hope this work leads to detailed analysis of the effects of dataset filtering design choices on downstream model performance in future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,data,filtering},
  annotation = {2 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{liu_,
  title = {Towards {{Efficient NLP}}: {{A Standard Evaluation}} and {{A Strong Baseline}}},
  author = {Liu, Xiangyang and Sun, Tianxiang and He, Junliang and Wu, Lingling and Zhang, Xinyu and Jiang, Hao and Cao, Zhao and Huang, Xuanjing and Qiu, Xipeng},
  pages = {15},
  abstract = {Supersized pre-trained language models have pushed the accuracy of various NLP tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, most works are pursuing improvement on other dimensions such as efficiency, leading to "Pareto SOTA". Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and public leaderboard for efficient NLP models. ELUE is dedicated to depict the Pareto Front for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also pre-train and release a strong baseline, ElasticBERT, whose elasticity is both static and dynamic. ElasticBERT is static in that it allows reducing model layers on demand. ElasticBERT is dynamic in that it selectively executes parts of model layers conditioned on the input. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. The ELUE benchmark is publicly available at http://eluebenchmark.fastnlp.top/1.},
  langid = {english},
  keywords = {benchmark,efficiency,nlp,nosource}
}

@article{jiang_2021,
  title = {Delphi: {{Towards Machine Ethics}} and {{Norms}}},
  shorttitle = {Delphi},
  author = {Jiang, Liwei and Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Forbes, Maxwell and Borchardt, Jon and Liang, Jenny and Etzioni, Oren and Sap, Maarten and Choi, Yejin},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07574 [cs]},
  eprint = {2110.07574},
  primaryclass = {cs},
  urldate = {2021-10-16},
  abstract = {What would it take to teach a machine to behave ethically? While broad ethical rules may seem straightforward to state ("thou shalt not kill"), applying such rules to real-world situations is far more complex. For example, while "helping a friend" is generally a good thing to do, "helping a friend spread fake news" is not. We identify four underlying challenges towards machine ethics and norms: (1) an understanding of moral precepts and social norms; (2) the ability to perceive real-world situations visually or by reading natural language descriptions; (3) commonsense reasoning to anticipate the outcome of alternative actions in different contexts; (4) most importantly, the ability to make ethical judgments given the interplay between competing values and their grounding in different contexts (e.g., the right to freedom of expression vs. preventing the spread of fake news). Our paper begins to address these questions within the deep learning paradigm. Our prototype model, Delphi, demonstrates strong promise of language-based commonsense moral reasoning, with up to 92.1\% accuracy vetted by humans. This is in stark contrast to the zero-shot performance of GPT-3 of 52.3\%, which suggests that massive scale alone does not endow pre-trained neural language models with human values. Thus, we present Commonsense Norm Bank, a moral textbook customized for machines, which compiles 1.7M examples of people's ethical judgments on a broad spectrum of everyday situations. In addition to the new resources and baseline performances for future research, our study provides new insights that lead to several important open research questions: differentiating between universal human values and personal values, modeling different moral frameworks, and explainable, consistent approaches to machine ethics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,nlp,social norm},
  annotation = {19 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{liu_2021,
  title = {P-{{Tuning}} v2: {{Prompt Tuning Can Be Comparable}} to {{Fine-tuning Universally Across Scales}} and {{Tasks}}},
  shorttitle = {P-{{Tuning}} V2},
  author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07602 [cs]},
  eprint = {2110.07602},
  primaryclass = {cs},
  urldate = {2021-10-16},
  abstract = {Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work and our results reveal that existing methods of prompt tuning do not perform well for normal-sized pre-trained models and for hard sequence tasks, indicating lack of universality. We present a novel empirical finding that properly-optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks, where it matches the performance of fine-tuning while having only 0.1{\textbackslash}\%-3{\textbackslash}\% tuned parameters. Our method P-Tuning v2 is not a new method but a version of prefix-tuning {\textbackslash}cite\{li2021prefix\} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative for fine-tuning and a strong baseline for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,nlp,prompt tuning},
  annotation = {19 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{biderman_2021,
  title = {Pitfalls in {{Machine Learning Research}}: {{Reexamining}} the {{Development Cycle}}},
  shorttitle = {Pitfalls in {{Machine Learning Research}}},
  author = {Biderman, Stella and Scheirer, Walter J.},
  year = {2021},
  month = aug,
  journal = {arXiv:2011.02832 [cs, stat]},
  eprint = {2011.02832},
  primaryclass = {cs, stat},
  urldate = {2021-08-27},
  abstract = {Machine learning has the potential to fuel further advances in data science, but it is greatly hindered by an ad hoc design process, poor data hygiene, and a lack of statistical rigor in model evaluation. Recently, these issues have begun to attract more attention as they have caused public and embarrassing issues in research and development. Drawing from our experience as machine learning researchers, we follow the machine learning process from algorithm design to data collection to model evaluation, drawing attention to common pitfalls and providing practical recommendations for improvements. At each step, case studies are introduced to highlight how these pitfalls occur in practice, and where things could be improved.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,development,machine learning,pitafalls,pitfalls,Statistics - Machine Learning,Statistics - Methodology},
  annotation = {5 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{wang_2021a,
  title = {{{LayoutReader}}: {{Pre-training}} of {{Text}} and {{Layout}} for {{Reading Order Detection}}},
  shorttitle = {{{LayoutReader}}},
  author = {Wang, Zilong and Xu, Yiheng and Cui, Lei and Shang, Jingbo and Wei, Furu},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.11591 [cs]},
  eprint = {2108.11591},
  primaryclass = {cs},
  urldate = {2021-10-10},
  abstract = {Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that contains reading order, text, and layout information for 500,000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically, our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. We will release the dataset and model at {\textbackslash}url\{https://aka.ms/layoutreader\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,document,layout,nlp},
  annotation = {3 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{koreeda_2021,
  title = {{{ContractNLI}}: {{A Dataset}} for {{Document-level Natural Language Inference}} for {{Contracts}}},
  shorttitle = {{{ContractNLI}}},
  author = {Koreeda, Yuta and Manning, Christopher D.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.01799 [cs]},
  eprint = {2110.01799},
  primaryclass = {cs},
  urldate = {2021-10-06},
  abstract = {Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose "document-level natural language inference (NLI) for contracts", a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as "Some obligations of Agreement may survive termination.") and a contract, and it is asked to classify whether each hypothesis is "entailed by", "contradicting to" or "not mentioned by" (neutral to) the contract as well as identifying "evidence" for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (1) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (2) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,contracts,dataset,document,nlp},
  annotation = {3 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{hartman_2021,
  title = {Scaling {{TensorFlow}} to 300 Million Predictions per Second},
  author = {Hartman, Jan and Kopi{\v c}, Davorin},
  year = {2021},
  month = sep,
  journal = {Fifteenth ACM Conference on Recommender Systems},
  eprint = {2109.09541},
  pages = {595--597},
  doi = {10.1145/3460231.3474605},
  urldate = {2021-09-25},
  abstract = {We present the process of transitioning machine learning models to the TensorFlow framework at a large scale in an online advertising ecosystem. In this talk we address the key challenges we faced and describe how we successfully tackled them; notably, implementing the models in TF and serving them efficiently with low latency using various optimization techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Performance,nlp,scaling,transformer},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-05]\\
0 citations (Semantic Scholar/DOI) [2022-04-05]}
}

@article{li_2021,
  title = {{{TrOCR}}: {{Transformer-based Optical Character Recognition}} with {{Pre-trained Models}}},
  shorttitle = {{{TrOCR}}},
  author = {Li, Minghao and Lv, Tengchao and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
  year = {2021},
  month = sep,
  urldate = {2021-09-23},
  abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches for text recognition are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on both printed and handwritten text recognition tasks. The code and models will be publicly available at https://aka.ms/TrOCR.},
  langid = {english},
  keywords = {nlp,ocr,plm,transformer},
  annotation = {5 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{gao_2020,
  title = {The {{Pile}}: {{An 800GB Dataset}} of {{Diverse Text}} for {{Language Modeling}}},
  shorttitle = {The {{Pile}}},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = {2020},
  month = dec,
  journal = {arXiv:2101.00027 [cs]},
  eprint = {2101.00027},
  primaryclass = {cs},
  urldate = {2021-09-11},
  abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,data,nlp},
  annotation = {78 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{kaplan_2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08361 [cs, stat]},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  urldate = {2021-09-11},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,lm,nlp,scale,Statistics - Machine Learning},
  annotation = {383 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{fedus_2021,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03961 [cs]},
  eprint = {2101.03961},
  primaryclass = {cs},
  urldate = {2021-09-10},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,moe,nosource,transformer},
  annotation = {277 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/CNTIHHGV/Fedus_et_al_(2021)_Switch_Transformers.pdf}
}

@article{wei_2021,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.01652 [cs]},
  eprint = {2109.01652},
  primaryclass = {cs},
  urldate = {2021-09-06},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially boosts zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of tasks and model scale are key components to the success of instruction tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,nlp,nosource,zero-short,zero-shot},
  annotation = {66 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/84P926GC/Wei_et_al_(2021)_Finetuned_Language_Models_Are_Zero-Shot_Learners.pdf}
}

@article{banino_2021,
  title = {{{PonderNet}}: {{Learning}} to {{Ponder}}},
  shorttitle = {{{PonderNet}}},
  author = {Banino, Andrea and Balaguer, Jan and Blundell, Charles},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.05407 [cs]},
  eprint = {2107.05407},
  primaryclass = {cs},
  urldate = {2021-08-14},
  abstract = {In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Machine Learning,machine learning,training},
  annotation = {11 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/VFVDIQAN/Banino_et_al_(2021)_PonderNet.pdf}
}

@article{hong_2020,
  title = {{{BROS}}: {{A Pre-trained Language Model}} for {{Understanding Texts}} in {{Document}}},
  shorttitle = {{{BROS}}},
  author = {Hong, Teakgyu and Kim, DongHyun and Ji, Mingi and Hwang, Wonseok and Nam, Daehyun and Park, Sungrae},
  year = {2020},
  month = sep,
  urldate = {2021-08-11},
  abstract = {Understanding document from their visual snapshots is an emerging and challenging problem that requires both advanced computer vision and NLP methods. Although the recent advance in OCR enables the...},
  langid = {english},
  keywords = {document,nlp}
}

@article{pinter_2021,
  title = {Learning to {{Look Inside}}: {{Augmenting Token-Based Encoders}} with {{Character-Level Information}}},
  shorttitle = {Learning to {{Look Inside}}},
  author = {Pinter, Yuval and Stent, Amanda and Dredze, Mark and Eisenstein, Jacob},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.00391 [cs]},
  eprint = {2108.00391},
  primaryclass = {cs},
  urldate = {2021-08-03},
  abstract = {Commonly-used transformer language models depend on a tokenization schema which sets an unchangeable subword vocabulary prior to pre-training, destined to be applied to all downstream tasks regardless of domain shift, novel word formations, or other sources of vocabulary mismatch. Recent work has shown that "token-free" models can be trained directly on characters or bytes, but training these models from scratch requires substantial computational resources, and this implies discarding the many domain-specific models that were trained on tokens. In this paper, we present XRayEmb, a method for retrofitting existing token-based models with character-level information. XRayEmb is composed of a character-level "encoder" that computes vector representations of character sequences, and a generative component that decodes from the internal representation to a character sequence. We show that incorporating XRayEmb's learned vectors into sequences of pre-trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on non-standard English text.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,nosource,token},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/JLUTX6YN/Pinter_et_al_(2021)_Learning_to_Look_Inside.pdf}
}

@article{jaegle_2021,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  shorttitle = {Perceiver {{IO}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and H{\'e}naff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Jo{\~a}o},
  year = {2021},
  month = aug,
  journal = {arXiv:2107.14795 [cs, eess]},
  eprint = {2107.14795},
  primaryclass = {cs, eess},
  urldate = {2021-08-03},
  abstract = {The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.},
  archiveprefix = {arXiv},
  keywords = {bert,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,input,nosource},
  annotation = {34 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/E83UCN2U/Jaegle_et_al_(2021)_Perceiver_IO.pdf}
}

@article{salesky_2021,
  title = {Robust {{Open-Vocabulary Translation}} from {{Visual Text Representations}}},
  author = {Salesky, Elizabeth and Etter, David and Post, Matt},
  year = {2021},
  journal = {arXiv},
  abstract = {Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an 'open-vocabulary.' This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text. We show that models using visual text representations approach or match performance of text baselines on clean TED datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German--English task where subword models degrade to 1.9.},
  keywords = {embedding,nlp},
  annotation = {7 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{clark_2021,
  title = {{{CANINE}}: {{Pre-training}} an {{Efficient Tokenization-Free Encoder}} for {{Language Representation}}},
  author = {Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
  year = {2021},
  journal = {arXiv},
  abstract = {Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences, without explicit tokenization or vocabulary, and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28\% fewer model parameters.},
  keywords = {nosource}
}

@article{wang_2021,
  title = {Dodrio: {{Exploring Transformer Models}} with {{Interactive Visualization}}},
  author = {Wang, Zijie J and Turko, Robert and Chau, Duen Horng},
  year = {2021},
  journal = {arXiv},
  abstract = {Why do large pre-trained transformer-based models perform so well across a wide variety of NLP tasks? Recent research suggests the key may lie in multi-headed attention mechanism's ability to learn and represent linguistic information. Understanding how these models represent both syntactic and semantic knowledge is vital to investigate why they succeed and fail, what they have learned, and how they can improve. We present Dodrio, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge. Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text. To facilitate the visual comparison of attention weights and linguistic knowledge, Dodrio applies different graph visualization techniques to represent attention weights scalable to longer input text. Case studies highlight how Dodrio provides insights into understanding the attention mechanism in transformer-based models. Dodrio is available at https://poloclub.github.io/dodrio/.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,nlp,transformer,visualization},
  file = {/Users/chenghao/Zotero/storage/SUNNIEKU/Wang_et_al_(2021)_Dodrio.pdf}
}

@article{xue_2021,
  title = {{{ByT5}}: {{Towards}} a Token-Free Future with Pre-Trained Byte-to-Byte Models},
  author = {Xue, Linting and Barua, Aditya and Constant, Noah and {Al-Rfou}, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  year = {2021},
  journal = {arXiv},
  abstract = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
  keywords = {embedding,nosource,pre-trained,tokenization}
}

@article{aghajanyan_2021a,
  title = {Muppet: {{Massive Multi-task Representations}} with {{Pre-Finetuning}}},
  author = {Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
  year = {2021},
  journal = {arXiv},
  abstract = {We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.{\textbackslash}textbackslashtextasciitildeRoBERTa) and generation models (e.g.{\textbackslash}textbackslashtextasciitildeBART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.},
  keywords = {nosource}
}

@article{appalaraju_2021,
  title = {{{DocFormer}}: {{End-to-End Transformer}} for {{Document Understanding}}},
  author = {Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R},
  year = {2021},
  journal = {arXiv},
  abstract = {We present DocFormer -- a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).},
  keywords = {document,nlp,nosource,transformer}
}

@article{yin_2021,
  title = {{{DocNLI}}: {{A Large-scale Dataset}} for {{Document-level Natural Language Inference}}},
  author = {Yin, Wenpeng and Radev, Dragomir and Xiong, Caiming},
  year = {2021},
  journal = {arXiv},
  abstract = {Natural language inference (NLI) is formulated as a unified framework for solving various NLP problems such as relation extraction, question answering, summarization, etc. It has been studied intensively in the past few years thanks to the availability of large-scale labeled datasets. However, most existing studies focus on merely sentence-level inference, which limits the scope of NLI's application in downstream NLP problems. This work presents DocNLI -- a newly-constructed large-scale dataset for document-level NLI. DocNLI is transformed from a broad range of NLP problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, DocNLI has pretty limited artifacts which unfortunately widely exist in some popular sentence-level NLI datasets. Our experiments demonstrate that, even without fine-tuning, a model pretrained on DocNLI shows promising performance on popular sentence-level benchmarks, and generalizes well to out-of-domain NLP tasks that rely on inference at document granularity. Task-specific fine-tuning can bring further improvements. Data, code, and pretrained models can be found at https://github.com/salesforce/DocNLI.}
}

@article{kim_2021,
  title = {Learned {{Token Pruning}} for {{Transformers}}},
  author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Hassoun, Joseph and Keutzer, Kurt},
  year = {2021},
  journal = {arXiv},
  abstract = {A major challenge in deploying transformer models is their prohibitive inference cost, which quadratically scales with the input sequence length. This makes it especially difficult to use transformers for processing long sequences. To address this, we present a novel Learned Token Pruning (LTP) method that reduces redundant tokens as the data passes through the different layers of the transformer. In particular, LTP prunes tokens with an attention score below a threshold value, which is learned during training. Importantly, our threshold based method avoids algorithmically expensive operations such as top-k token selection which are used in prior token pruning methods, and also leads to structured pruning. We extensively test the performance of our approach on multiple GLUE tasks and show that our learned threshold based method consistently outperforms the prior state-of-the-art top-k token based method by up to {\textbackslash}textbackslashtextasciitilde2\% higher accuracy with the same amount of FLOPs. Furthermore, our preliminary results show up to 1.4x and 1.9x throughput improvement on Tesla T4 GPU and Intel Haswell CPU, respectively, with less than 1\% of accuracy drop (and up to 2.1x FLOPs reduction). Our code has been developed in PyTorch and has been open-sourced.},
  keywords = {nlp,nosource,token pruning,transformer}
}

@article{dehghani_2021,
  title = {The {{Benchmark Lottery}}},
  author = {Dehghani, Mostafa and Tay, Yi and Gritsenko, Alexey A and Zhao, Zhe and Houlsby, Neil and Diaz, Fernando and Metzler, Donald and Vinyals, Oriol},
  year = {2021},
  journal = {arXiv},
  abstract = {The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of "a benchmark lottery" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.},
  keywords = {nosource}
}

@article{aghajanyan_2021,
  title = {{{HTLM}}: {{Hyper-Text Pre-Training}} and {{Prompting}} of {{Language Models}}},
  author = {Aghajanyan, Armen and Okhonko, Dmytro and Lewis, Mike and Joshi, Mandar and Xu, Hu and Ghosh, Gargi and Zettlemoyer, Luke},
  year = {2021},
  journal = {arXiv},
  abstract = {We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling title tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research.},
  keywords = {hypter-text language model,nlp,nosource,prompting}
}

@article{thoppilan_2022,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  shorttitle = {{{LaMDA}}},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and {Meier-Hellstern}, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and {Hoffman-John}, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and {Aguera-Arcas}, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.08239 [cs]},
  eprint = {2201.08239},
  primaryclass = {cs},
  urldate = {2022-01-22},
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,dialog,lm,nlp},
  annotation = {22 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{li_2020,
  title = {{{DocBank}}: {{A Benchmark Dataset}} for {{Document Layout Analysis}}},
  shorttitle = {{{DocBank}}},
  author = {Li, Minghao and Xu, Yiheng and Cui, Lei and Huang, Shaohan and Wei, Furu and Li, Zhoujun and Zhou, Ming},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.01038 [cs]},
  eprint = {2006.01038},
  primaryclass = {cs},
  urldate = {2022-02-07},
  abstract = {Document layout analysis usually relies on computer vision models to understand documents while ignoring textual information that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present {\textbackslash}textbf\{DocBank\}, a benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the {\textbackslash}LaTeX\{\} documents available on the arXiv.com. With DocBank, models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of document layout analysis. We build several strong baselines and manually split train/dev/test sets for evaluation. Experiment results show that models trained on DocBank accurately recognize the layout information for a variety of documents. The DocBank dataset is publicly available at {\textbackslash}url\{https://github.com/doc-analysis/DocBank\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,dataset,document,nlp},
  annotation = {42 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/CAA8EFJD/Li_et_al_(2020)_DocBank.pdf}
}

@article{xu_2022,
  title = {{{LayoutLMv2}}: {{Multi-modal Pre-training}} for {{Visually-Rich Document Understanding}}},
  shorttitle = {{{LayoutLMv2}}},
  author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
  year = {2022},
  month = jan,
  journal = {arXiv:2012.14740 [cs]},
  eprint = {2012.14740},
  primaryclass = {cs},
  urldate = {2022-02-07},
  abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 \${\textbackslash}to\$ 0.8420), CORD (0.9493 \${\textbackslash}to\$ 0.9601), SROIE (0.9524 \${\textbackslash}to\$ 0.9781), Kleister-NDA (0.8340 \${\textbackslash}to\$ 0.8520), RVL-CDIP (0.9443 \${\textbackslash}to\$ 0.9564), and DocVQA (0.7295 \${\textbackslash}to\$ 0.8672). We made our model and code publicly available at {\textbackslash}url\{https://aka.ms/layoutlmv2\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,document,layout,nlp,transformer},
  annotation = {GSCC: 0000109 \\
56 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/9367ZB9J/Xu_et_al_(2022)_LayoutLMv2.pdf}
}

@inproceedings{bowman_2021,
  title = {What {{Will}} It {{Take}} to {{Fix Benchmarking}} in {{Natural Language Understanding}}?},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Bowman, Samuel R. and Dahl, George},
  year = {2021},
  month = jun,
  pages = {4843--4855},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.385},
  urldate = {2022-02-20},
  abstract = {Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.},
  keywords = {benchmark,bias,dataset,nlp},
  annotation = {35 citations (Semantic Scholar/DOI) [2022-04-05]}
}

@inproceedings{manku_2007,
  title = {Detecting Near-Duplicates for Web Crawling},
  booktitle = {Proceedings of the 16th International Conference on {{World Wide Web}}  - {{WWW}} '07},
  author = {Manku, Gurmeet Singh and Jain, Arvind and Das Sarma, Anish},
  year = {2007},
  pages = {141},
  publisher = {ACM Press},
  address = {Banff, Alberta, Canada},
  doi = {10.1145/1242572.1242592},
  urldate = {2022-02-20},
  abstract = {Near-duplicate web documents are abundant. Two such documents differ from each other in a very small portion that displays advertisements, for example. Such differences are irrelevant for web search. So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not. In the course of developing a near-duplicate detection system for a multi-billion page repository, we make two research contributions. First, we demonstrate that Charikar's fingerprinting technique is appropriate for this goal. Second, we present an algorithmic technique for identifying existing f bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k. Our technique is useful for both online queries (single fingerprints) and batch queries (multiple fingerprints). Experimental evaluation over real data confirms the practicality of our design.},
  isbn = {978-1-59593-654-7},
  langid = {english},
  keywords = {data,deduplication},
  annotation = {593 citations (Semantic Scholar/DOI) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/JNKC9YBP/Manku_et_al_(2007)_Detecting_near-duplicates_for_web_crawling.pdf}
}

@article{baydin_2022,
  title = {Gradients without {{Backpropagation}}},
  author = {Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Pearlmutter, Barak A. and Syme, Don and Wood, Frank and Torr, Philip},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.08587 [cs, stat]},
  eprint = {2202.08587},
  primaryclass = {cs, stat},
  urldate = {2022-02-21},
  abstract = {Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differentiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one can compute exactly and efficiently via the forward mode. We call this formulation the forward gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for backpropagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training up to twice as fast in some cases.},
  archiveprefix = {arXiv},
  keywords = {68T07,algorithms,backpropagation,Computer Science - Machine Learning,I.2.5,I.2.6,neural networks,Statistics - Machine Learning},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/XI5P88B6/Baydin_et_al_(2022)_Gradients_without_Backpropagation.pdf}
}

@article{li_2022a,
  title = {{{DiT}}: {{Self-supervised Pre-training}} for {{Document Image Transformer}}},
  shorttitle = {{{DiT}}},
  author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.02378 [cs]},
  eprint = {2203.02378},
  primaryclass = {cs},
  urldate = {2022-03-10},
  abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, as well as table detection. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \${\textbackslash}rightarrow\$ 92.69), document layout analysis (91.0 \${\textbackslash}rightarrow\$ 94.9) and table detection (94.23 \${\textbackslash}rightarrow\$ 96.55). The code and pre-trained models are publicly available at {\textbackslash}url\{https://aka.ms/msdit\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,document,nlp,transformer},
  annotation = {GSCC: 0000008 \\
0 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/PKP4VARX/Li_et_al_(2022)_DiT.pdf}
}

@article{kuznetsov_2021,
  title = {Spelling {{Correction}} with {{Denoising Transformer}}},
  author = {Kuznetsov, Alex and Urdiales, Hector},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05977 [cs]},
  eprint = {2105.05977},
  primaryclass = {cs},
  urldate = {2022-03-23},
  abstract = {We present a novel method of performing spelling correction on short input strings, such as search queries or individual words. At its core lies a procedure for generating artificial typos which closely follow the error patterns manifested by humans. This procedure is used to train the production spelling correction model based on a transformer architecture. This model is currently served in the HubSpot product search. We show that our approach to typo generation is superior to the widespread practice of adding noise, which ignores human patterns. We also demonstrate how our approach may be extended to resource-scarce settings and train spelling correction models for Arabic, Greek, Russian, and Setswana languages, without using any labeled data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,nlp,spelling correction,transformer},
  annotation = {2 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{chalkidis_2022,
  title = {{{LexGLUE}}: {{A Benchmark Dataset}} for {{Legal Language Understanding}} in {{English}}},
  shorttitle = {{{LexGLUE}}},
  author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel Martin and Aletras, Nikolaos},
  year = {2022},
  month = mar,
  journal = {arXiv:2110.00976 [cs]},
  eprint = {2110.00976},
  primaryclass = {cs},
  urldate = {2022-03-26},
  abstract = {Laws and their interpretations, legal arguments and agreements{\textbackslash} are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.},
  archiveprefix = {arXiv},
  keywords = {benchmark,Computer Science - Computation and Language,dataset,legal,nlp},
  annotation = {7 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/ZW7HUSWM/Chalkidis_et_al_(2022)_LexGLUE.pdf}
}

@article{hou_2022,
  title = {Token {{Dropping}} for {{Efficient BERT Pretraining}}},
  author = {Hou, Le and Pang, Richard Yuanzhe and Zhou, Tianyi and Wu, Yuexin and Song, Xinying and Song, Xiaodan and Zhou, Denny},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.13240 [cs]},
  eprint = {2203.13240},
  primaryclass = {cs},
  urldate = {2022-04-02},
  abstract = {Transformer-based models generally allocate the same amount of computation for each token in a given sequence. We develop a simple but effective "token dropping" method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks. In short, we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens; the dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences. We leverage the already built-in masked language modeling (MLM) loss to identify unimportant tokens with practically no computational overhead. In our experiments, this simple approach reduces the pretraining cost of BERT by 25\% while achieving similar overall fine-tuning performance on standard downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,nlp,pre-training,token},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-05]}
}

@article{choshen_2022,
  title = {Fusing Finetuned Models for Better Pretraining},
  author = {Choshen, Leshem and Venezian, Elad and Slonim, Noam and Katz, Yoav},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.03044 [cs]},
  eprint = {2204.03044},
  primaryclass = {cs},
  urldate = {2022-04-12},
  abstract = {Pretrained models are the standard starting point for training. This approach consistently outperforms the use of a random initialization. However, pretraining is a costly endeavour that few can undertake. In this paper, we create better base models at hardly any cost, by fusing multiple existing fine tuned models into one. Specifically, we fuse by averaging the weights of these models. We show that the fused model results surpass the pretrained model ones. We also show that fusing is often better than intertraining. We find that fusing is less dependent on the target task. Furthermore, weight decay nullifies intertraining effects but not those of fusing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,finetuning,nlp,pretraining},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-12]},
  file = {/Users/chenghao/Zotero/storage/P2QF3RNC/Choshen_et_al_(2022)_Fusing_finetuned_models_for_better_pretraining.pdf}
}

@article{huang_2022,
  title = {{{LayoutLMv3}}: {{Pre-training}} for {{Document AI}} with {{Unified Text}} and {{Image Masking}}},
  shorttitle = {{{LayoutLMv3}}},
  author = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.08387 [cs]},
  eprint = {2204.08387},
  primaryclass = {cs},
  urldate = {2022-04-19},
  abstract = {Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,layout,layoutlm,nlp,plm},
  annotation = {GSCC: 0000012},
  file = {/Users/chenghao/Zotero/storage/6KT7KPRJ/Huang_et_al_(2022)_LayoutLMv3.pdf}
}

@article{decurto_2022,
  title = {Learning with {{Signatures}}},
  author = {{de Curt{\`o}}, J. and {de Zarz{\`a}}, I. and Calafate, Carlos T. and Yan, Hong},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.07953 [cs]},
  eprint = {2204.07953},
  primaryclass = {cs},
  urldate = {2022-04-24},
  abstract = {In this work we investigate the use of the Signature Transform in the context of Learning. Under this assumption, we advance a supervised framework that provides state-of-the-art classification accuracy with the use of very few labels without the need of credit assignment and with minimal or no overfitting. We leverage tools from harmonic analysis by the use of the signature and log-signature and use as a score function RMSE and MAE Signature and log-signature. We develop a closed-form equation to compute probably good optimal scale factors. Classification is performed at the CPU level orders of magnitude faster than other methods. We report results on AFHQ dataset, Four Shapes, MNIST and CIFAR10 achieving 100\% accuracy on all tasks.},
  archiveprefix = {arXiv},
  keywords = {classification,Computer Science - Computer Vision and Pattern Recognition,cv,supervised learning},
  file = {/Users/chenghao/Zotero/storage/4D7HTFVD/de_Curt_et_al_(2022)_Learning_with_Signatures.pdf}
}

@article{keren_2022,
  title = {Breaking {{Character}}: {{Are Subwords Good Enough}} for {{MRLs After All}}?},
  shorttitle = {Breaking {{Character}}},
  author = {Keren, Omri and Avinari, Tal and Tsarfaty, Reut and Levy, Omer},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.04748 [cs]},
  eprint = {2204.04748},
  primaryclass = {cs},
  urldate = {2022-04-30},
  abstract = {Large pretrained language models (PLMs) typically tokenize the input string into contiguous subwords before any pretraining or inference. However, previous studies have claimed that this form of subword tokenization is inadequate for processing morphologically-rich languages (MRLs). We revisit this hypothesis by pretraining a BERT-style masked language model over character sequences instead of word-pieces. We compare the resulting model, dubbed TavBERT, against contemporary PLMs based on subwords for three highly complex and ambiguous MRLs (Hebrew, Turkish, and Arabic), testing them on both morphological and semantic tasks. Our results show, for all tested languages, that while TavBERT obtains mild improvements on surface-level tasks {\textbackslash}`a la POS tagging and full morphological disambiguation, subword-based PLMs achieve significantly higher performance on semantic tasks, such as named entity recognition and extractive question answering. These results showcase and (re)confirm the potential of subword tokenization as a reasonable modeling assumption for many languages, including MRLs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,morphological rich language,nlp,subword,tokenization},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-29]},
  file = {/Users/chenghao/Zotero/storage/BR4CSAXG/Keren_et_al_(2022)_Breaking_Character.pdf}
}

@article{manning_2022,
  title = {Human {{Language Understanding}} \& {{Reasoning}}},
  author = {Manning, Christopher D.},
  year = {2022},
  month = may,
  journal = {Daedalus},
  volume = {151},
  number = {2},
  pages = {127--138},
  issn = {0011-5266, 1548-6192},
  doi = {10.1162/daed_a_01905},
  urldate = {2022-05-15},
  abstract = {Abstract             The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.},
  langid = {english},
  keywords = {language understanding},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-05-14]},
  file = {/Users/chenghao/Zotero/storage/8MTVTTJN/Manning_(2022)_Human_Language_Understanding_&_Reasoning.pdf}
}

@misc{lourie_2021,
  title = {{{UNICORN}} on {{RAINBOW}}: {{A Universal Commonsense Reasoning Model}} on a {{New Multitask Benchmark}}},
  shorttitle = {{{UNICORN}} on {{RAINBOW}}},
  author = {Lourie, Nicholas and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  year = {2021},
  month = mar,
  number = {arXiv:2103.13009},
  eprint = {2103.13009},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2103.13009},
  urldate = {2022-05-24},
  abstract = {Commonsense AI has long been seen as a near impossible goal -- until recently. Now, research interest has sharply increased with an influx of new benchmarks and models. We propose two new ways to evaluate commonsense models, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote research on commonsense models that generalize well over multiple tasks and datasets. Second, we propose a novel evaluation, the cost equivalent curve, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency. We perform extensive experiments -- over 200 experiments encompassing 4800 models -- and report multiple valuable and sometimes surprising findings, e.g., that transfer almost always leads to better or equivalent performance if following a particular recipe, that QA-based commonsense datasets transfer well with each other, while commonsense knowledge graphs do not, and that perhaps counter-intuitively, larger models benefit more from transfer than smaller ones. Last but not least, we introduce a new universal commonsense reasoning model, UNICORN, that establishes new state-of-the-art performance across 8 popular commonsense benchmarks, aNLI (87.3\%), CosmosQA (91.8\%), HellaSWAG (93.9\%), PIQA (90.1\%), SocialIQa (83.2\%), WinoGrande (86.6\%), CycIC (94.0\%) and CommonsenseQA (79.3\%).},
  archiveprefix = {arXiv},
  keywords = {benchmark,commonsense,Computer Science - Computation and Language,nlp},
  annotation = {38 citations (Semantic Scholar/arXiv) [2022-05-23]},
  file = {/Users/chenghao/Zotero/storage/FDDDW3S9/Lourie_et_al_(2021)_UNICORN_on_RAINBOW.pdf}
}

@misc{kojima_2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  month = may,
  number = {arXiv:2205.11916},
  eprint = {2205.11916},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.11916},
  urldate = {2022-05-27},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with an off-the-shelf 175B parameter model. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted through simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,few-shot,PLM,zero-shot},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-05-26]},
  file = {/Users/chenghao/Zotero/storage/QF5YF25E/Kojima_et_al_(2022)_Large_Language_Models_are_Zero-Shot_Reasoners.pdf}
}

@misc{gesmundo_2022,
  title = {An {{Evolutionary Approach}} to {{Dynamic Introduction}} of {{Tasks}} in {{Large-scale Multitask Learning Systems}}},
  author = {Gesmundo, Andrea and Dean, Jeff},
  year = {2022},
  month = may,
  number = {arXiv:2205.12755},
  eprint = {2205.12755},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.12755},
  urldate = {2022-05-27},
  abstract = {Multitask learning assumes that models capable of learning from multiple tasks can achieve better quality and efficiency via knowledge transfer, a key feature of human learning. Though, state of the art ML models rely on high customization for each task and leverage size and data scale rather than scaling the number of tasks. Also, continual learning, that adds the temporal aspect to multitask, is often focused to the study of common pitfalls such as catastrophic forgetting instead of being studied at a large scale as a critical component to build the next generation artificial intelligence. We propose an evolutionary method that can generate a large scale multitask model, and can support the dynamic and continuous addition of new tasks. The generated multitask model is sparsely activated and integrates a task-based routing that guarantees bounded compute cost and fewer added parameters per task as the model expands. The proposed method relies on a knowledge compartmentalization technique to achieve immunity against catastrophic forgetting and other common pitfalls such as gradient interference and negative transfer. We empirically show that the proposed method can jointly solve and achieve competitive results on 69image classification tasks, for example achieving the best test accuracy reported fora model trained only on public data for competitive tasks such as cifar10: 99.43\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-05-27]},
  file = {/Users/chenghao/Zotero/storage/EQS4NI8L/Gesmundo_Dean_(2022)_An_Evolutionary_Approach_to_Dynamic_Introduction_of_Tasks_in_Large-scale.pdf}
}

@misc{zhang_2022,
  title = {On the {{Paradox}} of {{Learning}} to {{Reason}} from {{Data}}},
  author = {Zhang, Honghua and Li, Liunian Harold and Meng, Tao and Chang, Kai-Wei and den Broeck, Guy Van},
  year = {2022},
  month = may,
  number = {arXiv:2205.11502},
  eprint = {2205.11502},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.11502},
  urldate = {2022-05-27},
  abstract = {Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be trained end-to-end to solve logical reasoning problems presented in natural language? We attempt to answer this question in a confined problem space where there exists a set of parameters that perfectly simulates logical reasoning. We make observations that seem to contradict each other: BERT attains near-perfect accuracy on in-distribution test examples while failing to generalize to other data distributions over the exact same problem space. Our study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems. We also show that it is infeasible to jointly remove statistical features from data, illustrating the difficulty of learning to reason in general. Our result naturally extends to other neural models and unveils the fundamental difference between learning to reason and learning to achieve high performance on NLP benchmarks using statistical features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-05-27]},
  file = {/Users/chenghao/Zotero/storage/NVX2ETEB/Zhang_et_al_(2022)_On_the_Paradox_of_Learning_to_Reason_from_Data.pdf}
}

@misc{zhang_2022a,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = may,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.01068},
  urldate = {2022-05-28},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,nlp,PLM},
  file = {/Users/chenghao/Zotero/storage/TBXAQMP4/Zhang_et_al_(2022)_OPT.pdf}
}

@misc{marcus_2022,
  title = {A Very Preliminary Analysis of {{DALL-E}} 2},
  author = {Marcus, Gary and Davis, Ernest and Aaronson, Scott},
  year = {2022},
  month = may,
  number = {arXiv:2204.13807},
  eprint = {2204.13807},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-04},
  abstract = {The DALL-E 2 system generates original synthetic images corresponding to an input text as caption. We report here on the outcome of fourteen tests of this system designed to assess its common sense, reasoning and ability to understand complex texts. All of our prompts were intentionally much more challenging than the typical ones that have been showcased in recent weeks. Nevertheless, for 5 out of the 14 prompts, at least one of the ten images fully satisfied our requests. On the other hand, on no prompt did all of the ten images satisfy our requests.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {3 citations (Semantic Scholar/arXiv) [2022-06-04]},
  file = {/Users/chenghao/Zotero/storage/X4VTAIE7/Marcus_et_al_(2022)_A_very_preliminary_analysis_of_DALL-E_2.pdf}
}

@misc{diliello_2022,
  title = {Paragraph-Based {{Transformer Pre-training}} for {{Multi-Sentence Inference}}},
  author = {Di Liello, Luca and Garg, Siddhant and Soldaini, Luca and Moschitti, Alessandro},
  year = {2022},
  month = may,
  number = {arXiv:2205.01228},
  eprint = {2205.01228},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-04},
  abstract = {Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models the paragraph-level semantics across multiple input sentences. Our evaluation on three AS2 and one fact verification datasets demonstrates the superiority of our pre-training technique over the traditional ones for transformers used as joint models for multi-candidate inference tasks, as well as when used as cross-encoders for sentence-pair formulations of these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-06-04]},
  file = {/Users/chenghao/Zotero/storage/9VEMW9NJ/Di_Liello_et_al_(2022)_Paragraph-based_Transformer_Pre-training_for_Multi-Sentence_Inference.pdf}
}

@misc{schwartz_2022,
  title = {On the {{Limitations}} of {{Dataset Balancing}}: {{The Lost Battle Against Spurious Correlations}}},
  shorttitle = {On the {{Limitations}} of {{Dataset Balancing}}},
  author = {Schwartz, Roy and Stanovsky, Gabriel},
  year = {2022},
  month = apr,
  number = {arXiv:2204.12708},
  eprint = {2204.12708},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2204.12708},
  urldate = {2022-06-05},
  abstract = {Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out "easy" instances (Sakaguchi et al., 2020), culminating in a recent proposal to eliminate single-word correlations altogether (Gardner et al., 2021). In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations. In parallel, a truly balanced dataset may be bound to "throw the baby out with the bathwater" and miss important signal encoding common sense and world knowledge. We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero- or few-shot setups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/5DHSTAZA/Schwartz_Stanovsky_(2022)_On_the_Limitations_of_Dataset_Balancing.pdf}
}

@misc{srivastava_2022,
  title = {Beyond the {{Imitation Game}}: {{Quantifying}} and Extrapolating the Capabilities of Language Models},
  shorttitle = {Beyond the {{Imitation Game}}},
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and {Garriga-Alonso}, Adri{\`a} and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Santilli, Andrea and Stuhlm{\"u}ller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karaka{\c s}, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bart{\l}omiej and {\"O}zyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ram{\'i}rez, C{\'e}sar Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and {Callison-Burch}, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and Gonz{\'a}lez, Daniel Mosegu{\'i} and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and {Mart{\'i}nez-Plumed}, Fernando and Happ{\'e}, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and {de Melo}, Gerard and Kruszewski, Germ{\'a}n and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and {Jaimovitch-L{\'o}pez}, Gonzalo and Betz, Gregor and {Gur-Ari}, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Sch{\"u}tze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fern{\'a}ndez and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Koco{\'n}, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and {Sohl-Dickstein}, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, J{\"o}rg and Rozen, Jos and {Hernandez-Orallo}, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and {Contreras-Ochando}, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Col{\'o}n, Luis Oliveros and Metz, Luke and {\c S}enel, L{\"u}tfi Kerem and Bosma, Maarten and Sap, Maarten and {ter Hoeve}, Maartje and Andrea, Madotto and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ram{\'i}rez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, M{\'a}ty{\'a}s and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Sw{\k e}drowski, Micha{\l} and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Mi{\l}kowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ram{\'o}n Risco and Milli{\`e}re, Rapha{\"e}l and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Th{\'e}o and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and {Telleen-Lawton}, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Xinran, Zhao and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04615},
  eprint = {2206.04615},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-06-11},
  abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/2J59UXU9/Srivastava_et_al_(2022)_Beyond_the_Imitation_Game.pdf}
}

@misc{schuff_2022,
  title = {Human {{Interpretation}} of {{Saliency-based Explanation Over Text}}},
  author = {Schuff, Hendrik and Jacovi, Alon and Adel, Heike and Goldberg, Yoav and Vu, Ngoc Thang},
  year = {2022},
  month = jan,
  number = {arXiv:2201.11569},
  eprint = {2201.11569},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-19},
  abstract = {While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople's interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees' importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-06-19]},
  file = {/Users/chenghao/Zotero/storage/UMGU4M7Q/Schuff_et_al_(2022)_Human_Interpretation_of_Saliency-based_Explanation_Over_Text.pdf}
}

@misc{hendrycks_2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2021},
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-23},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {50 citations (Semantic Scholar/arXiv) [2022-06-23]},
  file = {/Users/chenghao/Zotero/storage/ZEDGJKED/Hendrycks_et_al_(2021)_Measuring_Massive_Multitask_Language_Understanding.pdf}
}

@inproceedings{ruder_2022,
  title = {Square {{One Bias}} in {{NLP}}: {{Towards}} a {{Multi-Dimensional Exploration}} of the {{Research Manifold}}},
  shorttitle = {Square {{One Bias}} in {{NLP}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Ruder, Sebastian and Vuli{\'c}, Ivan and S{\o}gaard, Anders},
  year = {2022},
  pages = {2340--2354},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.184},
  urldate = {2022-06-26},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-06-26]},
  file = {/Users/chenghao/Zotero/storage/948DPXZN/Ruder_et_al_(2022)_Square_One_Bias_in_NLP.pdf}
}

@misc{sorscher_2022,
  title = {Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning},
  shorttitle = {Beyond Neural Scaling Laws},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14486},
  eprint = {2206.14486},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.14486},
  urldate = {2022-07-03},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this new exponential scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling performance on ResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-02]},
  file = {/Users/chenghao/Zotero/storage/R34G7VPG/Sorscher_et_al_(2022)_Beyond_neural_scaling_laws.pdf}
}

@misc{henderson_2022,
  title = {Pile of {{Law}}: {{Learning Responsible Data Filtering}} from the {{Law}} and a {{256GB Open-Source Legal Dataset}}},
  shorttitle = {Pile of {{Law}}},
  author = {Henderson, Peter and Krass, Mark S. and Zheng, Lucia and Guha, Neel and Manning, Christopher D. and Jurafsky, Dan and Ho, Daniel E.},
  year = {2022},
  month = jul,
  number = {arXiv:2207.00220},
  eprint = {2207.00220},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.00220},
  urldate = {2022-07-09},
  abstract = {One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take into account context. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may potentially help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,data filtering,dataset,law,natural language processing},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-09]\\
0 citations (Semantic Scholar/DOI) [2022-07-09]},
  file = {/Users/chenghao/Zotero/storage/S6ZYUG63/Henderson_et_al_(2022)_Pile_of_Law.pdf}
}

@misc{_a,
  title = {No {{Language Left Behind}}: {{Scaling Human-Centered Machine Translation}} - {{Meta Research}}},
  shorttitle = {No {{Language Left Behind}}},
  journal = {Meta Research},
  urldate = {2022-07-09},
  abstract = {We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over...},
  howpublished = {https://research.facebook.com/publications/no-language-left-behind/},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/SQNN5PQ2/No_Language_Left_Behind.pdf}
}

@misc{heffernan_2022,
  title = {Bitext {{Mining Using Distilled Sentence Representations}} for {{Low-Resource Languages}}},
  author = {Heffernan, Kevin and {\c C}elebi, Onur and Schwenk, Holger},
  year = {2022},
  month = may,
  number = {arXiv:2205.12654},
  eprint = {2205.12654},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.12654},
  urldate = {2022-07-09},
  abstract = {Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. A promising approach has been to train one-for-all multilingual models capable of cross-lingual transfer, but these models often suffer from insufficient capacity and interference between unrelated languages. Instead, we move away from this approach and focus on training multiple language (family) specific representations, but most prominently enable all languages to still be encoded in the same representational space. To achieve this, we focus on teacher-student training, allowing all encoders to be mutually compatible for bitext mining, and enabling fast learning of new languages. We introduce a new teacher-student training scheme which combines supervised and self-supervised training, allowing encoders to take advantage of monolingual training data, which is valuable in the low-resource setting. Our approach significantly outperforms the original LASER encoder. We study very low-resource languages and handle 50 African languages, many of which are not covered by any other model. For these languages, we train sentence encoders, mine bitexts, and validate the bitexts by training NMT systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,dataset,multilingual,natural language processing},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-09]\\
0 citations (Semantic Scholar/DOI) [2022-07-09]},
  file = {/Users/chenghao/Zotero/storage/9NBFQ29I/Heffernan_et_al_(2022)_Bitext_Mining_Using_Distilled_Sentence_Representations_for_Low-Resource.pdf}
}

@misc{kadavath_2022,
  title = {Language {{Models}} ({{Mostly}}) {{Know What They Know}}},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and {Tran-Johnson}, Eli and Johnston, Scott and {El-Showk}, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  year = {2022},
  month = jul,
  number = {arXiv:2207.05221},
  eprint = {2207.05221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.05221},
  urldate = {2022-07-23},
  abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-07-23]\\
1 citations (Semantic Scholar/DOI) [2022-07-23]},
  file = {/Users/chenghao/Zotero/storage/8MUJ4U2J/Kadavath_et_al_(2022)_Language_Models_(Mostly)_Know_What_They_Know.pdf}
}

@misc{schuster_2022,
  title = {Confident {{Adaptive Language Modeling}}},
  author = {Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q. and Tay, Yi and Metzler, Donald},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07061},
  eprint = {2207.07061},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.07061},
  urldate = {2022-07-23},
  abstract = {Recent advances in Transformer-based large language models (LLMs) have led to significant performance improvements across many tasks. These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time. In practice, however, the series of generations made by LLMs is composed of varying levels of difficulty. While certain predictions truly benefit from the models' full capacity, other continuations are more trivial and can be solved with reduced compute. In this work, we introduce Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep. Early exit decoding involves several challenges that we address here, such as: (1) what confidence measure to use; (2) connecting sequence-level constraints to local per-token exit decisions; and (3) attending back to missing hidden representations due to early exits in previous tokens. Through theoretical analysis and empirical experiments on three diverse text generation tasks, we demonstrate the efficacy of our framework in reducing compute -- potential speedup of up to \${\textbackslash}times 3\$ -- while provably maintaining high performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-23]\\
0 citations (Semantic Scholar/DOI) [2022-07-23]},
  file = {/Users/chenghao/Zotero/storage/HZ693FBT/Schuster_et_al_(2022)_Confident_Adaptive_Language_Modeling.pdf}
}

@misc{wortsman_2022,
  title = {Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  shorttitle = {Model Soups},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  year = {2022},
  month = jul,
  number = {arXiv:2203.05482},
  eprint = {2203.05482},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.05482},
  urldate = {2022-07-24},
  abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups." When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/8XGV2JEB/Wortsman_et_al_(2022)_Model_soups.pdf}
}

@misc{xiong_2020,
  title = {On {{Layer Normalization}} in the {{Transformer Architecture}}},
  author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan},
  year = {2020},
  month = jun,
  number = {arXiv:2002.04745},
  eprint = {2002.04745},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.04745},
  urldate = {2022-07-31},
  abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {229 citations (Semantic Scholar/arXiv) [2022-07-31]},
  file = {/Users/chenghao/Zotero/storage/3G42ED8E/Xiong_et_al_(2020)_On_Layer_Normalization_in_the_Transformer_Architecture.pdf}
}

@misc{xiong_2022,
  title = {Simple {{Local Attentions Remain Competitive}} for {{Long-Context Tasks}}},
  author = {Xiong, Wenhan and O{\u g}uz, Barlas and Gupta, Anchit and Chen, Xilun and Liskovich, Diana and Levy, Omer and Yih, Wen-tau and Mehdad, Yashar},
  year = {2022},
  month = may,
  number = {arXiv:2112.07210},
  eprint = {2112.07210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.07210},
  urldate = {2022-07-31},
  abstract = {Many NLP tasks require processing long contexts beyond the length limit of pretrained models. In order to scale these models to longer text sequences, many efficient long-range attention variants have been proposed. Despite the abundance of research along this direction, it is still difficult to gauge the relative effectiveness of these models in practical use cases, e.g., if we apply these models following the pretrain-and-finetune paradigm. In this work, we aim to conduct a thorough analysis of these emerging models with large-scale and controlled experiments. For each attention variant, we pretrain large-size models using the same long-doc corpus and then finetune these models for real-world long-context tasks. Our findings reveal pitfalls of an existing widely-used long-range benchmark and show none of the tested efficient attentions can beat a simple local window attention under standard pretraining paradigms. Further analysis on local attention variants suggests that even the commonly used attention-window overlap is not necessary to achieve good downstream results -- using disjoint local attentions, we are able to build a simpler and more efficient long-doc QA model that matches the performance of Longformer{\textasciitilde}{\textbackslash}citep\{longformer\} with half of its pretraining compute. The code to replicate our experiments can be found at https://github.com/pytorch/fairseq/tree/main/examples/xformers},
  archiveprefix = {arXiv},
  keywords = {architecture,attention,Computer Science - Computation and Language,efficacy,transformers},
  file = {/Users/chenghao/Zotero/storage/XNJKJQ29/Xiong_et_al_(2022)_Simple_Local_Attentions_Remain_Competitive_for_Long-Context_Tasks.pdf}
}

@misc{reed_2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and {Barth-Maron}, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and {de Freitas}, Nando},
  year = {2022},
  month = may,
  number = {arXiv:2205.06175},
  eprint = {2205.06175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.06175},
  urldate = {2022-08-06},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {12 citations (Semantic Scholar/arXiv) [2022-08-06]\\
12 citations (Semantic Scholar/DOI) [2022-08-06]},
  file = {/Users/chenghao/Zotero/storage/5JBJ8J72/Reed_et_al_(2022)_A_Generalist_Agent.pdf}
}

@misc{piantadosi_2022,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantadosi, Steven T. and Hill, Felix},
  year = {2022},
  month = aug,
  number = {arXiv:2208.02957},
  eprint = {2208.02957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.02957},
  urldate = {2022-08-15},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-08-14]\\
0 citations (Semantic Scholar/DOI) [2022-08-14]},
  file = {/Users/chenghao/Zotero/storage/2ZHRWW93/Piantadosi_Hill_(2022)_Meaning_without_reference_in_large_language_models.pdf}
}

@misc{li_2022b,
  title = {Branch-{{Train-Merge}}: {{Embarrassingly Parallel Training}} of {{Expert Language Models}}},
  shorttitle = {Branch-{{Train-Merge}}},
  author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03306},
  eprint = {2208.03306},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.03306},
  urldate = {2022-08-20},
  abstract = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-08-20]\\
1 citations (Semantic Scholar/DOI) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/C6SCND92/Li_et_al_(2022)_Branch-Train-Merge.pdf}
}

@misc{carlini_2022,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  year = {2022},
  month = feb,
  number = {arXiv:2202.07646},
  eprint = {2202.07646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.07646},
  urldate = {2022-08-21},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {28 citations (Semantic Scholar/arXiv) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/NIMTPH9M/Carlini_et_al_(2022)_Quantifying_Memorization_Across_Neural_Language_Models.pdf}
}

@inproceedings{bender_2020,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  shorttitle = {Climbing towards {{NLU}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bender, Emily M. and Koller, Alexander},
  year = {2020},
  month = jul,
  pages = {5185--5198},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.463},
  urldate = {2022-08-21},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as ``understanding'' language or capturing ``meaning''. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of ``Taking Stock of Where We've Been and Where We're Going'', we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  annotation = {267 citations (Semantic Scholar/DOI) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/MVXHHAGL/Bender_Koller_(2020)_Climbing_towards_NLU.pdf}
}

@misc{wei_2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = jun,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.07682},
  urldate = {2022-08-21},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {6 citations (Semantic Scholar/arXiv) [2022-08-20]\\
6 citations (Semantic Scholar/DOI) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/22AMCFG4/Wei_et_al_(2022)_Emergent_Abilities_of_Large_Language_Models.pdf}
}

@misc{hernandez_2022,
  title = {Scaling {{Laws}} and {{Interpretability}} of {{Learning}} from {{Repeated Data}}},
  author = {Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Henighan, Tom and Hume, Tristan and Johnston, Scott and Mann, Ben and Olah, Chris and Olsson, Catherine and Amodei, Dario and Joseph, Nicholas and Kaplan, Jared and McCandlish, Sam},
  year = {2022},
  month = may,
  number = {arXiv:2205.10487},
  eprint = {2205.10487},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.10487},
  urldate = {2022-08-21},
  abstract = {Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1\% of the data 100 times, despite the other 90\% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/Q4VIZQMZ/Hernandez_et_al_(2022)_Scaling_Laws_and_Interpretability_of_Learning_from_Repeated_Data.pdf}
}

@misc{kandpal_2022,
  title = {Deduplicating {{Training Data Mitigates Privacy Risks}} in {{Language Models}}},
  author = {Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  year = {2022},
  month = feb,
  number = {arXiv:2202.06539},
  eprint = {2202.06539},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.06539},
  urldate = {2022-08-21},
  abstract = {Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated {\textasciitilde}1000 times more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  annotation = {16 citations (Semantic Scholar/arXiv) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/YTDEHYLJ/Kandpal_et_al_(2022)_Deduplicating_Training_Data_Mitigates_Privacy_Risks_in_Language_Models.pdf}
}

@misc{treviso_2022,
  title = {Efficient {{Methods}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Efficient {{Methods}} for {{Natural Language Processing}}},
  author = {Treviso, Marcos and Ji, Tianchu and Lee, Ji-Ung and {van Aken}, Betty and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Martins, Pedro H. and Martins, Andr{\'e} F. T. and Milder, Peter and Raffel, Colin and Simpson, Edwin and Slonim, Noam and Balasubramanian, Niranjan and Derczynski, Leon and Schwartz, Roy},
  year = {2022},
  month = aug,
  number = {arXiv:2209.00099},
  eprint = {2209.00099},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.00099},
  urldate = {2022-09-04},
  abstract = {Getting the most out of limited resources allows advances in natural language processing (NLP) research and practice while being conservative with resources. Those resources may be data, time, storage, or energy. Recent work in NLP has yielded interesting results from scaling; however, using only scale to improve results means that resource consumption also scales. That relationship motivates research into efficient methods that require less resources to achieve similar results. This survey relates and synthesises methods and findings in those efficiencies in NLP, aiming to guide new researchers in the field and inspire the development of new methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/NQ37RYKY/Treviso_et_al_(2022)_Efficient_Methods_for_Natural_Language_Processing.pdf}
}

@misc{gutmann_2022,
  title = {Pen and {{Paper Exercises}} in {{Machine Learning}}},
  author = {Gutmann, Michael U.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.13446},
  eprint = {2206.13446},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.13446},
  urldate = {2022-09-10},
  abstract = {This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/6U3GWTYS/Gutmann_(2022)_Pen_and_Paper_Exercises_in_Machine_Learning.pdf}
}

@misc{tay_2022,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2022},
  month = mar,
  number = {arXiv:2009.06732},
  eprint = {2009.06732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.06732},
  urldate = {2022-09-10},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {345 citations (Semantic Scholar/arXiv) [2022-09-10]},
  file = {/Users/chenghao/Zotero/storage/WSG4Q3BT/Tay_et_al_(2022)_Efficient_Transformers.pdf}
}

@misc{ma_2022,
  title = {Mega: {{Moving Average Equipped Gated Attention}}},
  shorttitle = {Mega},
  author = {Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  year = {2022},
  month = sep,
  number = {arXiv:2209.10655},
  eprint = {2209.10655},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.10655},
  urldate = {2022-09-24},
  abstract = {The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-09-24]},
  file = {/Users/chenghao/Zotero/storage/6CACZM4C/Ma_et_al_(2022)_Mega.pdf}
}

@article{tao_2022,
  title = {Knowing {{Where}} and {{What}}: {{Unified Word Block Pretraining}} for {{Document Understanding}}},
  shorttitle = {Knowing {{Where}} and {{What}}},
  author = {Tao, Song and Wang, Zijian and Fan, Tiantian and Luo, Canjie and Huang, Can},
  year = {2022},
  journal = {undefined},
  urldate = {2022-10-07},
  abstract = {This paper focuses on the embedding learning of word blocks containing text and layout information, and proposes UTel, a language model with Unified TExt and Layout pre-training that achieves superior performance than previous methods on various downstream tasks, though requiring no image modality. Due to the complex layouts of documents, it is challenging to extract information for documents. Most previous studies develop multimodal pre-trained models in a self-supervised way. In this paper, we focus on the embedding learning of word blocks containing text and layout information, and propose UTel, a language model with Unified TExt and Layout pre-training. Specifically, we propose two pre-training tasks: Surrounding Word Prediction (SWP) for the layout learning, and Contrastive learning of Word Embeddings (CWE) for identifying different word blocks. Moreover, we replace the commonly used 1D position embedding with a 1D clipped relative position embedding. In this way, the joint training of Masked Layout-Language Modeling (MLLM) and two newly proposed tasks enables the interaction between semantic and spatial features in a unified way. Additionally, the proposed UTel can process arbitrary-length sequences by removing the 1D position embedding, while maintaining competitive performance. Extensive experimental results show UTel learns better joint representations and achieves superior performance than previous methods on various downstream tasks, though requiring no image modality. Code is available at https://github.com/taosong2019/UTel .},
  langid = {english},
  keywords = {nosource},
  annotation = {GSCC: 0000109}
}

@misc{wu_2021,
  title = {{{LAMPRET}}: {{Layout-Aware Multimodal PreTraining}} for {{Document Understanding}}},
  shorttitle = {{{LAMPRET}}},
  author = {Wu, Te-Lin and Li, Cheng and Zhang, Mingyang and Chen, Tao and Hombaiah, Spurthi Amba and Bendersky, Michael},
  year = {2021},
  month = apr,
  number = {arXiv:2104.08405},
  eprint = {2104.08405},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08405},
  urldate = {2022-10-07},
  abstract = {Document layout comprises both structural and visual (eg. font-sizes) information that is vital but often ignored by machine learning models. The few existing models which do use layout information only consider textual contents, and overlook the existence of contents in other modalities such as images. Additionally, spatial interactions of presented contents in a layout were never really fully exploited. To bridge this gap, we parse a document into content blocks (eg. text, table, image) and propose a novel layout-aware multimodal hierarchical framework, LAMPreT, to model the blocks and the whole document. Our LAMPreT encodes each block with a multimodal transformer in the lower-level and aggregates the block-level representations and connections utilizing a specifically designed transformer at the higher-level. We design hierarchical pretraining objectives where the lower-level model is trained similarly to multimodal grounding models, and the higher-level model is trained with our proposed novel layout-aware objectives. We evaluate the proposed model on two layout-aware tasks -- text block filling and image suggestion and show the effectiveness of our proposed hierarchical architecture as well as pretraining techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  annotation = {GSCC: 0000000 \\
9 citations (Semantic Scholar/arXiv) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/SMU5I3G3/Wu_et_al_(2021)_LAMPRET.pdf}
}

@article{borchmann_2021,
  title = {{{DUE}}: {{End-to-End Document Understanding Benchmark}}},
  shorttitle = {{{DUE}}},
  author = {Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Stanislawek, Tomasz and Jurkiewicz, Dawid and Turski, Micha{\l} and Szyndler, Karolina and Grali{\'n}ski, Filip},
  year = {2021},
  month = dec,
  journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  volume = {1},
  urldate = {2022-10-07},
  langid = {english},
  annotation = {GSCC: 0000079},
  file = {/Users/chenghao/Zotero/storage/CDD73SFD/Borchmann_et_al_(2021)_DUE.pdf}
}

@misc{subramani_2021,
  title = {A {{Survey}} of {{Deep Learning Approaches}} for {{OCR}} and {{Document Understanding}}},
  author = {Subramani, Nishant and Matton, Alexandre and Greaves, Malcolm and Lam, Adrian},
  year = {2021},
  month = feb,
  number = {arXiv:2011.13534},
  eprint = {2011.13534},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.13534},
  urldate = {2022-10-07},
  abstract = {Documents are a core part of many businesses in many fields such as law, finance, and technology among others. Automatic understanding of documents such as invoices, contracts, and resumes is lucrative, opening up many new avenues of business. The fields of natural language processing and computer vision have seen tremendous progress through the development of deep learning such that these methods have started to become infused in contemporary document understanding systems. In this survey paper, we review different techniques for document understanding for documents written in English and consolidate methodologies present in literature to act as a jumping-off point for researchers exploring this area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {GSCC: 0000011 \\
9 citations (Semantic Scholar/arXiv) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/PSDPX6YC/Subramani_et_al_(2021)_A_Survey_of_Deep_Learning_Approaches_for_OCR_and_Document_Understanding.pdf}
}

@inproceedings{jain_2020,
  title = {{{SciREX}}: {{A Challenge Dataset}} for {{Document-Level Information Extraction}}},
  shorttitle = {{{SciREX}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jain, Sarthak and {van Zuylen}, Madeleine and Hajishirzi, Hannaneh and Beltagy, Iz},
  year = {2020},
  month = jul,
  pages = {7506--7516},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.670},
  urldate = {2022-10-07},
  abstract = {Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .},
  annotation = {GSCC: 0000071 \\
57 citations (Semantic Scholar/DOI) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/QQCPH6NS/Jain_et_al_(2020)_SciREX.pdf}
}

@misc{stanislawek_2021,
  title = {Kleister: {{Key Information Extraction Datasets Involving Long Documents}} with {{Complex Layouts}}},
  shorttitle = {Kleister},
  author = {Stanis{\l}awek, Tomasz and Grali{\'n}ski, Filip and Wr{\'o}blewska, Anna and Lipi{\'n}ski, Dawid and Kaliska, Agnieszka and Rosalska, Paulina and Topolski, Bartosz and Biecek, Przemys{\l}aw},
  year = {2021},
  month = may,
  number = {arXiv:2105.05796},
  eprint = {2105.05796},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.05796},
  urldate = {2022-10-07},
  abstract = {The relevance of the Key Information Extraction (KIE) task is increasingly important in natural language processing problems. But there are still only a few well-defined problems that serve as benchmarks for solutions in this area. To bridge this gap, we introduce two new datasets (Kleister NDA and Kleister Charity). They involve a mix of scanned and born-digital long formal English-language documents. In these datasets, an NLP system is expected to find or infer various types of entities by employing both textual and structural layout features. The Kleister Charity dataset consists of 2,788 annual financial reports of charity organizations, with 61,643 unique pages and 21,612 entities to extract. The Kleister NDA dataset has 540 Non-disclosure Agreements, with 3,229 unique pages and 2,160 entities to extract. We provide several state-of-the-art baseline systems from the KIE domain (Flair, BERT, RoBERTa, LayoutLM, LAMBERT), which show that our datasets pose a strong challenge to existing models. The best model achieved an 81.77\% and an 83.57\% F1-score on respectively the Kleister NDA and the Kleister Charity datasets. We share the datasets to encourage progress on more in-depth and complex information extraction tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {GSCC: 0000007 \\
8 citations (Semantic Scholar/arXiv) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/KIETX8JQ/Stanisawek_et_al_(2021)_Kleister.pdf}
}

@misc{_2022,
  title = {Document {{AI}}: {{Fine-tuning LayoutLM}} for Document-Understanding Using {{Hugging Face Transformers}}},
  shorttitle = {Document {{AI}}},
  year = {2022},
  month = oct,
  journal = {philschmid blog},
  urldate = {2022-10-09},
  abstract = {Learn how to fine-tune LayoutLM for document-understand using Hugging Face Transformers. LayoutLM is a document image understanding and information extraction transformers.},
  howpublished = {https://www.philschmid.de/fine-tuning-layoutlm},
  langid = {english},
  keywords = {nosource}
}

@article{dahlgaard_2017,
  title = {Fast {{Similarity Sketching}}},
  author = {Dahlgaard, S{\o}ren and Knudsen, Mathias B{\ae}k Tejs and Thorup, Mikkel},
  year = {2017},
  month = apr,
  doi = {10.48550/arXiv.1704.04370},
  urldate = {2022-10-10},
  abstract = {We consider the Similarity Sketching problem: Given a universe \$[u]= {\textbackslash}\{0,{\textbackslash}ldots,u-1{\textbackslash}\}\$ we want a random function \$S\$ mapping subsets \$A{\textbackslash}subseteq [u]\$ into vectors \$S(A)\$ of size \$t\$, such that similarity is preserved. More precisely: Given sets \$A,B{\textbackslash}subseteq [u]\$, define \$X\_i=[S(A)[i]= S(B)[i]]\$ and \$X={\textbackslash}sum\_\{i{\textbackslash}in [t]\}X\_i\$. We want to have \$E[X]=t{\textbackslash}cdot J(A,B)\$, where \$J(A,B)={\textbar}A{\textbackslash}cap B{\textbar}/{\textbar}A{\textbackslash}cup B{\textbar}\$ and furthermore to have strong concentration guarantees (i.e. Chernoff-style bounds) for \$X\$. This is a fundamental problem which has found numerous applications in data mining, large-scale classification, computer vision, similarity search, etc. via the classic MinHash algorithm. The vectors \$S(A)\$ are also called sketches. The seminal \$t{\textbackslash}times\$MinHash algorithm uses \$t\$ random hash functions \$h\_1,{\textbackslash}ldots, h\_t\$, and stores \${\textbackslash}left({\textbackslash}min\_\{a{\textbackslash}in A\}h\_1(A),{\textbackslash}ldots, {\textbackslash}min\_\{a{\textbackslash}in A\}h\_t(A){\textbackslash}right)\$ as the sketch of \$A\$. The main drawback of MinHash is, however, its \$O(t{\textbackslash}cdot {\textbar}A{\textbar})\$ running time, and finding a sketch with similar properties and faster running time has been the subject of several papers. Addressing this, Li et al. [NIPS'12] introduced one permutation hashing (OPH), which creates a sketch of size \$t\$ in \$O(t + {\textbar}A{\textbar})\$ time, but with the drawback that possibly some of the \$t\$ entries are "empty" when \${\textbar}A{\textbar} = O(t)\$. One could argue that sketching is not necessary in this case, however the desire in most applications is to have one sketching procedure that works for sets of all sizes. Therefore, filling out these empty entries is the subject of several follow-up papers initiated by Shrivastava and Li [ICML'14]. However, these "densification" schemes fail to provide good concentration bounds exactly in the case \${\textbar}A{\textbar} = O(t)\$, where they are needed. (continued...)},
  langid = {english},
  annotation = {24 citations (Semantic Scholar/arXiv) [2022-10-09]},
  file = {/Users/chenghao/Zotero/storage/7QXV6QUI/Dahlgaard_et_al_(2017)_Fast_Similarity_Sketching.pdf}
}

@misc{peng_2022,
  title = {{{ERNIE-Layout}}: {{Layout Knowledge Enhanced Pre-training}} for {{Visually-rich Document Understanding}}},
  shorttitle = {{{ERNIE-Layout}}},
  author = {Peng, Qiming and Pan, Yinxu and Wang, Wenjin and Luo, Bin and Zhang, Zhenyu and Huang, Zhengjie and Hu, Teng and Yin, Weichong and Chen, Yongfeng and Zhang, Yin and Feng, Shikun and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06155},
  eprint = {2210.06155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.06155},
  urldate = {2022-10-13},
  abstract = {Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge enhancement in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model\_zoo/ernie-layout.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/4ECVFZ28/Peng_et_al_(2022)_ERNIE-Layout.pdf}
}

@article{anonymous_2022,
  title = {{{ERNIE-Layout}}: {{Layout-Knowledge Enhanced Multi-modal Pre-training}} for {{Document Understanding}}},
  shorttitle = {{{ERNIE-Layout}}},
  author = {Anonymous},
  year = {2022},
  month = jan,
  urldate = {2022-10-14},
  abstract = {We propose ERNIE-Layout, a knowledge enhanced pre-training approach for visual document understanding, which incorporates layout-knowledge into the pre-training of visual document understanding to learn a better joint multi-modal representation of text, layout and image. Previous works directly model serialized tokens from documents according to a raster-scan order, neglecting the importance of the reading order of documents, leading to sub-optimal performance. We incorporate layout-knowledge from Document-Parser into document pre-training, which is used to rearrange the tokens following an order more consistent with human reading habits. And we propose the Reading Order Prediction (ROP) task to enhance the interactions within segments and correlation between segments and a fine-grained cross-modal alignment pre-training task named Replaced Regions Prediction (RRP). ERNIE-Layout attempts to fuse textual and visual features in a unified Transformer model, which is based on our newly proposed spatial-aware disentangled attention mechanism. ERNIE-Layout achieves superior performance on various document understanding tasks, setting new SOTA for four tasks, including information extraction, document classification, document question answering.},
  langid = {english},
  annotation = {GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/TEGPI9JD/Anonymous_(2022)_ERNIE-Layout.pdf}
}

@misc{muennighoff_2022,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"i}c and Reimers, Nils},
  year = {2022},
  month = oct,
  number = {arXiv:2210.07316},
  eprint = {2210.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.07316},
  urldate = {2022-10-22},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 56 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://huggingface.co/spaces/mteb/leaderboard.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-10-22]\\
0 citations (Semantic Scholar/DOI) [2022-10-22]},
  file = {/Users/chenghao/Zotero/storage/TSLI8J3Z/Muennighoff_et_al_(2022)_MTEB.pdf}
}

@misc{kim_2022,
  title = {{{OCR-free Document Understanding Transformer}}},
  author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, Jeongyeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  year = {2022},
  month = oct,
  number = {arXiv:2111.15664},
  eprint = {2111.15664},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.15664},
  urldate = {2022-10-28},
  abstract = {Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-10-28]},
  file = {/Users/chenghao/Zotero/storage/75CYXN2T/Kim_et_al_(2022)_OCR-free_Document_Understanding_Transformer.pdf;/Users/chenghao/Zotero/storage/GC3QUA4P/2111.html}
}

@misc{powalski_2021,
  title = {Going {{Full-TILT Boogie}} on {{Document Understanding}} with {{Text-Image-Layout Transformer}}},
  author = {Powalski, Rafa{\l} and Borchmann, {\L}ukasz and Jurkiewicz, Dawid and Dwojak, Tomasz and Pietruszka, Micha{\l} and Pa{\l}ka, Gabriela},
  year = {2021},
  month = jul,
  number = {arXiv:2102.09550},
  eprint = {2102.09550},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09550},
  urldate = {2022-10-28},
  abstract = {We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of unifying a variety of problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. Our novel approach achieves state-of-the-art results in extracting information from documents and answering questions which demand layout understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process by employing an end-to-end model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {38 citations (Semantic Scholar/arXiv) [2022-10-28]},
  file = {/Users/chenghao/Zotero/storage/MALB6JGZ/Powalski_et_al_(2021)_Going_Full-TILT_Boogie_on_Document_Understanding_with_Text-Image-Layout.pdf;/Users/chenghao/Zotero/storage/LDAR9JXA/2102.html}
}

@article{lin_2021,
  title = {{{ViBERTgrid}}: {{A Jointly Trained Multi-Modal 2D Document Representation}} for {{Key Information Extraction}} from {{Documents}}},
  shorttitle = {{{ViBERTgrid}}},
  author = {Lin, Weihong and Gao, Qifang and Sun, Lei and Zhong, Zhuoyao and Hu, Kaiqin and Ren, Qin and Huo, Qiang},
  year = {2021},
  journal = {ICDAR},
  doi = {10.1007/978-3-030-86549-8_35},
  abstract = {This paper proposes a new multi-modal backbone network by concatenating a BERTgrid to an intermediate layer of a CNN model, to generate a more powerful grid-based document representation, named ViBERTgrid, which has achieved state-of-the-art performance on real-world datasets. Recent grid-based document representations like BERTgrid allow the simultaneous encoding of the textual and layout information of a document in a 2D feature map so that state-of-the-art image segmentation and/or object detection models can be straightforwardly leveraged to extract key information from documents. However, such methods have not achieved comparable performance to state-of-the-art sequenceand graph-based methods such as LayoutLM and PICK yet. In this paper, we propose a new multi-modal backbone network by concatenating a BERTgrid to an intermediate layer of a CNN model, where the input of CNN is a document image and the BERTgrid is a grid of word embeddings, to generate a more powerful grid-based document representation, named ViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal backbone network are trained jointly. Our experimental results demonstrate that this joint training strategy improves significantly the representation ability of ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction approach has achieved state-of-the-art performance on real-world datasets.},
  annotation = {7 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000011},
  file = {/Users/chenghao/Zotero/storage/N62XZ3KQ/Lin_et_al_(2021)_ViBERTgrid.pdf}
}

@article{chen_2022,
  title = {{{XDoc}}: {{Unified Pre-training}} for {{Cross-Format Document Understanding}}},
  shorttitle = {{{XDoc}}},
  author = {Chen, Jingye and Lv, Tengchao and Cui, Lei and Zhang, Changrong and Wei, Furu},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2210.02849},
  abstract = {XDoc is proposed, a unified pre-trained model which deals with different document formats in a single model, which is cost effective for real-world deployment and shares backbone parameters for different formats such as the word embedding layer and the Transformer layers. The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance, existing pre-trained models usually target one specific document format at one time, making it difficult to combine knowledge from multiple document formats. To address this, we propose XDoc, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7\% parameters, XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. The code and pre-trained models will be publicly available at https://aka.ms/xdoc.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/H7WN7YIJ/Chen_et_al_(2022)_XDoc.pdf;/Users/chenghao/Zotero/storage/B4Y6AKRX/e5b03c3165d3a056dc55ce18835be04f5f817f4b.html}
}

@article{cheng_2022,
  title = {{{TRIE}}++: {{Towards End-to-End Information Extraction}} from {{Visually Rich Documents}}},
  shorttitle = {{{TRIE}}++},
  author = {Cheng, Zhanzhan and Zhang, Peng and Li, Can and Liang, Qiao and Xu, Yunlu and Li, Pengfei and Pu, Shiliang and Niu, Yi and Wu, Fei},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2207.06744},
  abstract = {A end-to-end information extraction framework from visually rich documents, where text reading and information extraction can reinforce each other via a well-designed multi-modal context block is proposed. ---Recently, automatically extracting information from visually rich documents ( e.g., tickets and resumes) has become a hot and vital research topic due to its widespread commercial value. Most existing methods divide this task into two subparts: the text reading part for obtaining the plain text from the original document images and the information extraction part for extracting key contents. These methods mainly focus on improving the second, while neglecting that the two parts are highly correlated. This paper proposes a unified end-to-end information extraction framework from visually rich documents, where text reading and information extraction can reinforce each other via a well-designed multi-modal context block. Specifically, the text reading part provides multi-modal features like visual, textual and layout features. The multi-modal context block is developed to fuse the generated multi-modal features and even the prior knowledge from the pre-trained language model for better semantic representation. The information extraction part is responsible for generating key contents with the fused context features. The framework can be trained in an end-to-end trainable manner, achieving global optimization. What is more, we define and group visually rich documents into four categories across two dimensions, the layout and text type. For each document category, we provide or recommend the corresponding benchmarks, experimental settings and strong baselines for remedying the problem that this research area lacks the uniform evaluation standard. Extensive experiments on four kinds of benchmarks (from fixed layout to variable layout, from full-structured text to semi-unstructured text) are reported, demonstrating the proposed method's effectiveness. Data, source code and models are available.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/6AHMIQYY/Cheng_et_al_(2022)_TRIE++.pdf}
}

@inproceedings{nguyen_2021,
  title = {Skim-{{Attention}}: {{Learning}} to {{Focus}} via {{Document Layout}}},
  shorttitle = {Skim-{{Attention}}},
  booktitle = {{{EMNLP}}},
  author = {Nguyen, Laura and Scialom, Thomas and Staiano, Jacopo and Piwowarski, Benjamin},
  year = {2021},
  doi = {10.18653/v1/2021.findings-emnlp.207},
  abstract = {Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout, and can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2dimensional position of the words in a document. Our experiments show that SkimAttention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention.},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000001},
  file = {/Users/chenghao/Zotero/storage/BFFJHAXU/Nguyen_et_al_(2021)_Skim-Attention.pdf}
}

@article{zhu_2022,
  title = {Towards {{Complex Document Understanding By Discrete Reasoning}}},
  author = {Zhu, Fengbin and Lei, Wenqiang and Feng, Fuli and Wang, Chao and Zhang, Haozhou and Chua, Tat-Seng},
  year = {2022},
  journal = {ACM Multimedia},
  doi = {10.1145/3503161.3548422},
  abstract = {A novel model named MHST is developed that takes into account the information in multi-modalities to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. Document Visual Question Answering (VQA) aims to answer questions over visually-rich documents. In this work, we introduce a new Document VQA dataset, named TAT-DQA, which consists of 3,067 document pages comprising semi-structured table(s) and unstructured text as well as 16,558 question-answer pairs. The documents are sampled from financial reports and contain lots of numbers, which means discrete reasoning capability is demanded to answer the questions. Based on TAT-DQA, we further develop a novel model named MHST that takes into account the information in multi-modalities to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. The experiments show that MHST model significantly outperforms the baseline methods, demonstrating its effectiveness. However, the performance still lags far behind that of expert humans. We expect that our TAT-DQA dataset would facilitate the research on understanding of visually-rich documents, especially for scenarios that require discrete reasoning. Also, we hope the proposed model would inspire researchers to design more advanced Document VQA models in future.},
  annotation = {2 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000015},
  file = {/Users/chenghao/Zotero/storage/GYBDGDCT/Zhu_et_al_(2022)_Towards_Complex_Document_Understanding_By_Discrete_Reasoning.pdf}
}

@article{wang_2022a,
  title = {{{mmLayout}}: {{Multi-grained MultiModal Transformer}} for {{Document Understanding}}},
  shorttitle = {{{mmLayout}}},
  author = {Wang, Wenjin and Huang, Zhengjie and Luo, Bin and Chen, Qianglong and Peng, Qiming and Pan, Yinxu and Yin, Weichong and Feng, Shi and Sun, Yu and Yu, Dianhai and Zhang, Yin},
  year = {2022},
  journal = {ACM Multimedia},
  doi = {10.1145/3503161.3548406},
  abstract = {Experimental results on four tasks, including information extraction and document question answering, show that the proposed method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters. Recent efforts of multimodal Transformers have improved Visually Rich Document Understanding (VrDU) tasks via incorporating visual and textual information. However, existing approaches mainly focus on fine-grained elements such as words and document image patches, making it hard for them to learn from coarse-grained elements, including natural lexical units like phrases and salient visual regions like prominent image regions. In this paper, we attach more importance to coarse-grained elements containing high-density information and consistent semantics, which are valuable for document understanding. At first, a document graph is proposed to model complex relationships among multi-grained multimodal elements, in which salient visual regions are detected by a cluster-based method. Then, a multi-grained multimodal Transformer called mmLayout is proposed to incorporate coarse-grained information into existing pre-trained fine-grained multimodal Transformers based on the graph. In mmLayout, coarse-grained information is aggregated from fine-grained, and then, after further processing, is fused back into fine-grained for final prediction. Furthermore, common sense enhancement is introduced to exploit the semantic information of natural lexical units. Experimental results on four tasks, including information extraction and document question answering, show that our method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters. Qualitative analyses show that our method can capture consistent semantics in coarse-grained elements.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/9NR7ITF2/Wang_et_al_(2022)_mmLayout.pdf}
}

@article{dejean_2022,
  title = {{{LayoutXLM}} vs. {{GNN}}: {{An Empirical Evaluation}} of {{Relation Extraction}} for {{Documents}}},
  shorttitle = {{{LayoutXLM}} vs. {{GNN}}},
  author = {D'ejean, Herv'e and Clinchant, S. and Meunier, J.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2206.10304},
  abstract = {This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). For this benchmark, we use the XFUND dataset, released along with LayoutXLM. While both models reach similar results, they both exhibit very different charac-teristics. This raises the question on how to integrate various modalities in a neural network: by merging all modalities thanks to additional pretraining (LayoutXLM), or in a cascaded way (ECN). We conclude by discussing some methodological issues that must be considered for new datasets and task definition in the do-main of Information Extraction with complex documents.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/YMUZRP7Q/D'ejean_et_al_(2022)_LayoutXLM_vs.pdf}
}

@inproceedings{li_2022,
  title = {{{MarkupLM}}: {{Pre-training}} of {{Text}} and {{Markup Language}} for {{Visually Rich Document Understanding}}},
  shorttitle = {{{MarkupLM}}},
  booktitle = {{{ACL}}},
  author = {Li, Junlong and Xu, Yiheng and Cui, Lei and Wei, Furu},
  year = {2022},
  doi = {10.18653/v1/2022.acl-long.420},
  abstract = {This paper proposes MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Multimodal pre-training with text, layout, and image has made significant progress for Visually Rich Document Understanding (VRDU), especially the fixed-layout documents such as scanned document images. While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks. The pre-trained model and code will be publicly available at https://aka.ms/markuplm.},
  annotation = {10 citations (Semantic Scholar/DOI) [2022-10-28]\\
GSCC: 0000006},
  file = {/Users/chenghao/Zotero/storage/7QKFCIPS/Li_et_al_(2022)_MarkupLM.pdf}
}

@article{biten_2022,
  title = {{{OCR-IDL}}: {{OCR Annotations}} for {{Industry Document Library Dataset}}},
  shorttitle = {{{OCR-IDL}}},
  author = {Biten, Ali Furkan and Tito, Rub{\`e}n P{\'e}rez and G{\'o}mez, Llu{\'i}s and Valveny, Ernest and Karatzas, Dimosthenis},
  year = {2022},
  journal = {ArXiv},
  abstract = {This work makes public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. Pretraining has proven successful in Document Intelligence tasks where deluge of documents are used to pretrain the models only later to be finetuned on downstream tasks. One of the problems of the pretraining approaches is the inconsistent usage of pretraining data with different OCR engines leading to incomparable results between models. In other words, it is not obvious whether the performance gain is coming from diverse usage of amount of data and distinct OCR engines or from the proposed models. To remedy the problem, we make public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. The contributed dataset (OCR-IDL) has an estimated monetary value over 20K US\$. It is our hope that OCR-IDL can be a starting point for future works on Document Intelligence. All of our data and its collection process with the annotations can be found in https://github.com/furkanbiten/idl\_data.},
  annotation = {GSCC: 0000003},
  file = {/Users/chenghao/Zotero/storage/6L39XAAA/Biten_et_al_(2022)_OCR-IDL.pdf}
}

@article{garncarek_2021,
  title = {{{LAMBERT}}: {{Layout-Aware Language Modeling}} for {{Information Extraction}}},
  shorttitle = {{{LAMBERT}}},
  author = {Garncarek, Lukasz and Powalski, Rafal and Stanis{\l}awek, Tomasz and Topolski, Bartosz and Halama, Piotr and Turski, M. and Grali'nski, Filip},
  year = {2021},
  journal = {undefined},
  urldate = {2022-10-28},
  abstract = {This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). For this benchmark, we use the XFUND dataset, released along with LayoutXLM. While both models reach similar results, they both exhibit very different charac-teristics. This raises the question on how to integrate various modalities in a neural network: by merging all modalities thanks to additional pretraining (LayoutXLM), or in a cascaded way (ECN). We conclude by discussing some methodological issues that must be considered for new datasets and task definition in the do-main of Information Extraction with complex documents.},
  langid = {english},
  annotation = {GSCC: 0000031},
  file = {/Users/chenghao/Zotero/storage/9GPAHSAZ/Garncarek_et_al_(2021)_LAMBERT.pdf;/Users/chenghao/Zotero/storage/L2WQW7V9/f344647eef878684ad231f804bca20da068d9736.html}
}

@article{mathew_2020,
  title = {Document {{Visual Question Answering Challenge}} 2020},
  author = {Mathew, Minesh and Tito, Rub{\`e}n P{\'e}rez and Karatzas, Dimosthenis and Manmatha, R. and Jawahar, C. V.},
  year = {2020},
  journal = {ArXiv},
  abstract = {This paper presents results of Document Visual Question Answering Challenge organized as part of "Text and Documents in the Deep Learning Era" workshop, in CVPR 2020, with results of two tasks concerned with asking questions on a single document image. This paper presents results of Document Visual Question Answering Challenge organized as part of "Text and Documents in the Deep Learning Era" workshop, in CVPR 2020. The challenge introduces a new problem - Visual Question Answering on document images. The challenge comprised two tasks. The first task concerns with asking questions on a single document image. On the other hand, the second task is set as a retrieval task where the question is posed over a collection of images. For the task 1 a new dataset is introduced comprising 50,000 questions-answer(s) pairs defined over 12,767 document images. For task 2 another dataset has been created comprising 20 questions over 14,362 document images which share the same document template.},
  annotation = {GSCC: 0000023},
  file = {/Users/chenghao/Zotero/storage/AEEJ27EA/Mathew_et_al_(2020)_Document_Visual_Question_Answering_Challenge_2020.pdf}
}

@article{davis_2022,
  title = {End-to-End {{Document Recognition}} and {{Understanding}} with {{Dessurt}}},
  author = {Davis, Brian L. and Morse, B. and Price, Bryan and Tensmeyer, Chris and Wigington, Curtis and Morariu, Vlad I.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2203.16618},
  abstract = {Dessurt is an end-to-end architecture that performs text recognition in addition to the document understanding, it does not require an external recognition model as prior methods do and is able to handle a variety of document domains and tasks. . We introduce Dessurt, a relatively simple document understanding transformer capable of being fine-tuned on a greater variety of document tasks than prior methods. It receives a document image and task string as input and generates arbitrary text autoregressively as output. Because Dessurt is an end-to-end architecture that performs text recognition in addition to the document understanding, it does not require an external recognition model as prior methods do. Dessurt is a more flexible model than prior methods and is able to handle a variety of document domains and tasks. We show that this model is effective at 9 different dataset-task combinations.},
  annotation = {GSCC: 0000001},
  file = {/Users/chenghao/Zotero/storage/IF2BZV6H/Davis_et_al_(2022)_End-to-end_Document_Recognition_and_Understanding_with_Dessurt.pdf}
}

@inproceedings{skalicky_2022,
  title = {Business {{Document Information Extraction}}: {{Towards Practical Benchmarks}}},
  shorttitle = {Business {{Document Information Extraction}}},
  booktitle = {{{CLEF}}},
  author = {Skalick{\'y}, Maty{\'a}s and Simsa, Step{\'a}n and U{\v r}i{\v c}{\'a}{\v r}, Michal and {\v S}ulc, Milan},
  year = {2022},
  doi = {10.48550/arXiv.2206.11229},
  abstract = {There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive, and potential sources of available documents including synthetic data are discussed. Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.},
  annotation = {GSCC: 0000132},
  file = {/Users/chenghao/Zotero/storage/CVJE2559/Skalick_et_al_(2022)_Business_Document_Information_Extraction.pdf}
}

@inproceedings{tito_2021,
  title = {Document {{Collection Visual Question Answering}}},
  booktitle = {{{ICDAR}}},
  author = {Tito, Rub{\`e}n P{\'e}rez and Karatzas, Dimosthenis and Valveny, Ernest},
  year = {2021},
  doi = {10.1007/978-3-030-86331-9_50},
  abstract = {This work introduces Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context useful for their interpretation. To address this problem, we introduce Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.},
  annotation = {GSCC: 0000066},
  file = {/Users/chenghao/Zotero/storage/GY2FX2NF/Tito_et_al_(2021)_Document_Collection_Visual_Question_Answering.pdf}
}

@article{mcdonald_2022,
  title = {Detect, {{Retrieve}}, {{Comprehend}}: {{A Flexible Framework}} for {{Zero-Shot Document-Level Question Answering}}},
  shorttitle = {Detect, {{Retrieve}}, {{Comprehend}}},
  author = {McDonald, T. and Tsan, Brian and Saini, Amar and Ordo{\~n}ez, Juanita and Gutierrez, Luis and Nguyen, Phan-Anh-Huy and Mason, Blake and Ng, Brenda},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2210.01959},
  abstract = {This work presents a three-stage document QA approach: text extraction from PDF; evidence retrieval from extracted texts to form well-posed contexts; and QA to extract knowledge from contexts to return high-quality an- swers -- extractive, abstractive, or Boolean. Businesses generate thousands of documents that communi- cate their strategic vision and provide details of key prod-ucts, services, entities, and processes. Knowledge workers then face the laborious task of reading these documents to identify, extract, and synthesize information relevant to their organizational goals. To automate information gather-ing, question answering (QA) offers a flexible framework where human-posed questions can be adapted to extract diverse knowledge. Finetuning QA systems requires access to labeled data (tuples of context, question and answer). How-ever, data curation for document QA is uniquely challeng- ing because the context (i.e. answer evidence passage) needs to be retrieved from potentially long, ill-formatted docu- ments. Existing QA datasets sidestep this challenge by providing short, well-defined contexts that are unrealistic in real- world applications. We present a three-stage document QA approach: (1) text extraction from PDF; (2) evidence retrieval from extracted texts to form well-posed contexts; (3) QA to extract knowledge from contexts to return high-quality an- swers -- extractive, abstractive, or Boolean. Using Q ASPER for evaluation, our detect-retrieve-comprehend (DRC) sys- tem achieves a +6.25 improvement in Answer- F 1 over existing baselines while delivering superior context selection. Our results demonstrate that DRC holds tremendous promise as a flexible framework for practical document QA.},
  annotation = {GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/ZAPHE3H8/McDonald_et_al_(2022)_Detect,_Retrieve,_Comprehend.pdf}
}

@article{cui_2021,
  title = {Document {{AI}}: {{Benchmarks}}, {{Models}} and {{Applications}}},
  shorttitle = {Document {{AI}}},
  author = {Cui, Lei and Xu, Yiheng and Lv, Tengchao and Wei, Furu},
  year = {2021},
  journal = {ArXiv},
  abstract = {Early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods are introduced, and future directions for Document AI research are looked into. Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.},
  annotation = {GSCC: 0000873},
  file = {/Users/chenghao/Zotero/storage/6LPU7CZK/Cui_et_al_(2021)_Document_AI.pdf}
}

@inproceedings{nguyen_2021a,
  title = {A {{Span Extraction Approach}} for {{Information Extraction}} on {{Visually-Rich Documents}}},
  booktitle = {{{ICDAR Workshops}}},
  author = {Nguyen, Tuan-Anh Dang and Vu, Hieu M. and Son, Nguyen Hong and Nguyen, Minh-Tien},
  year = {2021},
  doi = {10.1007/978-3-030-86159-9_25},
  abstract = {A new query-based IE model that employs span extraction instead of using the common sequence labeling approach is introduced and a new training task focusing on modelling the relationships among semantic entities within a document is proposed. Information extraction (IE) from visually-rich documents (VRDs) has achieved SOTA performance recently thanks to the adaptation of Transformer-based language models, which demonstrates great potential of pre-training methods. In this paper, we present a new approach to improve the capability of language model pre-training on VRDs. Firstly, we introduce a new IE model that is query-based and employs the span extraction formulation instead of the commonly used sequence labelling approach. Secondly, to further extend the span extraction formulation, we propose a new training task which focuses on modelling the relationships between semantic entities within a document. This task enables the spans to be extracted recursively and can be used as both a pre-training objective as well as an IE downstream task. Evaluation on various datasets of popular business documents (invoices, receipts) shows that our proposed method can improve the performance of existing models significantly, while providing a mechanism to accumulate model knowledge from multiple downstream IE tasks.},
  annotation = {GSCC: 0000003},
  file = {/Users/chenghao/Zotero/storage/TK7T5JTG/Nguyen_et_al_(2021)_A_Span_Extraction_Approach_for_Information_Extraction_on_Visually-Rich_Documents.pdf}
}

@misc{khashabi_2020,
  title = {{{UnifiedQA}}: {{Crossing Format Boundaries With}} a {{Single QA System}}},
  shorttitle = {{{UnifiedQA}}},
  author = {Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  year = {2020},
  month = oct,
  number = {arXiv:2005.00700},
  eprint = {2005.00700},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.00700},
  urldate = {2022-10-31},
  abstract = {Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/XET925BI/Khashabi_et_al_(2020)_UnifiedQA.pdf;/Users/chenghao/Zotero/storage/RHM7IZLN/2005.html}
}

@misc{klaiman_2021,
  title = {{{DocReader}}: {{Bounding-Box Free Training}} of a {{Document Information Extraction Model}}},
  shorttitle = {{{DocReader}}},
  author = {Klaiman, Shachar and Lehne, Marius},
  year = {2021},
  month = may,
  number = {arXiv:2105.04313},
  eprint = {2105.04313},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.04313},
  urldate = {2022-10-31},
  abstract = {Information extraction from documents is a ubiquitous first step in many business applications. During this step, the entries of various fields must first be read from the images of scanned documents before being further processed and inserted into the corresponding databases. While many different methods have been developed over the past years in order to automate the above extraction step, they all share the requirement of bounding-box or text segment annotations of their training documents. In this work we present DocReader, an end-to-end neural-network-based information extraction solution which can be trained using solely the images and the target values that need to be read. The DocReader can thus leverage existing historical extraction data, completely eliminating the need for any additional annotations beyond what is naturally available in existing human-operated service centres. We demonstrate that the DocReader can reach and surpass other methods which require bounding-boxes for training, as well as provide a clear path for continual learning during its deployment in production.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/2XZC5UL8/Klaiman_Lehne_(2021)_DocReader.pdf;/Users/chenghao/Zotero/storage/LJKC8CWT/2105.html}
}

@misc{xu_2021,
  title = {{{LayoutXLM}}: {{Multimodal Pre-training}} for {{Multilingual Visually-rich Document Understanding}}},
  shorttitle = {{{LayoutXLM}}},
  author = {Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Wei, Furu},
  year = {2021},
  month = sep,
  number = {arXiv:2104.08836},
  eprint = {2104.08836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08836},
  urldate = {2022-10-31},
  abstract = {Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also introduce a multilingual form understanding benchmark dataset named XFUND, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled for each language. Experiment results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset. The pre-trained LayoutXLM model and the XFUND dataset are publicly available at https://aka.ms/layoutxlm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/H5VEIFM8/Xu_et_al_(2021)_LayoutXLM.pdf;/Users/chenghao/Zotero/storage/WMR73VUP/2104.html}
}

@misc{mathew_2021,
  title = {{{DocVQA}}: {{A Dataset}} for {{VQA}} on {{Document Images}}},
  shorttitle = {{{DocVQA}}},
  author = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, C. V.},
  year = {2021},
  month = jan,
  number = {arXiv:2007.00398},
  eprint = {2007.00398},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.00398},
  urldate = {2022-10-31},
  abstract = {We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36\% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {/Users/chenghao/Zotero/storage/NIMCPAHB/Mathew_et_al_(2021)_DocVQA.pdf;/Users/chenghao/Zotero/storage/7YRDRMHG/2007.html}
}

@misc{hong_2022,
  title = {{{BROS}}: {{A Pre-trained Language Model Focusing}} on {{Text}} and {{Layout}} for {{Better Key Information Extraction}} from {{Documents}}},
  shorttitle = {{{BROS}}},
  author = {Hong, Teakgyu and Kim, Donghyun and Ji, Mingi and Hwang, Wonseok and Nam, Daehyun and Park, Sungrae},
  year = {2022},
  month = apr,
  number = {arXiv:2108.04539},
  eprint = {2108.04539},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.04539},
  urldate = {2022-10-31},
  abstract = {Key information extraction (KIE) from document images requires understanding the contextual and spatial semantics of texts in two-dimensional (2D) space. Many recent studies try to solve the task by developing pre-trained language models focusing on combining visual features from document images with texts and their layout. On the other hand, this paper tackles the problem by going back to the basic: effective combination of text and layout. Specifically, we propose a pre-trained language model, named BROS (BERT Relying On Spatiality), that encodes relative positions of texts in 2D space and learns from unlabeled documents with area-masking strategy. With this optimized training scheme for understanding texts in 2D space, BROS shows comparable or better performance compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and SciTSR) without relying on visual features. This paper also reveals two real-world challenges in KIE tasks-(1) minimizing the error from incorrect text ordering and (2) efficient learning from fewer downstream examples-and demonstrates the superiority of BROS over previous methods. Code is available at https://github.com/clovaai/bros.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/L2HDGR8S/Hong_et_al_(2022)_BROS.pdf;/Users/chenghao/Zotero/storage/Z3R5QME2/2108.html}
}

@misc{pramanik_2022,
  title = {Towards a {{Multi-modal}}, {{Multi-task Learning}} Based {{Pre-training Framework}} for {{Document Representation Learning}}},
  author = {Pramanik, Subhojeet and Mujumdar, Shashank and Patel, Hima},
  year = {2022},
  month = jan,
  number = {arXiv:2009.14457},
  eprint = {2009.14457},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.14457},
  urldate = {2022-11-02},
  abstract = {Recent approaches in literature have exploited the multi-modal information in documents (text, layout, image) to serve specific downstream document tasks. However, they are limited by their - (i) inability to learn cross-modal representations across text, layout and image dimensions for documents and (ii) inability to process multi-page documents. Pre-training techniques have been shown in Natural Language Processing (NLP) domain to learn generic textual representations from large unlabelled datasets, applicable to various downstream NLP tasks. In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation applicable to various downstream document tasks. Specifically, we introduce Document Topic Modelling and Document Shuffle Prediction as novel pre-training tasks to learn rich image representations along with the text and layout representations for documents. We utilize the Longformer network architecture as the backbone to encode the multi-modal information from multi-page documents in an end-to-end fashion. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We evaluate our framework on different standard document datasets and conduct exhaustive experiments to compare performance against various ablations of our framework and state-of-the-art baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/HNSNJFM7/Pramanik_et_al_(2022)_Towards_a_Multi-modal,_Multi-task_Learning_based_Pre-training_Framework_for.pdf;/Users/chenghao/Zotero/storage/MPB9TDLX/2009.html}
}

@inproceedings{sachan_2021,
  title = {End-to-{{End Training}} of {{Multi-Document Reader}} and {{Retriever}} for {{Open-Domain Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sachan, Devendra Singh and Reddy, Siva and Hamilton, William L. and Dyer, Chris and Yogatama, Dani},
  year = {2021},
  month = oct,
  urldate = {2022-11-02},
  abstract = {We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3\% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/532JWNMA/Sachan_et_al_(2021)_End-to-End_Training_of_Multi-Document_Reader_and_Retriever_for_Open-Domain.pdf;/Users/chenghao/Zotero/storage/IRZWIX25/forum.html}
}

@article{anonymous_2022a,
  title = {Layout-{{Aware Neural Model}} for {{Resolving Hierarchical Table Structure}}},
  author = {Anonymous},
  year = {2022},
  month = jan,
  urldate = {2022-11-04},
  abstract = {While many pipelines for extracting information from tables assume simple table structure, tables in the financial domain frequently have complex, hierarchical structure. The main example would be parent-child relationships between header cells. Most prior datasets of tables annotated from images or .pdf and most models for extracting table structure concentrate on the problems of table, cell, row, and column bounding box extraction. The area of fine-grained table structure remains relatively unexplored. In this study, we present a dataset of 887 tables, manually labeled for cell types and column hierarchy relations. The tables are selected from IBM FinTabNet, a much larger dataset of more than 100,000 financial tables having cell, row, and column bounding boxes extracted by deep learning, but not including semantic cell type or cell-to-cell relation labels, which we add. Selection of these 887 tables is performed using heuristics which result in a much larger proportion, roughly half, of the selected tables having complex hierarchical structure, than a random sample from FinTabNet. Further, we fine-tune models based on LayoutLM on the cell-type classification task and on the identification of hiearchical relations among column headers. We achieve F1 scores of 95\% and 70\% on the respective tasks. Finally, we use the trained model to create soft labels for the entirety of FinTabNet.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/4FWQSQA4/Anonymous_(2022)_Layout-Aware_Neural_Model_for_Resolving_Hierarchical_Table_Structure.pdf;/Users/chenghao/Zotero/storage/SWFQ2AFB/forum.html}
}

@misc{meng_2022,
  title = {Locating and {{Editing Factual Associations}} in {{GPT}}},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2022},
  month = oct,
  number = {arXiv:2202.05262},
  eprint = {2202.05262},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.05262},
  urldate = {2022-11-13},
  abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  annotation = {5 citations (Semantic Scholar/arXiv) [2022-11-12]},
  file = {/Users/chenghao/Zotero/storage/A5ZJ74VU/Meng_et_al_(2022)_Locating_and_Editing_Factual_Associations_in_GPT.pdf}
}

@inproceedings{shah_2022,
  title = {Situating {{Search}}},
  booktitle = {{{ACM SIGIR Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Shah, Chirag and Bender, Emily M.},
  year = {2022},
  month = mar,
  pages = {221--232},
  publisher = {ACM},
  address = {Regensburg Germany},
  doi = {10.1145/3498366.3505816},
  urldate = {2022-11-17},
  abstract = {Search systems, like many other applications of machine learning, have become increasingly complex and opaque. The notions of relevance, usefulness, and trustworthiness with respect to information were already overloaded and often difficult to articulate, study, or implement. Newly surfaced proposals that aim to use large language models to generate relevant information for a user's needs pose even greater threat to transparency, provenance, and user interactions in a search system. In this perspective paper we revisit the problem of search in the larger context of information seeking and argue that removing or reducing interactions in an effort to retrieve presumably more relevant information can be detrimental to many fundamental aspects of search, including information verification, information literacy, and serendipity. In addition to providing suggestions for counteracting some of the potential problems posed by such models, we present a vision for search systems that are intelligent and effective, while also providing greater transparency and accountability.},
  isbn = {978-1-4503-9186-3},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/DOI) [2022-11-16]},
  file = {/Users/chenghao/Zotero/storage/VV96L4WX/Shah_Bender_(2022)_Situating_Search.pdf}
}

@misc{taylor_2022,
  title = {Galactica: {{A Large Language Model}} for {{Science}}},
  shorttitle = {Galactica},
  author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09085},
  eprint = {2211.09085},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.09085},
  urldate = {2022-11-21},
  abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-11-20]},
  file = {/Users/chenghao/Zotero/storage/9TDKTR7G/Taylor_et_al_(2022)_Galactica.pdf}
}

@misc{raschka_2020,
  title = {Model {{Evaluation}}, {{Model Selection}}, and {{Algorithm Selection}} in {{Machine Learning}}},
  author = {Raschka, Sebastian},
  year = {2020},
  month = nov,
  number = {arXiv:1811.12808},
  eprint = {1811.12808},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.12808},
  urldate = {2022-11-22},
  abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,nosource,Statistics - Machine Learning},
  annotation = {\{"size": 1955226, "pages": 49, "previous": "390 citations (Semantic Scholar/arXiv) [2022-11-21]{\textbackslash}narXiv:1811.12808 [cs, stat]{\textbackslash}nversion: 2"\}},
  file = {/Users/chenghao/Zotero/storage/5647EYZI/Raschka - 2020 - Model Evaluation, Model Selection, and Algorithm S.pdf}
}

@article{_,
  title = {[{{PDF}}] {{Noise-Robust De-Duplication}} at {{Scale}} {\textbar} {{Semantic Scholar}}},
  doi = {10.48550/arXiv.2210.04261},
  urldate = {2022-11-22},
  abstract = {It is shown that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours, and the neural approaches significantly outperform hashing and N -gram overlap. Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on N -grams. Limited efforts have been made to evaluate how well N -gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and N -gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a ``re-rank'' style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and N -gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. The public release of our NEWS-COPY de-duplication dataset will facilitate further research and applications.},
  langid = {english},
  keywords = {nosource},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-11-21]},
  file = {/Users/chenghao/Zotero/storage/NRPYCGI6/[PDF]_Noise-Robust_De-Duplication_at_Scale_Semantic_Scholar.pdf}
}

@article{breiman_,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  author = {Breiman, Leo},
  journal = {THE TWO CULTURES},
  pages = {33},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  langid = {english},
  annotation = {\{"size": 300414, "pages": 33, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/PXPNIHWQ/Breiman_(Statistical_Modeling.pdf}
}

@inproceedings{frobe_2021,
  title = {{{CopyCat}}: {{Near-Duplicates Within}} and {{Between}} the {{ClueWeb}} and the {{Common Crawl}}},
  shorttitle = {{{CopyCat}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Fr{\"o}be, Maik and Bevendorff, Janek and Gienapp, Lukas and V{\"o}lske, Michael and Stein, Benno and Potthast, Martin and Hagen, Matthias},
  year = {2021},
  month = jul,
  pages = {2398--2404},
  publisher = {ACM},
  address = {Virtual Event Canada},
  doi = {10.1145/3404835.3463246},
  urldate = {2022-11-24},
  abstract = {The amount of near-duplicates in web crawls like the ClueWeb or Common Crawl demands from their users either to develop a preprocessing pipeline for deduplication, which is costly both computationally and in person hours, or accepting the undesired effects that near-duplicates have on reliability and validity of experiments. We introduce ChatNoir-CopyCat-21, which simplifies deduplication significantly. It comes in two parts: (1) A compilation of near-duplicate documents within the ClueWeb09, the ClueWeb12, and two Common Crawl snapshots, as well as between selections of these crawls, and (2) a software library that implements the deduplication of arbitrary document sets. Our analysis shows that 14--52 \% of the documents within a crawl and around 0.7--2.5 \% between the crawls are near-duplicates. Two showcases demonstrate the application and usefulness of our resource.},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  keywords = {/unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2022-11-24]},
  file = {/Users/chenghao/Zotero/storage/FJDK5F2H/Frbe_et_al_(2021)_CopyCat.pdf}
}

@misc{overwijk_2022,
  title = {{{ClueWeb22}}: 10 {{Billion Web Documents}} with {{Rich Information}}},
  shorttitle = {{{ClueWeb22}}},
  author = {Overwijk, Arnold and Xiong, Chenyan and Liu, Xiao and VandenBerg, Cameron and Callan, Jamie},
  year = {2022},
  month = nov,
  number = {arXiv:2211.15848},
  eprint = {2211.15848},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.15848},
  urldate = {2022-11-30},
  abstract = {ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10 billion web pages affiliated with rich information. Its design was influenced by the need for a high quality, large scale web corpus to support a range of academic and industry research, for example, in information systems, retrieval-augmented AI systems, and model pretraining. Compared with earlier ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of higher-quality, and aligned with the document distributions in commercial web search. Besides raw HTML, ClueWeb22 includes rich information about the web pages provided by industry-standard document understanding systems, including the visual representation of pages rendered by a web browser, parsed HTML structure information from a neural network parser, and pre-processed cleaned document text to lower the barrier to entry. Many of these signals have been widely used in industry but are available to the research community for the first time at this scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-11-30]},
  file = {/Users/chenghao/Zotero/storage/BARSZZUK/Overwijk_et_al_(2022)_ClueWeb22.pdf}
}

@misc{tang_2022,
  title = {Unifying {{Vision}}, {{Text}}, and {{Layout}} for {{Universal Document Processing}}},
  author = {Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  year = {2022},
  month = dec,
  number = {arXiv:2212.02623},
  doi = {10.48550/arXiv.2212.02623},
  urldate = {2022-12-07},
  abstract = {We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 9 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and websites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark (DUE).},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 9015523, "pages": 16, "previous": "arXiv:2212.02623 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/VD79KSPF/Tang_et_al_(2022)_Unifying_Vision,_Text,_and_Layout_for_Universal_Document_Processing.pdf}
}

@misc{jiang_2022,
  title = {Retrieval as {{Attention}}: {{End-to-end Learning}} of {{Retrieval}} and {{Reading}} within a {{Single Transformer}}},
  shorttitle = {Retrieval as {{Attention}}},
  author = {Jiang, Zhengbao and Gao, Luyu and Araki, Jun and Ding, Haibo and Wang, Zhiruo and Callan, Jamie and Neubig, Graham},
  year = {2022},
  month = dec,
  number = {arXiv:2212.02027},
  eprint = {2212.02027},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.02027},
  urldate = {2022-12-10},
  abstract = {Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents to generate answers. Retrievers and readers are usually modeled separately, which necessitates a cumbersome implementation and is hard to train and adapt in an end-to-end fashion. In this paper, we revisit this design and eschew the separate architecture and training in favor of a single Transformer that performs Retrieval as Attention (ReAtt), and end-to-end training solely based on supervision from the end QA task. We demonstrate for the first time that a single model trained end-to-end can achieve both competitive retrieval and QA performance, matching or slightly outperforming state-of-the-art separately trained retrievers and readers. Moreover, end-to-end adaptation significantly boosts its performance on out-of-domain datasets in both supervised and unsupervised settings, making our model a simple and adaptable solution for knowledge-intensive tasks. Code and models are available at https://github.com/jzbjyb/ReAtt.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-09]\\
0 citations (Semantic Scholar/DOI) [2022-12-09]},
  file = {/Users/chenghao/Zotero/storage/7EJ7BXX8/Jiang_et_al_(2022)_Retrieval_as_Attention.pdf}
}

@misc{somepalli_2022,
  title = {Diffusion {{Art}} or {{Digital Forgery}}? {{Investigating Data Replication}} in {{Diffusion Models}}},
  shorttitle = {Diffusion {{Art}} or {{Digital Forgery}}?},
  author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03860},
  eprint = {2212.03860},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.03860},
  urldate = {2022-12-09},
  abstract = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they stealing content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-09]},
  file = {/Users/chenghao/Zotero/storage/65JD2YNJ/Somepalli_et_al_(2022)_Diffusion_Art_or_Digital_Forgery.pdf}
}

@misc{andreas_2022,
  title = {Language {{Models}} as {{Agent Models}}},
  author = {Andreas, Jacob},
  year = {2022},
  month = dec,
  number = {arXiv:2212.01681},
  eprint = {2212.01681},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.01681},
  urldate = {2022-12-09},
  abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Multiagent Systems},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-09]\\
0 citations (Semantic Scholar/DOI) [2022-12-09]},
  file = {/Users/chenghao/Zotero/storage/FUVKUZJR/Andreas_(2022)_Language_Models_as_Agent_Models.pdf}
}

@misc{shanahan_2022,
  title = {Talking {{About Large Language Models}}},
  author = {Shanahan, Murray},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03551},
  eprint = {2212.03551},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-12-11},
  abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as ``knows'', ``believes'', and ``thinks'', when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-11]},
  file = {/Users/chenghao/Zotero/storage/LN5A4MU8/Shanahan_(2022)_Talking_About_Large_Language_Models.pdf}
}

@misc{akiki_2022,
  title = {{{BigScience}}: {{A Case Study}} in the {{Social Construction}} of a {{Multilingual Large Language Model}}},
  shorttitle = {{{BigScience}}},
  author = {Akiki, Christopher and Pistilli, Giada and Mieskes, Margot and Gall{\'e}, Matthias and Wolf, Thomas and Ili{\'c}, Suzana and Jernite, Yacine},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04960},
  eprint = {2212.04960},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.04960},
  urldate = {2022-12-12},
  abstract = {The BigScience Workshop was a value-driven initiative that spanned one and half years of interdisciplinary research and culminated in the creation of ROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the largest multilingual language models to date. In addition to the technical outcomes and artifacts, the workshop fostered multidisciplinary collaborations around large models, datasets, and their analysis. This in turn led to a wide range of research publications spanning topics from ethics to law, data governance, modeling choices and distributed training. This paper focuses on the collaborative research aspects of BigScience and takes a step back to look at the challenges of large-scale participatory research, with respect to participant diversity and the tasks required to successfully carry out such a project. Our main goal is to share the lessons we learned from this experience, what we could have done better and what we did well. We show how the impact of such a social approach to scientific research goes well beyond the technical artifacts that were the basis of its inception.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/chenghao/Zotero/storage/7MIQN5U7/Akiki_et_al_(2022)_BigScience.pdf}
}

@article{bai_,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and {Telleen-Lawton}, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through selfimprovement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as `Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use `RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but nonevasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/4ULNGVNR/Bai_et_al_(Constitutional_AI.pdf}
}

@article{bohnet_2022,
  title = {Attributed {{Question Answering}}: {{Evaluation}} and {{Modeling}} for {{Attributed Large Language Models}}},
  shorttitle = {Attributed {{Question Answering}}},
  author = {Bohnet, Bernd and Tran, Vinh Q. and Verga, Pat and Aharoni, Roee and Andor, Daniel and Soares, Livio Baldini and Eisenstein, Jacob and Ganchev, Kuzman and Herzig, Jonathan and Hui, Kai and Kwiatkowski, Tom and Ma, Ji and Ni, Jianmo and Schuster, Tal and Cohen, William W. and Collins, Michael and Das, Dipanjan and Metzler, Donald and Petrov, Slav and Webster, Kellie},
  year = {2022},
  month = dec,
  doi = {10.48550/arXiv.2212.08037},
  urldate = {2022-12-18},
  abstract = {Large language models (LLMs) have shown impressive results across a variety of tasks while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial for both system developers and users in this setting. We propose and study Attributed QA as a key first step in the development of attributed LLMs. We develop a reproducable evaluation framework for the task, using human annotations as a gold standard and a correlated automatic metric that we show is suitable for development settings. We describe and benchmark a broad set of architectures for the task. Our contributions give some concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third key question (How to build LLMs with attribution?).},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-17]},
  file = {/Users/chenghao/Zotero/storage/UX9BWF25/Bohnet_et_al_(2022)_Attributed_Question_Answering.pdf}
}

@misc{opitz_2022,
  title = {{{SBERT}} Studies {{Meaning Representations}}: {{Decomposing Sentence Embeddings}} into {{Explainable Semantic Features}}},
  shorttitle = {{{SBERT}} Studies {{Meaning Representations}}},
  author = {Opitz, Juri and Frank, Anette},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07023},
  eprint = {2206.07023},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.07023},
  urldate = {2022-12-19},
  abstract = {Models based on large-pretrained language models, such as S(entence)BERT, provide effective and efficient sentence embeddings that show high correlation to human similarity ratings, but lack interpretability. On the other hand, graph metrics for graph-based meaning representations (e.g., Abstract Meaning Representation, AMR) can make explicit the semantic aspects in which two sentences are similar. However, such metrics tend to be slow, rely on parsers, and do not reach state-of-the-art performance when rating sentence similarity. In this work, we aim at the best of both worlds, by learning to induce \$S\$emantically \$S\$tructured \$S\$entence BERT embeddings (S\${\textasciicircum}3\$BERT). Our S\${\textasciicircum}3\$BERT embeddings are composed of explainable sub-embeddings that emphasize various semantic sentence features (e.g., semantic roles, negation, or quantification). We show how to i) learn a decomposition of the sentence embeddings into semantic features, through approximation of a suite of interpretable AMR graph metrics, and how to ii) preserve the overall power of the neural embeddings by controlling the decomposition learning process with a second objective that enforces consistency with the similarity ratings of an SBERT teacher model. In our experimental studies, we show that our approach offers interpretability -- while fully preserving the effectiveness and efficiency of the neural sentence embeddings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/3ZZ5EDHY/Opitz_Frank_(2022)_SBERT_studies_Meaning_Representations.pdf}
}

@inproceedings{laurencon_2022,
  title = {The {{BigScience ROOTS Corpus}}: {{A}} 1.{{6TB Composite Multilingual Dataset}}},
  shorttitle = {The {{BigScience ROOTS Corpus}}},
  booktitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Lauren{\c c}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Scao, Teven Le and Werra, Leandro Von and Mou, Chenghao and Ponferrada, Eduardo Gonz{\'a}lez and Nguyen, Huu and Frohberg, J{\"o}rg and {\v S}a{\v s}ko, Mario and Lhoest, Quentin and {McMillan-Major}, Angelina and Dupont, G{\'e}rard and Biderman, Stella and Rogers, Anna and Allal, Loubna Ben and Toni, Francesco De and Pistilli, Giada and Nguyen, Olivier and Nikpoor, Somaieh and Masoud, Maraim and Colombo, Pierre and de la Rosa, Javier and Villegas, Paulo and Thrush, Tristan and Longpre, Shayne and Nagel, Sebastian and Weber, Leon and Mu{\~n}oz, Manuel Romero and Zhu, Jian and Strien, Daniel Van and Alyafeai, Zaid and Almubarak, Khalid and Chien, Vu Minh and {Gonzalez-Dios}, Itziar and Soroa, Aitor and Lo, Kyle and Dey, Manan and Suarez, Pedro Ortiz and Gokaslan, Aaron and Bose, Shamik and Adelani, David Ifeoluwa and Phan, Long and Tran, Hieu and Yu, Ian and Pai, Suhas and Chim, Jenny and Lepercq, Violette and Ilic, Suzana and Mitchell, Margaret and Luccioni, Sasha and Jernite, Yacine},
  year = {2022},
  month = oct,
  urldate = {2022-12-20},
  abstract = {As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {nosource},
  file = {/Users/chenghao/Zotero/storage/KK7EIDKL/Laurenon et al. - 2022 - The BigScience ROOTS Corpus A 1.6TB Composite Multilingual Dataset.pdf}
}

@misc{kocetkov_2022,
  title = {The {{Stack}}: 3 {{TB}} of Permissively Licensed Source Code},
  shorttitle = {The {{Stack}}},
  author = {Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Ferrandis, Carlos Mu{\~n}oz and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and {von Werra}, Leandro and {de Vries}, Harm},
  year = {2022},
  month = nov,
  number = {arXiv:2211.15533},
  eprint = {2211.15533},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.15533},
  urldate = {2022-12-20},
  abstract = {Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,nosource},
  file = {/Users/chenghao/Zotero/storage/LA6EBPZS/Kocetkov et al. - 2022 - The Stack 3 TB of permissively licensed source code.pdf}
}

@misc{wang_2022,
  title = {Pretraining {{Without Attention}}},
  author = {Wang, Junxiong and Yan, Jing Nathan and Gu, Albert and Rush, Alexander M.},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10544},
  eprint = {2212.10544},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.10544},
  urldate = {2022-12-21},
  abstract = {Transformers have been essential to pretraining success in NLP. Other architectures have been used, but require attention layers to match benchmark accuracy. This work explores pretraining without attention. We test recently developed routing layers based on state-space models (SSM) and model architectures based on multiplicative gating. Used together these modeling choices have a large impact on pretraining accuracy. Empirically the proposed Bidirectional Gated SSM (BiGS) replicates BERT pretraining results without attention and can be extended to long-form pretraining of 4096 tokens without approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/9EJTKXBB/Wang_et_al_(2022)_Pretraining_Without_Attention.pdf}
}

@misc{riedl_2014,
  title = {The {{Lovelace}} 2.0 {{Test}} of {{Artificial Creativity}} and {{Intelligence}}},
  author = {Riedl, Mark O.},
  year = {2014},
  month = dec,
  number = {arXiv:1410.6142},
  eprint = {1410.6142},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1410.6142},
  urldate = {2022-12-22},
  abstract = {Observing that the creation of certain types of artistic artifacts necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an alternative to the Turing Test as a means of determining whether an agent is intelligent. The Lovelace 2.0 Test builds off prior tests of creativity and additionally provides a means of directly comparing the relative intelligence of different agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Intelligence},
  annotation = {42 citations (Semantic Scholar/arXiv) [2022-12-22]},
  file = {/Users/chenghao/Zotero/storage/ZJST9YRT/Riedl_(2014)_The_Lovelace_2.pdf}
}

@misc{perry_2022,
  title = {Do {{Users Write More Insecure Code}} with {{AI Assistants}}?},
  author = {Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
  year = {2022},
  month = dec,
  number = {arXiv:2211.03622},
  eprint = {2211.03622},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.03622},
  urldate = {2022-12-26},
  abstract = {We conduct the first large-scale user study examining how users interact with an AI Code assistant to solve a variety of security related tasks across different programming languages. Overall, we find that participants who had access to an AI assistant based on OpenAI's codex-davinci-002 model wrote significantly less secure code than those without access. Additionally, participants with access to an AI assistant were more likely to believe they wrote secure code than those without access to the AI assistant. Furthermore, we find that participants who trusted the AI less and engaged more with the language and format of their prompts (e.g. re-phrasing, adjusting temperature) provided code with fewer security vulnerabilities. Finally, in order to better inform the design of future AI-based Code assistants, we provide an in-depth analysis of participants' language and interaction behavior, as well as release our user interface as an instrument to conduct similar studies in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-12-26]\\
1 citations (Semantic Scholar/DOI) [2022-12-26]},
  file = {/Users/chenghao/Zotero/storage/VBN3HP3Z/Perry_et_al_(2022)_Do_Users_Write_More_Insecure_Code_with_AI_Assistants.pdf}
}

@inproceedings{raji_2021,
  title = {You {{Can}}'t {{Sit With Us}}: {{Exclusionary Pedagogy}} in {{AI Ethics Education}}},
  shorttitle = {You {{Can}}'t {{Sit With Us}}},
  author = {Raji, Inioluwa Deborah and Scheuerman, Morgan Klaus and Amironesei, Razvan},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {515--525},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445914},
  urldate = {2022-12-27},
  abstract = {Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)---and its implications for AI---has led to the current "ethics crisis". However, we claim that the current AI ethics education space relies on a form of "exclusionary pedagogy," where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as "ethical unicorns" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.},
  isbn = {978-1-4503-8309-7},
  annotation = {27 citations (Semantic Scholar/DOI) [2022-12-27]},
  file = {/Users/chenghao/Zotero/storage/4N7ZXGNU/Raji_et_al_(2021)_You_Can't_Sit_With_Us.pdf}
}

@misc{jurgenschmidhuber_2022,
  title = {Annotated {{History}} of {{Modern AI}} and {{Deep Learning}}},
  author = {{J{\"u}rgen Schmidhuber}},
  year = {2022},
  month = dec,
  keywords = {nosource},
  annotation = {\{"size": 6899575, "pages": 86, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/VI5H6374/Jrgen_Schmidhuber_(2022)_Annotated_History_of_Modern_AI_and_Deep_Learning.pdf}
}

@misc{lee_2022,
  title = {Do {{Language Models Plagiarize}}?},
  author = {Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
  year = {2022},
  month = mar,
  number = {arXiv:2203.07618},
  eprint = {2203.07618},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.07618},
  urldate = {2023-01-06},
  abstract = {Past literature has illustrated that language models do not fully understand the context and sensitivity of text and can sometimes memorize phrases or sentences present in their training sets. In this paper, we investigate whether they not only memorize but also plagiarize training samples when generating artificial texts. Our findings support that they, especially GPT-2, reuse particular pieces of texts from the training corpus with or without obfuscation. We have four main results: 1) language models with more capacity plagiarize more; 2) fine-tuned language models demonstrate differing patterns of plagiarism based on characteristics of auxiliary data; 3) sampling from truncated language modeling distributions tends to heighten the degree of plagiarism as opposed to temperature sampling, and 4) plagiarism in language models can have serious privacy consequences. Overall, our work implies that future research on neural language models should take precautions to avoid models plagiarizing their training datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {1 citations (Semantic Scholar/arXiv) [2023-01-05]\\
1 citations (Semantic Scholar/DOI) [2023-01-05]},
  file = {/Users/chenghao/Zotero/storage/RKF678WJ/Lee_et_al_(2022)_Do_Language_Models_Plagiarize.pdf}
}

@misc{geiping_2022,
  title = {Cramming: {{Training}} a {{Language Model}} on a {{Single GPU}} in {{One Day}}},
  shorttitle = {Cramming},
  author = {Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  month = dec,
  number = {arXiv:2212.14034},
  eprint = {2212.14034},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-07},
  abstract = {Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-01-06]},
  file = {/Users/chenghao/Zotero/storage/CY3R4SLI/Geiping_Goldstein_(2022)_Cramming.pdf}
}

@misc{xu_2023,
  title = {Why Do {{Nearest Neighbor Language Models Work}}?},
  author = {Xu, Frank F. and Alon, Uri and Neubig, Graham},
  year = {2023},
  month = jan,
  number = {arXiv:2301.02828},
  eprint = {2301.02828},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.02828},
  urldate = {2023-01-11},
  abstract = {Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-01-10]},
  file = {/Users/chenghao/Zotero/storage/UR5QTJJ5/Xu_et_al_(2023)_Why_do_Nearest_Neighbor_Language_Models_Work.pdf}
}

@misc{shi_2023,
  title = {Large {{Language Models Can Be Easily Distracted}} by {{Irrelevant Context}}},
  author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2302.00093},
  eprint = {2302.00093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00093},
  urldate = {2023-02-04},
  abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,nosource},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-02-04]},
  file = {/Users/chenghao/Zotero/storage/IV246Z3J/Shi_et_al_(2023)_Large_Language_Models_Can_Be_Easily_Distracted_by_Irrelevant_Context.pdf}
}

@article{schick_2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  doi = {10.48550/arXiv.2302.04761},
  urldate = {2023-02-10},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/WHLJ3MX3/Schick_et_al_(2023)_Toolformer.pdf}
}

@misc{bang_2023,
  title = {A {{Multitask}}, {{Multilingual}}, {{Multimodal Evaluation}} of {{ChatGPT}} on {{Reasoning}}, {{Hallucination}}, and {{Interactivity}}},
  author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04023},
  eprint = {2302.04023},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.04023},
  urldate = {2023-02-10},
  abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 21 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 64.33\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-02-09]},
  file = {/Users/chenghao/Zotero/storage/BZGLV6VK/Bang_et_al_(2023)_A_Multitask,_Multilingual,_Multimodal_Evaluation_of_ChatGPT_on_Reasoning,.pdf}
}

@misc{bowman_2022,
  title = {The {{Dangers}} of {{Underclaiming}}: {{Reasons}} for {{Caution When Reporting How NLP Systems Fail}}},
  shorttitle = {The {{Dangers}} of {{Underclaiming}}},
  author = {Bowman, Samuel R.},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08300},
  eprint = {2110.08300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08300},
  urldate = {2023-02-13},
  abstract = {Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/DCDGCVID/Bowman - 2022 - The Dangers of Underclaiming Reasons for Caution .pdf}
}

@misc{erscoi_2023,
  title = {Pygmalion {{Displacement}}: {{When Humanising AI Dehumanises Women}}},
  shorttitle = {Pygmalion {{Displacement}}},
  author = {Erscoi, Lelia and Kleinherenbrink, Annelies and Guest, Olivia},
  year = {2023},
  month = feb,
  publisher = {SocArXiv},
  doi = {10.31235/osf.io/jqxb6},
  urldate = {2023-02-19},
  abstract = {We use the myth of Pygmalion as a lens to investigate and frame the relationship between women and artificial intelligence (AI). Pygmalion was a legendary ancient king of Cyprus and sculptor. Having been repulsed by women, he used his skills to create a statue, which was imbued with life by the goddess Aphrodite. This can be seen as one of the primordial AI-like myths, wherein humanity creates intelligent life-like self-images to reproduce or replace ourselves. In addition, the myth prefigures historical and present gendered dynamics within the field of AI and between AI and society at large. Throughout history, the theme of women being replaced by inanimate objects (e.g. automata, algorithms) has been repeated, and continues to repeat in contemporary AI technologies. However, this socially detrimental pattern in technology --- what we dub Pygmalion displacement --- is often overlooked, whether due to naive excitement about new developments, or due to an unacknowledged sexist history of the field itself. As we demonstrate herein, Pygmalion displacement prefigures heavily, but in an unacknowledged way, in the original Turing test, the imitation game: a central thought experiment, foundational to AI. With women, and the feminine generally, being both dislocated and erased from and by technology, AI is and has been (presented as) created mainly by privileged men, subserving capitalist patriarchal ends. This poses serious dangers to women and other marginalised people. By tracing the historical and ongoing entwinement of femininity (from a patriarchal perspective) and AI, we aim to understand, make visible, and start a dialogue on the ways in which AI harms women.},
  langid = {american},
  keywords = {and Sexuality Studies,Arts and Humanities,Feminist,Gender,Psychology,Science and Technology Studies,Social and Behavioral Sciences,Theory and Philosophy,Women's Studies},
  file = {/Users/chenghao/Zotero/storage/GTB4XUC3/Erscoi_et_al_(2023)_Pygmalion_Displacement.pdf}
}

@misc{huang_2023,
  title = {Language {{Is Not All You Need}}: {{Aligning Perception}} with {{Language Models}}},
  shorttitle = {Language {{Is Not All You Need}}},
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  year = {2023},
  month = feb,
  number = {arXiv:2302.14045},
  eprint = {2302.14045},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.14045},
  urldate = {2023-02-28},
  abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 3743140, "pages": 26, "previous": "arXiv:2302.14045 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/FX4IH3JF/Huang_et_al_(2023)_Language_Is_Not_All_You_Need.pdf}
}

@misc{hermann_2023,
  title = {For {{Human-Like Models}}, {{Train}} on {{Human-Like Tasks}}},
  author = {Hermann, Katherine and Nayebi, Aran and van Steenkiste, Sjoerd and Jones, Matthew},
  year = {2023},
  month = mar,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/a35mt},
  urldate = {2023-03-04},
  abstract = {Bowers et al. (2022) express skepticism about deep neural networks (DNNs) as models of human vision due to DNNs' failures to account for results from psychological research. We argue that to fairly assess DNNs, we must first train them on more human-like tasks which we hypothesize will induce more human-like behaviors and representations.},
  langid = {american},
  keywords = {Computational Neuroscience,Deep Neural Networks,Human--Machine Comparison,Neuroscience,Perception},
  file = {/Users/chenghao/Zotero/storage/XLHL6IS7/Hermann et al. - 2023 - For Human-Like Models, Train on Human-Like Tasks.pdf}
}

@misc{ehsan_2023,
  title = {Charting the {{Sociotechnical Gap}} in {{Explainable AI}}: {{A Framework}} to {{Address}} the {{Gap}} in {{XAI}}},
  shorttitle = {Charting the {{Sociotechnical Gap}} in {{Explainable AI}}},
  author = {Ehsan, Upol and Saha, Koustuv and De Choudhury, Munmun and Riedl, Mark O.},
  year = {2023},
  month = feb,
  doi = {10.1145/3579467},
  urldate = {2023-03-04},
  abstract = {Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap--divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  annotation = {\{"size": 996277, "pages": 33, "previous": "arXiv:2302.00799 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/SSSGT8ZY/Ehsan et al. - 2023 - Charting the Sociotechnical Gap in Explainable AI.pdf}
}

@inproceedings{bender_2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? \&\#x1f99c;},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  urldate = {2023-03-05},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  annotation = {958 citations (Semantic Scholar/DOI) [2023-03-05]},
  file = {/Users/chenghao/Zotero/storage/YU57XQY7/Bender_et_al_(2021)_On_the_Dangers_of_Stochastic_Parrots.pdf}
}

@misc{lee_2022a,
  title = {Deduplicating {{Training Data Makes Language Models Better}}},
  author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and {Callison-Burch}, Chris and Carlini, Nicholas},
  year = {2022},
  month = mar,
  number = {arXiv:2107.06499},
  eprint = {2107.06499},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.06499},
  urldate = {2023-03-12},
  abstract = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1\% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4\% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {101 citations (Semantic Scholar/arXiv) [2023-03-12]},
  file = {/Users/chenghao/Zotero/storage/SVZ4YK4G/Lee et al. - 2022 - Deduplicating Training Data Makes Language Models .pdf}
}

@misc{gandikota_2023,
  title = {Erasing {{Concepts}} from {{Diffusion Models}}},
  author = {Gandikota, Rohit and Materzynska, Joanna and {Fiotto-Kaufman}, Jaden and Bau, David},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07345},
  eprint = {2303.07345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.07345},
  urldate = {2023-03-14},
  abstract = {Motivated by recent advancements in text-to-image diffusion, we study erasure of specific concepts from the model's weights. While Stable Diffusion has shown promise in producing explicit or realistic artwork, it has raised concerns regarding its potential for misuse. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at https://erasing.baulab.info/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-03-14]},
  file = {/Users/chenghao/Zotero/storage/CX87I6ZL/Gandikota_et_al_(2023)_Erasing_Concepts_from_Diffusion_Models.pdf}
}

@misc{allal_2023,
  title = {{{SantaCoder}}: Don't Reach for the Stars!},
  shorttitle = {{{SantaCoder}}},
  author = {Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and Umapathi, Logesh Kumar and Anderson, Carolyn Jane and Zi, Yangtian and Poirier, Joel Lamy and Schoelkopf, Hailey and Troshin, Sergey and Abulkhanov, Dmitry and Romero, Manuel and Lappert, Michael and De Toni, Francesco and {del R{\'i}o}, Bernardo Garc{\'i}a and Liu, Qian and Bose, Shamik and Bhattacharyya, Urvashi and Zhuo, Terry Yue and Yu, Ian and Villegas, Paulo and Zocca, Marco and Mangrulkar, Sourab and Lansky, David and Nguyen, Huu and Contractor, Danish and Villa, Luis and Li, Jia and Bahdanau, Dzmitry and Jernite, Yacine and Hughes, Sean and Fried, Daniel and Guha, Arjun and {de Vries}, Harm and {von Werra}, Leandro},
  year = {2023},
  month = feb,
  number = {arXiv:2301.03988},
  eprint = {2301.03988},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.03988},
  urldate = {2023-03-16},
  abstract = {The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Software Engineering},
  annotation = {4 citations (Semantic Scholar/arXiv) [2023-03-15]\\
4 citations (Semantic Scholar/DOI) [2023-03-15]},
  file = {/Users/chenghao/Zotero/storage/YE63DMSI/Allal_et_al_(2023)_SantaCoder.pdf}
}

@inproceedings{kiveris_2014,
  title = {Connected {{Components}} in {{MapReduce}} and {{Beyond}}},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Kiveris, Raimondas and Lattanzi, Silvio and Mirrokni, Vahab and Rastogi, Vibhor and Vassilvitskii, Sergei},
  year = {2014},
  month = nov,
  series = {{{SOCC}} '14},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2670979.2670997},
  urldate = {2023-03-15},
  abstract = {Computing connected components of a graph lies at the core of many data mining algorithms, and is a fundamental subroutine in graph clustering. This problem is well studied, yet many of the algorithms with good theoretical guarantees perform poorly in practice, especially when faced with graphs with hundreds of billions of edges. In this paper, we design improved algorithms based on traditional MapReduce architecture for large scale data analysis. We also explore the effect of augmenting MapReduce with a distributed hash table (DHT) service. We show that these algorithms have provable theoretical guarantees, and easily outperform previously studied algorithms, sometimes by more than an order of magnitude. In particular, our iterative MapReduce algorithms run 3 to 15 times faster than the best previously studied algorithms, and the MapReduce implementation using a DHT is 10 to 30 times faster than the best previously studied algorithms. These are the fastest algorithms that easily scale to graphs with hundreds of billions of edges.},
  isbn = {978-1-4503-3252-1},
  keywords = {/unread,Connected Components,MapReduce Algorithms},
  annotation = {64 citations (Semantic Scholar/DOI) [2023-03-15]},
  file = {/Users/chenghao/Zotero/storage/DICTWYC5/Kiveris_et_al_(2014)_Connected_Components_in_MapReduce_and_Beyond.pdf}
}

@misc{abbas_2023,
  title = {{{SemDeDup}}: {{Data-efficient}} Learning at Web-Scale through Semantic Deduplication},
  shorttitle = {{{SemDeDup}}},
  author = {Abbas, Amro and Tirumala, Kushal and Simig, D{\'a}niel and Ganguli, Surya and Morcos, Ari S.},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09540},
  eprint = {2303.09540},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-17},
  abstract = {Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50\% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-03-17]},
  file = {/Users/chenghao/Zotero/storage/HC4NWGKM/Abbas_et_al_(2023)_SemDeDup.pdf}
}

@misc{gutierrez-fandino_2021,
  title = {Spanish {{Legalese Language Model}} and {{Corpora}}},
  author = {{Guti{\'e}rrez-Fandi{\~n}o}, Asier and {Armengol-Estap{\'e}}, Jordi and {Gonzalez-Agirre}, Aitor and Villegas, Marta},
  year = {2021},
  month = oct,
  number = {arXiv:2110.12201},
  eprint = {2110.12201},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.12201},
  urldate = {2023-03-18},
  abstract = {There are many Language Models for the English language according to its worldwide relevance. However, for the Spanish language, even if it is a widely spoken language, there are very few Spanish Language Models which result to be small and too general. Legal slang could be think of a Spanish variant on its own as it is very complicated in vocabulary, semantics and phrase understanding. For this work we gathered legal-domain corpora from different sources, generated a model and evaluated against Spanish general domain tasks. The model provides reasonable results in those tasks.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/L5A5INR4/Gutirrez-Fandio et al. - 2021 - Spanish Legalese Language Model and Corpora.pdf}
}

@misc{fried_2022,
  title = {{{InCoder}}: {{A Generative Model}} for {{Code Infilling}} and {{Synthesis}}},
  shorttitle = {{{InCoder}}},
  author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05999},
  eprint = {2204.05999},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.05999},
  urldate = {2023-03-18},
  abstract = {Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  annotation = {69 citations (Semantic Scholar/arXiv) [2023-03-18]\\
69 citations (Semantic Scholar/DOI) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/NW4DX7KA/Fried_et_al_(2022)_InCoder.pdf}
}

@misc{nijkamp_2023,
  title = {{{CodeGen}}: {{An Open Large Language Model}} for {{Code}} with {{Multi-Turn Program Synthesis}}},
  shorttitle = {{{CodeGen}}},
  author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  year = {2023},
  month = feb,
  number = {arXiv:2203.13474},
  eprint = {2203.13474},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.13474},
  urldate = {2023-03-18},
  abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages},
  annotation = {22 citations (Semantic Scholar/arXiv) [2023-03-18]\\
66 citations (Semantic Scholar/DOI) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/MF6JVVJ4/Nijkamp_et_al_(2023)_CodeGen.pdf}
}

@article{li_2022c,
  title = {Competition-{{Level Code Generation}} with {{AlphaCode}}},
  author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and {d'Autume}, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and {de Freitas}, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  year = {2022},
  month = dec,
  journal = {Science},
  volume = {378},
  number = {6624},
  eprint = {2203.07814},
  primaryclass = {cs},
  pages = {1092--1097},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abq1158},
  urldate = {2023-03-18},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  annotation = {173 citations (Semantic Scholar/arXiv) [2023-03-18]\\
173 citations (Semantic Scholar/DOI) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/U4J685K8/Li_et_al_(2022)_Competition-Level_Code_Generation_with_AlphaCode.pdf}
}

@misc{chowdhery_2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and {Gur-Ari}, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and {Meier-Hellstern}, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  year = {2022},
  month = oct,
  number = {arXiv:2204.02311},
  eprint = {2204.02311},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.02311},
  urldate = {2023-03-18},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {642 citations (Semantic Scholar/arXiv) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/K8IZGNU6/Chowdhery_et_al_(2022)_PaLM.pdf}
}

@misc{_c,
  title = {Natural {{Language Processing}} with {{Transformers}}, {{Revised Edition}} [{{Book}}]},
  urldate = {2023-03-18},
  abstract = {Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or {\dots} - Selection from Natural Language Processing with Transformers, Revised Edition [Book]},
  howpublished = {https://www.oreilly.com/library/view/natural-language-processing/9781098136789/},
  isbn = {9781098136796},
  langid = {english},
  keywords = {/unread}
}

@misc{xu_2022a,
  title = {A {{Systematic Evaluation}} of {{Large Language Models}} of {{Code}}},
  author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J.},
  year = {2022},
  month = may,
  number = {arXiv:2202.13169},
  eprint = {2202.13169},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-18},
  abstract = {Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Programming Languages},
  annotation = {57 citations (Semantic Scholar/arXiv) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/356QAF5S/Xu_et_al_(2022)_A_Systematic_Evaluation_of_Large_Language_Models_of_Code.pdf}
}

@misc{ainslie_2023,
  title = {{{CoLT5}}: {{Faster Long-Range Transformers}} with {{Conditional Computation}}},
  shorttitle = {{{CoLT5}}},
  author = {Ainslie, Joshua and Lei, Tao and {de Jong}, Michiel and Onta{\~n}{\'o}n, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and {Lee-Thorp}, James and Tay, Yi and Sung, Yun-Hsuan and Sanghai, Sumit},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09752},
  eprint = {2303.09752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.09752},
  urldate = {2023-03-20},
  abstract = {Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/XC3BHKTH/Ainslie et al. - 2023 - CoLT5 Faster Long-Range Transformers with Conditi.pdf}
}

@article{gutierrez-fandino_2022,
  title = {{{MarIA}}: {{Spanish Language Models}}},
  shorttitle = {{{MarIA}}},
  author = {{Guti{\'e}rrez-Fandi{\~n}o}, Asier and {Armengol-Estap{\'e}}, Jordi and P{\`a}mies, Marc and {Llop-Palao}, Joan and {Silveira-Ocampo}, Joaqu{\'i}n and Carrino, Casimiro Pio and {Gonzalez-Agirre}, Aitor and {Armentano-Oller}, Carme and {Rodriguez-Penagos}, Carlos and Villegas, Marta},
  year = {2022},
  journal = {Procesamiento del Lenguaje Natural},
  eprint = {2107.07253},
  primaryclass = {cs},
  pages = {39--60},
  issn = {1989-7553},
  doi = {10.26342/2022-68-3},
  urldate = {2023-03-21},
  abstract = {This work presents MarIA, a family of Spanish language models and associated resources made available to the industry and the research community. Currently, MarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish language models, which can arguably be presented as the largest and most proficient language models in Spanish. The models were pretrained using a massive corpus of 570GB of clean and deduplicated texts with 135 billion words extracted from the Spanish Web Archive crawled by the National Library of Spain between 2009 and 2019. We assessed the performance of the models with nine existing evaluation datasets and with a novel extractive Question Answering dataset created ex novo. Overall, MarIA models outperform the existing Spanish models across a variety of NLU tasks and training settings.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {18 citations (Semantic Scholar/arXiv) [2023-03-20]\\
18 citations (Semantic Scholar/DOI) [2023-03-20]},
  file = {/Users/chenghao/Zotero/storage/X7LJS5L7/Gutirrez-Fandio et al. - 2022 - MarIA Spanish Language Models.pdf}
}

@misc{lewistunstall_,
  title = {Natural {{Language Processing}} with {{Transformers}}, {{Revised Edition}}},
  author = {{Lewis Tunstall} and {Leandro von Werra} and {Thomas Wolf}},
  urldate = {2023-03-21},
  abstract = {Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or {\dots} - Selection from Natural Language Processing with Transformers, Revised Edition [Book]},
  howpublished = {https://www.oreilly.com/library/view/natural-language-processing/9781098136789/},
  isbn = {9781098136796},
  langid = {english},
  keywords = {/unread}
}

@misc{austin_2021,
  title = {Program {{Synthesis}} with {{Large Language Models}}},
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  year = {2021},
  month = aug,
  number = {arXiv:2108.07732},
  eprint = {2108.07732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07732},
  urldate = {2023-03-21},
  abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Machine Learning,Computer Science - Programming Languages},
  annotation = {177 citations (Semantic Scholar/arXiv) [2023-03-20]},
  file = {/Users/chenghao/Zotero/storage/LXCEL3YS/Austin et al. - 2021 - Program Synthesis with Large Language Models.pdf}
}

@misc{gilardi_2023,
  title = {{{ChatGPT Outperforms Crowd-Workers}} for {{Text-Annotation Tasks}}},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  year = {2023},
  month = mar,
  number = {arXiv:2303.15056},
  eprint = {2303.15056},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.15056},
  urldate = {2023-03-28},
  abstract = {Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/chenghao/Zotero/storage/FIGARLHI/Gilardi_et_al_(2023)_ChatGPT_Outperforms_Crowd-Workers_for_Text-Annotation_Tasks.pdf}
}

@misc{gururangan_2023,
  title = {Scaling {{Expert Language Models}} with {{Unsupervised Domain Discovery}}},
  author = {Gururangan, Suchin and Li, Margaret and Lewis, Mike and Shi, Weijia and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2023},
  month = mar,
  number = {arXiv:2303.14177},
  eprint = {2303.14177},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.14177},
  urldate = {2023-03-28},
  abstract = {Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-03-27]},
  file = {/Users/chenghao/Zotero/storage/PSJ29H5F/Gururangan_et_al_(2023)_Scaling_Expert_Language_Models_with_Unsupervised_Domain_Discovery.pdf}
}

@misc{lialin_2023,
  title = {Scaling {{Down}} to {{Scale Up}}: {{A Guide}} to {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {Scaling {{Down}} to {{Scale Up}}},
  author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  year = {2023},
  month = mar,
  number = {arXiv:2303.15647},
  eprint = {2303.15647},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.15647},
  urldate = {2023-03-29},
  abstract = {This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {\{"size": 860410, "pages": 21, "previous": "0 citations (Semantic Scholar/arXiv) [2023-03-29]{\textbackslash}narXiv:2303.15647 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/B4XJ674F/Lialin_et_al_(2023)_Scaling_Down_to_Scale_Up.pdf}
}

@misc{niven_2019,
  title = {Probing {{Neural Network Comprehension}} of {{Natural Language Arguments}}},
  author = {Niven, Timothy and Kao, Hung-Yu},
  year = {2019},
  month = sep,
  number = {arXiv:1907.07355},
  eprint = {1907.07355},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.07355},
  urldate = {2023-03-30},
  abstract = {We are surprised to find that BERT's peak performance of 77\% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {332 citations (Semantic Scholar/arXiv) [2023-03-29]},
  file = {/Users/chenghao/Zotero/storage/AJ8RUM55/Niven_Kao_(2019)_Probing_Neural_Network_Comprehension_of_Natural_Language_Arguments.pdf}
}

@misc{touvron_2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-04-04},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {35 citations (Semantic Scholar/arXiv) [2023-04-03]\\
35 citations (Semantic Scholar/DOI) [2023-04-03]},
  file = {/Users/chenghao/Zotero/storage/3GSHT8AZ/Touvron_et_al_(2023)_LLaMA.pdf}
}

@misc{el-mhamdi_2022,
  title = {{{SoK}}: {{On}} the {{Impossible Security}} of {{Very Large Foundation Models}}},
  shorttitle = {{{SoK}}},
  author = {{El-Mhamdi}, El-Mahdi and Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Hoang, L{\^e}-Nguy{\^e}n and Pinot, Rafael and Stephan, John},
  year = {2022},
  month = sep,
  number = {arXiv:2209.15259},
  eprint = {2209.15259},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.15259},
  urldate = {2023-04-08},
  abstract = {Large machine learning models, or so-called foundation models, aim to serve as base-models for application-oriented machine learning. Although these models showcase impressive performance, they have been empirically found to pose serious security and privacy issues. We may however wonder if this is a limitation of the current models, or if these issues stem from a fundamental intrinsic impossibility of the foundation model learning problem itself. This paper aims to systematize our knowledge supporting the latter. More precisely, we identify several key features of today's foundation model learning problem which, given the current understanding in adversarial machine learning, suggest incompatibility of high accuracy with both security and privacy. We begin by observing that high accuracy seems to require (1) very high-dimensional models and (2) huge amounts of data that can only be procured through user-generated datasets. Moreover, such data is fundamentally heterogeneous, as users generally have very specific (easily identifiable) data-generating habits. More importantly, users' data is filled with highly sensitive information, and maybe heavily polluted by fake users. We then survey lower bounds on accuracy in privacy-preserving and Byzantine-resilient heterogeneous learning that, we argue, constitute a compelling case against the possibility of designing a secure and privacy-preserving high-accuracy foundation model. We further stress that our analysis also applies to other high-stake machine learning applications, including content recommendation. We conclude by calling for measures to prioritize security and privacy, and to slow down the race for ever larger models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/S5MNGT6Y/El-Mhamdi et al. - 2022 - SoK On the Impossible Security of Very Large Foun.pdf}
}

@misc{madaan_2023,
  title = {Self-{{Refine}}: {{Iterative Refinement}} with {{Self-Feedback}}},
  shorttitle = {Self-{{Refine}}},
  author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Welleck, Sean and Majumder, Bodhisattwa Prasad and Gupta, Shashank and Yazdanbakhsh, Amir and Clark, Peter},
  year = {2023},
  month = mar,
  number = {arXiv:2303.17651},
  eprint = {2303.17651},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17651},
  urldate = {2023-04-08},
  abstract = {Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving on average by absolute 20\% across tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/VIVK8Z8T/Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf}
}

@misc{xie_2022,
  title = {Calibrating {{Trust}} of {{Multi-Hop Question Answering Systems}} with {{Decompositional Probes}}},
  author = {Xie, Kaige and Wiegreffe, Sarah and Riedl, Mark},
  year = {2022},
  month = oct,
  number = {arXiv:2204.07693},
  eprint = {2204.07693},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.07693},
  urldate = {2023-04-08},
  abstract = {Multi-hop Question Answering (QA) is a challenging task since it requires an accurate aggregation of information from multiple context paragraphs and a thorough understanding of the underlying reasoning chains. Recent work in multi-hop QA has shown that performance can be boosted by first decomposing the questions into simpler, single-hop questions. In this paper, we explore one additional utility of the multi-hop decomposition from the perspective of explainable NLP: to create explanation by probing a neural QA model with them. We hypothesize that in doing so, users will be better able to predict when the underlying QA system will give the correct answer. Through human participant studies, we verify that exposing the decomposition probes and answers to the probes to users can increase their ability to predict system performance on a question instance basis. We show that decomposition is an effective form of probing QA systems as well as a promising approach to explanation generation. In-depth analyses show the need for improvements in decomposition systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/XG8MMK6Q/Xie et al. - 2022 - Calibrating Trust of Multi-Hop Question Answering .pdf}
}

@misc{_b,
  title = {From {{Deep}} to {{Long Learning}}?},
  urldate = {2023-04-09},
  howpublished = {https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/DA5WIHW2/From_Deep_to_Long_Learning.pdf;/Users/chenghao/Zotero/storage/CBLRFZ2D/2023-03-27-long-learning.html}
}

@misc{bowman_2023,
  title = {Eight {{Things}} to {{Know}} about {{Large Language Models}}},
  author = {Bowman, Samuel R.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.00612},
  eprint = {2304.00612},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.00612},
  urldate = {2023-04-11},
  abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/MVAMVATB/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf}
}

@article{vassei_,
  title = {{{AI Transparency}} in {{Practice}}},
  author = {Vasse'i, Ramak Molavi and McCrosky, Jesse},
  langid = {english},
  annotation = {\{"size": 4148982, "pages": 44, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/2LX5ALN5/Vasse'i and McCrosky - AI Transparency in Practice.pdf}
}

@misc{liu_2023,
  title = {Evaluating {{Verifiability}} in {{Generative Search Engines}}},
  author = {Liu, Nelson F. and Zhang, Tianyi and Liang, Percy},
  year = {2023},
  month = apr,
  number = {arXiv:2304.09848},
  eprint = {2304.09848},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.09848},
  urldate = {2023-04-20},
  abstract = {Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5\% of generated sentences are fully supported by citations and only 74.5\% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {\{"size": 1320879, "pages": 25, "previous": "0 citations (Semantic Scholar/arXiv) [2023-04-20]{\textbackslash}narXiv:2304.09848 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/BL2WVD4C/Liu_et_al_(2023)_Evaluating_Verifiability_in_Generative_Search_Engines.pdf}
}

@misc{bulatov_2023,
  title = {Scaling {{Transformer}} to {{1M}} Tokens and beyond with {{RMT}}},
  author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11062},
  eprint = {2304.11062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11062},
  urldate = {2023-04-25},
  abstract = {This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/YNBRHW5E/Bulatov et al. - 2023 - Scaling Transformer to 1M tokens and beyond with R.pdf}
}

@misc{rust_2022,
  title = {Language {{Modelling}} with {{Pixels}}},
  author = {Rust, Phillip and Lotz, Jonas F. and Bugliarello, Emanuele and Salesky, Elizabeth and {de Lhoneux}, Miryam and Elliott, Desmond},
  year = {2022},
  month = jul,
  number = {arXiv:2207.06991},
  eprint = {2207.06991},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.06991},
  urldate = {2023-04-25},
  abstract = {Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches, instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 2648383, "pages": 35, "previous": "arXiv:2207.06991 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/PU2C8FBM/Rust et al. - 2022 - Language Modelling with Pixels.pdf}
}

@misc{wolf_2023,
  title = {Fundamental {{Limitations}} of {{Alignment}} in {{Large Language Models}}},
  author = {Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11082},
  eprint = {2304.11082},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11082},
  urldate = {2023-04-25},
  abstract = {An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback increase the LLM's proneness to being prompted into the undesired behaviors. Moreover, we include the notion of personas in our BEB framework, and find that behaviors which are generally very unlikely to be exhibited by the model can be brought to the front by prompting the model to behave as specific persona. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {\{"size": 1188383, "pages": 22, "previous": "arXiv:2304.11082 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/G2RH5MVZ/Wolf et al. - 2023 - Fundamental Limitations of Alignment in Large Lang.pdf}
}

@misc{biderman_2023,
  title = {Pythia: {{A Suite}} for {{Analyzing Large Language Models Across Training}} and {{Scaling}}},
  shorttitle = {Pythia},
  author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and {van der Wal}, Oskar},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01373},
  eprint = {2304.01373},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.01373},
  urldate = {2023-04-25},
  abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/JP79AKQG/Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Model.pdf}
}

@misc{liang_2023,
  title = {Unleashing {{Infinite-Length Input Capacity}} for {{Large-scale Language Models}} with {{Self-Controlled Memory System}}},
  author = {Liang, Xinnian and Wang, Bing and Huang, Hui and Wu, Shuangzhi and Wu, Peihao and Lu, Lu and Ma, Zejun and Li, Zhoujun},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13343},
  eprint = {2304.13343},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13343},
  urldate = {2023-04-27},
  abstract = {Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not optimized for multi-turn dialogue, to achieve multi-turn dialogue capabilities that are comparable to ChatGPT, and to outperform ChatGPT in scenarios involving ultra-long document summarization or long-term conversations. Additionally, we will supply a test set, which covers common long-text input scenarios, for evaluating the abilities of LLMs in processing long documents.{\textasciitilde}{\textbackslash}footnote\{Working in progress.\}{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/wbbeyourself/SCM4LLMs\}\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/EZPS3CTL/Liang et al. - 2023 - Unleashing Infinite-Length Input Capacity for Larg.pdf}
}

@misc{schlangen_2023,
  title = {What {{A Situated Language-Using Agent Must}} Be {{Able}} to {{Do}}: {{A Top-Down Analysis}}},
  shorttitle = {What {{A Situated Language-Using Agent Must}} Be {{Able}} to {{Do}}},
  author = {Schlangen, David},
  year = {2023},
  month = feb,
  number = {arXiv:2302.08590},
  eprint = {2302.08590},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.08590},
  urldate = {2023-04-27},
  abstract = {Even in our increasingly text-intensive times, the primary site of language use is situated, co-present interaction. It is primary ontogenetically and phylogenetically, and it is arguably also still primary in negotiating everyday social situations. Situated interaction is also the final frontier of Natural Language Processing, where, compared to the area of text processing, very little progress has been made in the past decade, and where a myriad of practical applications is waiting to be unlocked. While the usual approach in the field is to reach, bottom-up, for the ever next "adjacent possible", in this paper I attempt a top-down analysis of what the demands are that unrestricted situated interaction makes on the participating agent, and suggest ways in which this analysis can structure computational models and research on them. Specifically, I discuss representational demands (the building up and application of world model, language model, situation model, discourse model, and agent model) and what I call anchoring processes (incremental processing, incremental learning, conversational grounding, multimodal grounding) that bind the agent to the here, now, and us.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/KGHRRVEG/Schlangen - 2023 - What A Situated Language-Using Agent Must be Able .pdf}
}

@misc{bulatov_2022,
  title = {Recurrent {{Memory Transformer}}},
  author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S.},
  year = {2022},
  month = dec,
  number = {arXiv:2207.06881},
  eprint = {2207.06881},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.06881},
  urldate = {2023-04-30},
  abstract = {Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {4 citations (Semantic Scholar/arXiv) [2023-04-30]\\
4 citations (Semantic Scholar/DOI) [2023-04-30]},
  file = {/Users/chenghao/Zotero/storage/XPWWA6U3/Bulatov_et_al_(2022)_Recurrent_Memory_Transformer.pdf}
}

@misc{schaeffer_2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.15004},
  eprint = {2304.15004},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.15004},
  urldate = {2023-05-02},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers). In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-05-01]},
  file = {/Users/chenghao/Zotero/storage/YY6N3FJP/Schaeffer_et_al_(2023)_Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage.pdf}
}

@misc{turski_2023,
  title = {{{CCpdf}}: {{Building}} a {{High Quality Corpus}} for {{Visually Rich Documents}} from {{Web Crawl Data}}},
  shorttitle = {{{CCpdf}}},
  author = {Turski, Micha{\l} and Stanis{\l}awek, Tomasz and Kaczmarek, Karol and Dyda, Pawe{\l} and Grali{\'n}ski, Filip},
  year = {2023},
  month = apr,
  number = {arXiv:2304.14953},
  eprint = {2304.14953},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.14953},
  urldate = {2023-05-02},
  abstract = {In recent years, the field of document understanding has progressed a lot. A significant part of this progress has been possible thanks to the use of language models pretrained on large amounts of documents. However, pretraining corpora used in the domain of document understanding are single domain, monolingual, or nonpublic. Our goal in this paper is to propose an efficient pipeline for creating a big-scale, diverse, multilingual corpus of PDF files from all over the Internet using Common Crawl, as PDF files are the most canonical types of documents as considered in document understanding. We analysed extensively all of the steps of the pipeline and proposed a solution which is a trade-off between data quality and processing time. We also share a CCpdf corpus in a form or an index of PDF files along with a script for downloading them, which produces a collection useful for language model pretraining. The dataset and tools published with this paper offer researchers the opportunity to develop even better multilingual language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-05-01]},
  file = {/Users/chenghao/Zotero/storage/PEUI6V2C/Turski_et_al_(2023)_CCpdf.pdf}
}

@misc{bertsch_2023,
  title = {Unlimiformer: {{Long-Range Transformers}} with {{Unlimited Length Input}}},
  shorttitle = {Unlimiformer},
  author = {Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew R.},
  year = {2023},
  month = may,
  number = {arXiv:2305.01625},
  eprint = {2305.01625},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01625},
  urldate = {2023-05-03},
  abstract = {Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single \$k\$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-\$k\$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/2BNNLJIG/Bertsch_et_al_(2023)_Unlimiformer.pdf}
}

@article{krzywinski_2013,
  title = {Error Bars},
  author = {Krzywinski, Martin and Altman, Naomi},
  year = {2013},
  month = oct,
  journal = {Nature Methods},
  volume = {10},
  number = {10},
  pages = {921--922},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2659},
  urldate = {2023-05-08},
  abstract = {The meaning of error bars is often misinterpreted, as is the statistical significance of their overlap.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Publishing,Research data,Statistical methods},
  file = {/Users/chenghao/Zotero/storage/PUE4BBCE/Krzywinski and Altman - 2013 - Error bars.pdf}
}

@misc{fernandes_2023,
  title = {Bridging the {{Gap}}: {{A Survey}} on {{Integrating}} ({{Human}}) {{Feedback}} for {{Natural Language Generation}}},
  shorttitle = {Bridging the {{Gap}}},
  author = {Fernandes, Patrick and Madaan, Aman and Liu, Emmy and Farinhas, Ant{\'o}nio and Martins, Pedro Henrique and Bertsch, Amanda and {de Souza}, Jos{\'e} G. C. and Zhou, Shuyan and Wu, Tongshuang and Neubig, Graham and Martins, Andr{\'e} F. T.},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-05-10},
  abstract = {Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.},
  howpublished = {https://arxiv.org/abs/2305.00955v1},
  langid = {english},
  annotation = {\{"size": 394060, "pages": 24, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/UHZ3F26Q/Fernandes et al. - 2023 - Bridging the Gap A Survey on Integrating (Human) .pdf}
}

@misc{burns_2023,
  title = {A {{Suite}} of {{Generative Tasks}} for {{Multi-Level Multimodal Webpage Understanding}}},
  author = {Burns, Andrea and Srinivasan, Krishna and Ainslie, Joshua and Brown, Geoff and Plummer, Bryan A. and Saenko, Kate and Ni, Jianmo and Guo, Mandy},
  year = {2023},
  month = may,
  number = {arXiv:2305.03668},
  eprint = {2305.03668},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03668},
  urldate = {2023-05-12},
  abstract = {Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Experiments show that the new annotations from WikiWeb2M improve task performance compared to data from prior work. We also include ablations on sequence length, input features, and model size.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 7675000, "pages": 25, "previous": "arXiv:2305.03668 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/HV9P5NZM/Burns et al. - 2023 - A Suite of Generative Tasks for Multi-Level Multim.pdf}
}

@misc{_d,
  title = {Data {{Statements}}: {{From Technical Concept}} to {{Community Practice}} {\textbar} {{ACM Journal}} on {{Responsible Computing}}},
  urldate = {2023-05-12},
  howpublished = {https://dl.acm.org/doi/10.1145/3594737},
  annotation = {\{"size": 794925, "pages": 17, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/AJ3XXS2K/Data_Statements.pdf}
}

@article{ogorman_1993,
  title = {The Document Spectrum for Page Layout Analysis},
  author = {O'Gorman, L.},
  year = {Nov./1993},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {15},
  number = {11},
  pages = {1162--1173},
  issn = {01628828},
  doi = {10.1109/34.244677},
  urldate = {2023-05-13},
  abstract = {Page layout analysis is a document processing technique used to determine the format of a page. This paper describes the document spectrum, or docstrum, which is a method for structural page layout analysis based on bottom-up, nearestneighbor clustering of page components. The method yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks. It is advantageous over many other methods in three main ways: independencefrom skew angle, independence from different text spacings, and the ability to process local regions of different text orientations within the same image. Results of the method shown for several different page formats and for randomly oriented subpages on the same image illustrate the versatility of the method. We also discuss the differences, advantages, and disadvantages of the docstrum with respect to other lay-out methods.},
  langid = {english},
  annotation = {748 citations (Semantic Scholar/DOI) [2023-05-13]},
  file = {/Users/chenghao/Zotero/storage/CX8ERB6L/O'Gorman - 1993 - The document spectrum for page layout analysis.pdf}
}

@misc{schuett_2023,
  title = {Towards Best Practices in {{AGI}} Safety and Governance: {{A}} Survey of Expert Opinion},
  shorttitle = {Towards Best Practices in {{AGI}} Safety and Governance},
  author = {Schuett, Jonas and Dreksler, Noemi and Anderljung, Markus and McCaffary, David and Heim, Lennart and Bluemke, Emma and Garfinkel, Ben},
  year = {2023},
  month = may,
  number = {arXiv:2305.07153},
  eprint = {2305.07153},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07153},
  urldate = {2023-05-15},
  abstract = {A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98\% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  annotation = {\{"size": 845610, "pages": 38, "previous": "arXiv:2305.07153 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/BKSWPTAH/Schuett et al. - 2023 - Towards best practices in AGI safety and governanc.pdf}
}

@misc{yu_2023,
  title = {{{MEGABYTE}}: {{Predicting Million-byte Sequences}} with {{Multiscale Transformers}}},
  shorttitle = {{{MEGABYTE}}},
  author = {Yu, Lili and Simig, D{\'a}niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
  year = {2023},
  month = may,
  number = {arXiv:2305.07185},
  eprint = {2305.07185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07185},
  urldate = {2023-05-15},
  abstract = {Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/I68R5QSM/Yu et al. - 2023 - MEGABYTE Predicting Million-byte Sequences with M.pdf}
}

@misc{landeghem_2023,
  title = {Document {{Understanding Dataset}} and {{Evaluation}} ({{DUDE}})},
  author = {Landeghem, Jordy and Tito, Rub{\'e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and J{\'o}ziak, Pawe{\l} and Powalski, Rafa{\l} and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Ackaert, Bertrand and Valveny, Ernest and Blaschko, Matthew and Moens, Sien and Stanis{\l}awek, Tomasz},
  year = {2023},
  month = may,
  number = {arXiv:2305.08455},
  eprint = {2305.08455},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.08455},
  urldate = {2023-05-16},
  abstract = {We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance of finding more efficient ways to model language, images, and layout in DocAI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 2472696, "pages": 22, "previous": "arXiv:2305.08455 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/PJHTRVXH/Landeghem et al. - 2023 - Document Understanding Dataset and Evaluation (DUD.pdf}
}

@misc{xie_2023,
  title = {{{DoReMi}}: {{Optimizing Data Mixtures Speeds Up Language Model Pretraining}}},
  shorttitle = {{{DoReMi}}},
  author = {Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V. and Ma, Tengyu and Yu, Adams Wei},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-05-20},
  abstract = {The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5\% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.},
  howpublished = {https://arxiv.org/abs/2305.10429v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/RMN9UIIK/Xie et al. - 2023 - DoReMi Optimizing Data Mixtures Speeds Up Languag.pdf}
}

@misc{zhao_2023,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = may,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  publisher = {arXiv},
  urldate = {2023-05-22},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {\{"size": 2395727, "pages": 58, "previous": "45 citations (Semantic Scholar/arXiv) [2023-05-22]{\textbackslash}narXiv:2303.18223 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/AHZMLRQN/Zhao_et_al_(2023)_A_Survey_of_Large_Language_Models.pdf}
}

@article{min_,
  title = {{{FACTSCORE}}: {{Fine-grained Atomic Evaluation}} of {{Factual Precision}} in {{Long Form Text Generation}}},
  author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/FKE3IDEU/Min et al. - FACTSCORE Fine-grained Atomic Evaluation of Factu.pdf}
}

@misc{krishna_2023,
  title = {{{USB}}: {{A Unified Summarization Benchmark Across Tasks}} and {{Domains}}},
  shorttitle = {{{USB}}},
  author = {Krishna, Kundan and Gupta, Prakhar and Ramprasad, Sanjana and Wallace, Byron C. and Bigham, Jeffrey P. and Lipton, Zachary C.},
  year = {2023},
  month = may,
  number = {arXiv:2305.14296},
  eprint = {2305.14296},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14296},
  urldate = {2023-05-24},
  abstract = {An abundance of datasets exist for training and evaluating models on the task of summary generation.However, these datasets are often derived heuristically, and lack sufficient annotations to support research into all aspects of summarization, such as evidence extraction and controllable summarization. We introduce a benchmark comprising 8 tasks that require multi-dimensional understanding of summarization, e.g., surfacing evidence for a summary, assessing its correctness, and gauging its relevance to different topics. We compare various methods on this benchmark and discover that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models. For factuality related tasks, we also evaluate existing heuristics to create training data and find that training on them performs worse than training on \$20{\textbackslash}times\$ less human-labeled data. Our benchmark consists of data from 6 different domains, allowing us to study cross-domain performance of trained models. We find that for some tasks, the amount of training data matters more than the domain where it comes from, while for other tasks training specifically on data from the target domain, even if limited, is more beneficial. Our work fulfills the need for a well-annotated summarization benchmark with diverse tasks, and provides useful insights about the impact of the quality, size and domain of training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 1756720, "pages": 18, "previous": "arXiv:2305.14296 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/HC492MGS/Krishna et al. - 2023 - USB A Unified Summarization Benchmark Across Task.pdf}
}

@misc{kim_2023,
  title = {Cream: {{Visually-Situated Natural Language Understanding}} with {{Contrastive Reading Model}} and {{Frozen Large Language Models}}},
  shorttitle = {Cream},
  author = {Kim, Geewook and Lee, Hodong and Kim, Daehee and Jung, Haeji and Park, Sanghee and Kim, Yoonsik and Yun, Sangdoo and Kil, Taeho and Lee, Bado and Park, Seunghyun},
  year = {2023},
  month = may,
  number = {arXiv:2305.15080},
  eprint = {2305.15080},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15080},
  urldate = {2023-05-25},
  abstract = {Advances in Large Language Models (LLMs) have inspired a surge of research exploring their expansion into the visual domain. While recent models exhibit promise in generating abstract captions for images and conducting natural conversations, their performance on text-rich images leaves room for improvement. In this paper, we propose the Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details typically overlooked by existing methods. Cream integrates vision and auxiliary encoders, complemented by a contrastive feature alignment technique, resulting in a more effective understanding of textual information within document images. Our approach, thus, seeks to bridge the gap between vision and language understanding, paving the way for more sophisticated Document Intelligence Assistants. Rigorous evaluations across diverse tasks, such as visual question answering on document images, demonstrate the efficacy of Cream as a state-of-the-art model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {\{"size": 6931297, "pages": 23, "previous": "arXiv:2305.15080 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/J7I8F5EV/Kim et al. - 2023 - Cream Visually-Situated Natural Language Understa.pdf}
}

@misc{shevlane_2023,
  title = {Model Evaluation for Extreme Risks},
  author = {Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and Ho, Lewis and Siddarth, Divya and Avin, Shahar and Hawkins, Will and Kim, Been and Gabriel, Iason and Bolina, Vijay and Clark, Jack and Bengio, Yoshua and Christiano, Paul and Dafoe, Allan},
  year = {2023},
  month = may,
  number = {arXiv:2305.15324},
  eprint = {2305.15324},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15324},
  urldate = {2023-05-25},
  abstract = {Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,K.4.1},
  annotation = {\{"size": 803057, "pages": 20, "previous": "arXiv:2305.15324 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/CBXV3SUE/Shevlane et al. - 2023 - Model evaluation for extreme risks.pdf}
}

@misc{gudibande_2023,
  title = {The {{False Promise}} of {{Imitating Proprietary LLMs}}},
  author = {Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  year = {2023},
  month = may,
  number = {arXiv:2305.15717},
  eprint = {2305.15717},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15717},
  urldate = {2023-05-26},
  abstract = {An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/4C83PGN7/Gudibande et al. - 2023 - The False Promise of Imitating Proprietary LLMs.pdf}
}

@misc{muennighoff_2023,
  title = {Scaling {{Data-Constrained Language Models}}},
  author = {Muennighoff, Niklas and Rush, Alexander M. and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  year = {2023},
  month = may,
  number = {arXiv:2305.16264},
  eprint = {2305.16264},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16264},
  urldate = {2023-05-26},
  abstract = {The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are publicly available at https://github.com/huggingface/datablations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/HMI42UTG/Muennighoff et al. - 2023 - Scaling Data-Constrained Language Models.pdf}
}

@article{penedo_,
  title = {The {{RefinedWeb Dataset}} for {{Falcon LLM}}: {{Outperforming Curated Corpora}} with {{Web Data}}, and {{Web Data Only}}},
  author = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  abstract = {Large language models are commonly trained on a mixture of filtered web data and curated ``high-quality'' corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our REFINEDWEB dataset, and 1.3/7.5B parameters language models trained on it*.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/6UXEE77Z/Penedo et al. - The RefinedWeb Dataset for Falcon LLM Outperformi.pdf}
}

@misc{abercrombie_2023,
  title = {Mirages: {{On Anthropomorphism}} in {{Dialogue Systems}}},
  shorttitle = {Mirages},
  author = {Abercrombie, Gavin and Curry, Amanda Cercas and Dinkar, Tanvi and Talat, Zeerak},
  year = {2023},
  month = may,
  number = {arXiv:2305.09800},
  eprint = {2305.09800},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.09800},
  urldate = {2023-05-30},
  abstract = {Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-05-29]\\
0 citations (Semantic Scholar/DOI) [2023-05-29]},
  file = {/Users/chenghao/Zotero/storage/DJ92DHUC/Abercrombie_et_al_(2023)_Mirages.pdf}
}

@misc{liu_2023b,
  title = {Blockwise {{Parallel Transformer}} for {{Long Context Large Models}}},
  author = {Liu, Hao and Abbeel, Pieter},
  year = {2023},
  month = may,
  number = {arXiv:2305.19370},
  eprint = {2305.19370},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.19370},
  urldate = {2023-06-05},
  abstract = {Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/388UWMPN/Liu and Abbeel - 2023 - Blockwise Parallel Transformer for Long Context La.pdf}
}

@misc{malladi_2023,
  title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},
  author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},
  year = {2023},
  month = may,
  number = {arXiv:2305.17333},
  eprint = {2305.17333},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17333},
  urldate = {2023-06-07},
  abstract = {Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-06-06]},
  file = {/Users/chenghao/Zotero/storage/DLMCW8S6/[2305.17333] Fine-Tuning Language Models with Just.pdf}
}

@article{wasserstein_2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2023-06-19},
  annotation = {4291 citations (Semantic Scholar/DOI) [2023-06-19]},
  file = {/Users/chenghao/Zotero/storage/L67EHAC7/Wasserstein_Lazar_(2016)_The_ASA_Statement_on_p-Values.pdf}
}

@misc{sancheti_2022,
  title = {What to {{Read}} in a {{Contract}}? {{Party-Specific Summarization}} of {{Important Obligations}}, {{Entitlements}}, and {{Prohibitions}} in {{Legal Documents}}},
  shorttitle = {What to {{Read}} in a {{Contract}}?},
  author = {Sancheti, Abhilasha and Garimella, Aparna and Srinivasan, Balaji Vasan and Rudinger, Rachel},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09825},
  eprint = {2212.09825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09825},
  urldate = {2023-06-20},
  abstract = {Legal contracts, such as employment or lease agreements, are important documents as they govern the obligations and entitlements of the various contracting parties. However, these documents are typically long and written in legalese resulting in lots of manual hours spent in understanding them. In this paper, we address the task of summarizing legal contracts for each of the contracting parties, to enable faster reviewing and improved understanding of them. Specifically, we collect a dataset consisting of pairwise importance comparison annotations by legal experts for {\textasciitilde}293K sentence pairs from lease agreements. We propose a novel extractive summarization system to automatically produce a summary consisting of the most important obligations, entitlements, and prohibitions in a contract. It consists of two modules: (1) a content categorize to identify sentences containing each of the categories (i.e., obligation, entitlement, and prohibition) for a party, and (2) an importance ranker to compare the importance among sentences of each category for a party to obtain a ranked list. The final summary is produced by selecting the most important sentences of a category for each of the parties. We demonstrate the effectiveness of our proposed system by comparing it against several text ranking baselines via automatic and human evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/9Z73NXIY/Sancheti et al. - 2022 - What to Read in a Contract Party-Specific Summari.pdf;/Users/chenghao/Zotero/storage/HJRTA6DL/2212.html}
}

@inproceedings{xu_2022b,
  title = {{{ConReader}}: {{Exploring Implicit Relations}} in {{Contracts}} for {{Contract Clause Extraction}}},
  shorttitle = {{{ConReader}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Xu, Weiwen and Deng, Yang and Lei, Wenqiang and Zhao, Wenlong and Chua, Tat-Seng and Lam, Wai},
  year = {2022},
  month = dec,
  pages = {2581--2594},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-06-22},
  abstract = {We study automatic Contract Clause Extraction (CCE) by modeling implicit relations in legal contracts. Existing CCE methods mostly treat contracts as plain text, creating a substantial barrier to understanding contracts of high complexity. In this work, we first comprehensively analyze the complexity issues of contracts and distill out three implicit relations commonly found in contracts, namely, 1) Long-range Context Relation that captures the correlations of distant clauses; 2) Term-Definition Relation that captures the relation between important terms with their corresponding definitions, and 3) Similar Clause Relation that captures the similarities between clauses of the same type. Then we propose a novel framework ConReader to exploit the above three relations for better contract understanding and improving CCE. Experimental results show that ConReader makes the prediction more interpretable and achieves new state-of-the-art on two CCE tasks in both conventional and zero-shot settings.},
  annotation = {\{"size": 1453425, "pages": 14, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/AEL5PTVS/Xu et al. - 2022 - ConReader Exploring Implicit Relations in Contrac.pdf}
}

@misc{birhane_2023,
  title = {On {{Hate Scaling Laws For Data-Swamps}}},
  author = {Birhane, Abeba and Prabhu, Vinay and Han, Sang and Boddeti, Vishnu Naresh},
  year = {2023},
  month = jun,
  number = {arXiv:2306.13141},
  eprint = {2306.13141},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.13141},
  urldate = {2023-07-03},
  abstract = {`Scale the model, scale the data, scale the GPU-farms' is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts remain under explored. This is especially of critical importance in the context of visio-linguistic datasets whose main source is the World Wide Web, condensed and packaged as the CommonCrawl dump. This large scale data-dump, which is known to have numerous drawbacks, is repeatedly mined and serves as the data-motherlode for large generative models. In this paper, we: 1) investigate the effect of scaling datasets on hateful content through a comparative audit of the LAION-400M and LAION-2B-en, containing 400 million and 2 billion samples respectively, and 2) evaluate the downstream impact of scale on visio-linguistic models trained on these dataset variants by measuring racial bias of the models trained on them using the Chicago Face Dataset (CFD) as a probe. Our results show that 1) the presence of hateful content in datasets, when measured with a Hate Content Rate (HCR) metric on the inferences of the Pysentimiento hate-detection Natural Language Processing (NLP) model, increased by nearly \$12{\textbackslash}\%\$ and 2) societal biases and negative stereotypes were also exacerbated with scale on the models we evaluated. As scale increased, the tendency of the model to associate images of human faces with the `human being' class over 7 other offensive classes reduced by half. Furthermore, for the Black female category, the tendency of the model to associate their faces with the `criminal' class doubled, while quintupling for Black male faces. We present a qualitative and historical analysis of the model audit results, reflect on our findings and its implications for dataset curation practice, and close with a summary of our findings and potential future work to be done in this area.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computers and Society},
  annotation = {\{"size": 6239992, "pages": 27, "previous": "0 citations (Semantic Scholar/arXiv) [2023-07-03]{\textbackslash}n0 citations (Semantic Scholar/DOI) [2023-07-03]{\textbackslash}narXiv:2306.13141 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/CXGZHNFJ/Birhane_et_al_(2023)_On_Hate_Scaling_Laws_For_Data-Swamps.pdf}
}

@article{lukas_2023,
  title = {Analyzing {{Leakage}} of {{Personally Identifiable Information}} in {{Language Models}}},
  author = {Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and {Zanella-B{\'e}guelin}, Santiago},
  year = {2023},
  doi = {10.48550/ARXIV.2302.00539},
  urldate = {2023-07-05},
  abstract = {Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10\${\textbackslash}times\$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3\% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing\_pii\_leakage.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {/unread,FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {\{"size": 1224364, "pages": 18, "previous": "5 citations (Semantic Scholar/arXiv) [2023-07-06]{\textbackslash}n5 citations (Semantic Scholar/DOI) [2023-07-06]{\textbackslash}nPublisher: arXiv{\textbackslash}nVersion Number: 4"\}},
  file = {/Users/chenghao/Zotero/storage/R5NSNBR6/Lukas et al. - 2023 - Analyzing Leakage of Personally Identifiable Infor.pdf}
}

@inproceedings{papadopoulou_2022,
  title = {Neural {{Text Sanitization}} with {{Explicit Measures}} of {{Privacy Risk}}},
  booktitle = {Proceedings of the 2nd {{Conference}} of the {{Asia-Pacific Chapter}} of the {{Association}} for {{Computational Linguistics}} and the 12th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Papadopoulou, Anthi and Yu, Yunhao and Lison, Pierre and {\O}vrelid, Lilja},
  year = {2022},
  month = nov,
  pages = {217--229},
  publisher = {Association for Computational Linguistics},
  address = {Online only},
  urldate = {2023-07-05},
  abstract = {We present a novel approach for text sanitization, which is the task of editing a document to mask all (direct and indirect) personal identifiers and thereby conceal the identity of the individuals(s) mentioned in the text. In contrast to previous work, the approach relies on explicit measures of privacy risk, making it possible to explicitly control the trade-off between privacy protection and data utility. The approach proceeds in three steps. A neural, privacy-enhanced entity recognizer is first employed to detect and classify potential personal identifiers. We then determine which entities, or combination of entities, are likely to pose a re-identification risk through a range of privacy risk assessment measures. We present three such measures of privacy risk, respectively based on (1) span probabilities derived from a BERT language model, (2) web search queries and (3) a classifier trained on labelled data. Finally, a linear optimization solver decides which entities to mask to minimize the semantic loss while simultaneously ensuring that the estimated privacy risk remains under a given threshold. We evaluate the approach both in the absence and presence of manually annotated data. Our results highlight the potential of the approach, as well as issues specific types of personal data can introduce to the process.},
  keywords = {/unread},
  file = {/Users/chenghao/Zotero/storage/6MRP82FU/Papadopoulou et al. - 2022 - Neural Text Sanitization with Explicit Measures of.pdf}
}

@misc{kleinberg_2022,
  title = {Textwash -- Automated Open-Source Text Anonymisation},
  author = {Kleinberg, Bennett and Davies, Toby and Mozes, Maximilian},
  year = {2022},
  month = aug,
  number = {arXiv:2208.13081},
  eprint = {2208.13081},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-05},
  abstract = {The increased use of text data in social science research has benefited from easy-to-access data (e.g., Twitter). That trend comes at the cost of research requiring sensitive but hard-to-share data (e.g., interview data, police reports, electronic health records). We introduce a solution to that stalemate with the open-source text anonymisation software\_Textwash\_. This paper presents the empirical evaluation of the tool using the TILD criteria: a technical evaluation (how accurate is the tool?), an information loss evaluation (how much information is lost in the anonymisation process?) and a de-anonymisation test (can humans identify individuals from anonymised text data?). The findings suggest that Textwash performs similar to state-of-the-art entity recognition models and introduces a negligible information loss of 0.84\%. For the de-anonymisation test, we tasked humans to identify individuals by name from a dataset of crowdsourced person descriptions of very famous, semi-famous and non-existing individuals. The de-anonymisation rate ranged from 1.01-2.01\% for the realistic use cases of the tool. We replicated the findings in a second study and concluded that Textwash succeeds in removing potentially sensitive information that renders detailed person descriptions practically anonymous.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Computers and Society},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-07-06]},
  file = {/Users/chenghao/Zotero/storage/HR5DWPCG/Kleinberg et al. - 2022 - Textwash -- automated open-source text anonymisati.pdf;/Users/chenghao/Zotero/storage/GWG3XRLV/2208.html}
}

@article{pilan_2022,
  title = {The {{Text Anonymization Benchmark}} ({{TAB}}): {{A Dedicated Corpus}} and {{Evaluation Framework}} for {{Text Anonymization}}},
  shorttitle = {The {{Text Anonymization Benchmark}} ({{TAB}})},
  author = {Pil{\'a}n, Ildik{\'o} and Lison, Pierre and {\O}vrelid, Lilja and Papadopoulou, Anthi and S{\'a}nchez, David and Batet, Montserrat},
  year = {2022},
  month = dec,
  journal = {Computational Linguistics},
  volume = {48},
  number = {4},
  pages = {1053--1101},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00458},
  urldate = {2023-07-05},
  abstract = {Abstract             We present a novel benchmark and associated evaluation metrics for assessing the performance of text anonymization methods. Text anonymization, defined as the task of editing a text document to prevent the disclosure of personal information, currently suffers from a shortage of privacy-oriented annotated text resources, making it difficult to properly evaluate the level of privacy protection offered by various anonymization methods. This paper presents TAB (Text Anonymization Benchmark), a new, open-source annotated corpus developed to address this shortage. The corpus comprises 1,268 English-language court cases from the European Court of Human Rights (ECHR) enriched with comprehensive annotations about the personal information appearing in each document, including their semantic category, identifier type, confidential attributes, and co-reference relations. Compared with previous work, the TAB corpus is designed to go beyond traditional de-identification (which is limited to the detection of predefined semantic categories), and explicitly marks which text spans ought to be masked in order to conceal the identity of the person to be protected.             Along with presenting the corpus and its annotation layers, we also propose a set of evaluation metrics that are specifically tailored toward measuring the performance of text anonymization, both in terms of privacy protection and utility preservation. We illustrate the use of the benchmark and the proposed metrics by assessing the empirical performance of several baseline text anonymization models. The full corpus along with its privacy-oriented annotation guidelines, evaluation scripts, and baseline models are available on: https://github.com/NorskRegnesentral/text-anonymization-benchmark.},
  langid = {english},
  keywords = {/unread},
  annotation = {\{"size": 1249147, "pages": 49, "previous": "17 citations (Semantic Scholar/DOI) [2023-07-06]"\}},
  file = {/Users/chenghao/Zotero/storage/5GUC9694/Piln_et_al_(2022)_The_Text_Anonymization_Benchmark_(TAB).pdf}
}

@article{csanyi_2021,
  title = {Challenges and {{Open Problems}} of {{Legal Document Anonymization}}},
  author = {Cs{\'a}nyi, Gergely M{\'a}rk and Nagy, D{\'a}niel and V{\'a}gi, Ren{\'a}t{\'o} and Vad{\'a}sz, J{\'a}nos P{\'a}l and Orosz, Tam{\'a}s},
  year = {2021},
  month = aug,
  journal = {Symmetry},
  volume = {13},
  number = {8},
  pages = {1490},
  issn = {2073-8994},
  doi = {10.3390/sym13081490},
  urldate = {2023-07-05},
  abstract = {Data sharing is a central aspect of judicial systems. The openly accessible documents can make the judiciary system more transparent. On the other hand, the published legal documents can contain much sensitive information about the involved persons or companies. For this reason, the anonymization of these documents is obligatory to prevent privacy breaches. General Data Protection Regulation (GDPR) and other modern privacy-protecting regulations have strict definitions of private data containing direct and indirect identifiers. In legal documents, there is a wide range of attributes regarding the involved parties. Moreover, legal documents can contain additional information about the relations between the involved parties and rare events. Hence, the personal data can be represented by a sparse matrix of these attributes. The application of Named Entity Recognition methods is essential for a fair anonymization process but is not enough. Machine learning-based methods should be used together with anonymization models, such as differential privacy, to reduce re-identification risk. On the other hand, the information content (utility) of the text should be preserved. This paper aims to summarize and highlight the open and symmetrical problems from the fields of structured and unstructured text anonymization. The possible methods for anonymizing legal documents discussed and illustrated by case studies from the Hungarian legal practice.},
  langid = {english},
  keywords = {/unread},
  annotation = {\{"size": 1209150, "pages": 25, "previous": "18 citations (Semantic Scholar/DOI) [2023-07-06]"\}},
  file = {/Users/chenghao/Zotero/storage/ZUMCQBD6/Csnyi_et_al_(2021)_Challenges_and_Open_Problems_of_Legal_Document_Anonymization.pdf}
}

@misc{ding_2023,
  title = {{{LongNet}}: {{Scaling Transformers}} to 1,000,000,000 {{Tokens}}},
  shorttitle = {{{LongNet}}},
  author = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Wei, Furu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02486},
  eprint = {2307.02486},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.02486},
  urldate = {2023-07-06},
  abstract = {Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 483820, "pages": 15, "previous": "arXiv:2307.02486 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/C22FMD8K/Ding_et_al_(2023)_LongNet.pdf}
}

@inproceedings{lison_2021,
  title = {Anonymisation {{Models}} for {{Text Data}}: {{State}} of the Art, {{Challenges}} and {{Future Directions}}},
  shorttitle = {Anonymisation {{Models}} for {{Text Data}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lison, Pierre and Pil{\'a}n, Ildik{\'o} and Sanchez, David and Batet, Montserrat and {\O}vrelid, Lilja},
  year = {2021},
  month = aug,
  pages = {4188--4203},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.323},
  urldate = {2023-07-10},
  abstract = {This position paper investigates the problem of automated text anonymisation, which is a prerequisite for secure sharing of documents containing sensitive information about individuals. We summarise the key concepts behind text anonymisation and provide a review of current approaches. Anonymisation methods have so far been developed in two fields with little mutual interaction, namely natural language processing and privacy-preserving data publishing. Based on a case study, we outline the benefits and limitations of these approaches and discuss a number of open challenges, such as (1) how to account for multiple types of semantic inferences, (2) how to strike a balance between disclosure risk and data utility and (3) how to evaluate the quality of the resulting anonymisation. We lay out a case for moving beyond sequence labelling models and incorporate explicit measures of disclosure risk into the text anonymisation process.},
  keywords = {/unread},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/chenghao/Zotero/storage/NDVLSZSI/Lison et al. - 2021 - Anonymisation Models for Text Data State of the a.pdf}
}

@misc{liesenfeld_2023,
  title = {Opening up {{ChatGPT}}: {{Tracking}} Openness, Transparency, and Accountability in Instruction-Tuned Text Generators},
  shorttitle = {Opening up {{ChatGPT}}},
  author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
  year = {2023},
  month = jul,
  eprint = {2307.05532},
  primaryclass = {cs},
  doi = {10.1145/3571884.3604316},
  urldate = {2023-07-17},
  abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI's ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-07-17]\\
0 citations (Semantic Scholar/DOI) [2023-07-17]},
  file = {/Users/chenghao/Zotero/storage/DB656MRP/Liesenfeld et al. - 2023 - Opening up ChatGPT Tracking openness, transparenc.pdf}
}

@misc{chen_2023,
  title = {Extending {{Context Window}} of {{Large Language Models}} via {{Positional Interpolation}}},
  author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  year = {2023},
  month = jun,
  number = {arXiv:2306.15595},
  doi = {10.48550/arXiv.2306.15595},
  urldate = {2023-07-17},
  abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least \${\textbackslash}sim 600 {\textbackslash}times\$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 751217, "pages": 18, "previous": "1 citations (Semantic Scholar/arXiv) [2023-07-17]{\textbackslash}n1 citations (Semantic Scholar/DOI) [2023-07-17]{\textbackslash}narXiv:2306.15595 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/LEMWM4WI/Chen et al. - 2023 - Extending Context Window of Large Language Models via Positional Interpolation.pdf}
}

@misc{liu_2023a,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03172},
  doi = {10.48550/arXiv.2307.03172},
  urldate = {2023-07-17},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {\{"size": 804284, "pages": 15, "previous": "1 citations (Semantic Scholar/arXiv) [2023-07-17]{\textbackslash}n1 citations (Semantic Scholar/DOI) [2023-07-17]{\textbackslash}narXiv:2307.03172 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/ZD2SJ3FS/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long Contexts.pdf}
}

@misc{rooij_2023,
  title = {Reclaiming {{AI}} as a Theoretical Tool for Cognitive Science},
  author = {van Rooij, Iris and Guest, Olivia and Adolfi, Federico G. and de Haan, Ronald and Kolokolova, Antonina and Rich, Patricia},
  year = {2023},
  month = aug,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/4cbuv},
  urldate = {2023-08-07},
  abstract = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
  langid = {american},
  keywords = {artificial intelligence (AI),Cognitive Psychology,cognitive science,computational complexity,engineering,explanation,Meta-science,Social and Behavioral Sciences,theory,Theory and Philosophy of Science},
  annotation = {\{"size": 513481, "pages": 21, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/BU8AWR2I/Rooij et al. - 2023 - Reclaiming AI as a theoretical tool for cognitive .pdf}
}

@misc{lenat_2023,
  title = {Getting from {{Generative AI}} to {{Trustworthy AI}}: {{What LLMs}} Might Learn from {{Cyc}}},
  shorttitle = {Getting from {{Generative AI}} to {{Trustworthy AI}}},
  author = {Lenat, Doug and Marcus, Gary},
  year = {2023},
  month = jul,
  number = {arXiv:2308.04445},
  eprint = {2308.04445},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.04445},
  urldate = {2023-08-10},
  abstract = {Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable. We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however a catch: if the logical language is expressive enough to fully represent the meaning of anything we can say in English, then the inference engine runs much too slowly. That's why symbolic AI systems typically settle for some fast but much less expressive logic, such as knowledge graphs. We describe how one AI system, Cyc, has developed ways to overcome that tradeoff and is able to reason in higher order logic in real time. We suggest that any trustworthy general AI will need to hybridize the approaches, the LLM approach and more formal approach, and lay out a path to realizing that dream.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0},
  annotation = {\{"size": 415258, "pages": 21, "previous": "arXiv:2308.04445 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/FQXV2AM6/Lenat and Marcus - 2023 - Getting from Generative AI to Trustworthy AI What.pdf}
}

@misc{grosse_2023,
  title = {Studying {{Large Language Model Generalization}} with {{Influence Functions}}},
  author = {Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage, Nelson and Tamkin, Alex and Tajdini, Amirhossein and Steiner, Benoit and Li, Dustin and Durmus, Esin and Perez, Ethan and Hubinger, Evan and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared and Bowman, Samuel R.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03296},
  eprint = {2308.03296},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.03296},
  urldate = {2023-08-11},
  abstract = {When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {\{"size": 3772019, "pages": 119, "previous": "arXiv:2308.03296 [cs, stat]"\}},
  file = {/Users/chenghao/Zotero/storage/WMWTLVGR/Grosse et al. - 2023 - Studying Large Language Model Generalization with .pdf}
}

@misc{widder_2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Open ({{For Business}}): {{Big Tech}}, {{Concentrated Power}}, and the {{Political Economy}} of {{Open AI}}},
  shorttitle = {Open ({{For Business}})},
  author = {Widder, David Gray and West, Sarah and Whittaker, Meredith},
  year = {2023},
  month = aug,
  number = {4543807},
  address = {Rochester, NY},
  urldate = {2023-08-17},
  abstract = {This paper examines `open' AI in the context of recent attention to open and open source AI systems. We find that the terms `open' and `open source' are used in confusing and diverse ways, often constituting more aspiration or marketing than technical descriptor, and frequently blending concepts from both open source software and open science. This complicates an already complex landscape, in which there is currently no agreed on definition of `open' in the context of AI, and as such the term is being applied to widely divergent offerings with little reference to a stable descriptor. So, what exactly is `open' about `open' AI, and what does `open' AI enable? To better answer these questions we begin this paper by looking at the various resources required to create and deploy AI systems, alongside the components that comprise these systems. We do this with an eye to which of these can, or cannot, be made open to scrutiny, reuse, and extension. What does `open' mean in practice, and what are its limits in the context of AI? We find that while a handful of maximally open AI systems exist, which offer intentional and extensive transparency, reusability, and extensibility-- the resources needed to build AI from scratch, and to deploy large AI systems at scale, remain `closed'---available only to those with significant (almost always corporate) resources. From here, we zoom out and examine the history of open source, its cleave from free software in the mid 1990s, and the contested processes by which open source has been incorporated into, and instrumented by, large tech corporations. As a current day example of the overbroad and ill-defined use of the term by tech companies, we look at  `open' in the context of OpenAI the company. We trace its moves from a humanity-focused nonprofit to a for-profit partnered with Microsoft, and its shifting position on `open' AI. Finally, we examine the current discourse around `open' AI--looking at how the term and the (mis)understandings about what `open' enables are being deployed to shape the public's and policymakers' understanding about AI, its capabilities, and the power of the AI industry. In particular, we examine the arguments being made for and against `open' and open source AI, who's making them, and how they are being deployed in the debate over AI regulation. Taken together, we find that `open' AI can, in its more maximal instantiations, provide transparency, reusability, and extensibility that can enable third parties to deploy and build on top of powerful off-the-shelf AI models. These maximalist forms of `open' AI can also allow some forms of auditing and oversight. But even the most open of `open' AI systems do not, on their own, ensure democratic access to or meaningful competition in AI, nor does openness alone solve the problem of oversight and scrutiny. While we recognize that there is a vibrant community of earnest contributors building and contributing to `open' AI efforts in the name of expanding access and insight, we also find that marketing around openness and investment in (somewhat) open AI systems is being leveraged by powerful companies to bolster their positions in the face of growing interest in AI regulation. And that some companies have moved to embrace `open' AI as a mechanism to entrench dominance, using the rhetoric of `open' AI to expand market power while investing in `open' AI efforts in ways that allow them to set standards of development while benefiting from the free labor of open source contributors.},
  langid = {english},
  keywords = {AI,artificial intelligence,Big Tech,competition,data,open source,policy,political economy,privacy},
  file = {/Users/chenghao/Zotero/storage/EAJ8HB3A/Widder et al. - 2023 - Open (For Business) Big Tech, Concentrated Power,.pdf}
}

@misc{leviathan_2023,
  title = {Fast {{Inference}} from {{Transformers}} via {{Speculative Decoding}}},
  author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  year = {2023},
  month = may,
  number = {arXiv:2211.17192},
  eprint = {2211.17192},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.17192},
  urldate = {2023-09-02},
  abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/SU5Z4XYC/Leviathan et al. - 2023 - Fast Inference from Transformers via Speculative Decoding.pdf}
}

@misc{kudugunta_2023,
  title = {{{MADLAD-400}}: {{A Multilingual And Document-Level Large Audited Dataset}}},
  shorttitle = {{{MADLAD-400}}},
  author = {Kudugunta, Sneha and Caswell, Isaac and Zhang, Biao and Garcia, Xavier and {Choquette-Choo}, Christopher A. and Lee, Katherine and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan},
  year = {2023},
  month = sep,
  number = {arXiv:2309.04662},
  eprint = {2309.04662},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.04662},
  urldate = {2023-09-13},
  abstract = {We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/MLTNEEKK/Kudugunta et al. - 2023 - MADLAD-400 A Multilingual And Document-Level Large Audited Dataset.pdf}
}

@misc{lacki_2018,
  title = {Connected {{Components}} at {{Scale}} via {{Local Contractions}}},
  author = {{\L}{\k a}cki, Jakub and Mirrokni, Vahab and W{\l}odarczyk, Micha{\l}},
  year = {2018},
  month = jul,
  number = {arXiv:1807.10727},
  eprint = {1807.10727},
  publisher = {arXiv},
  urldate = {2023-09-17},
  abstract = {As a fundamental tool in hierarchical graph clustering, computing connected components has been a central problem in large-scale data mining. While many known algorithms have been developed for this problem, they are either not scalable in practice or lack strong theoretical guarantees on the parallel running time, that is, the number of communication rounds. So far, the best proven guarantee is \${\textbackslash}Oh({\textbackslash}log n)\$, which matches the running time in the PRAM model. In this paper, we aim to design a distributed algorithm for this problem that works well in theory and practice. In particular, we present a simple algorithm based on contractions and provide a scalable implementation of it in MapReduce. On the theoretical side, in addition to showing \${\textbackslash}Oh({\textbackslash}log n)\$ convergence for all graphs, we prove an \${\textbackslash}Oh({\textbackslash}log {\textbackslash}log n)\$ parallel running time with high probability for a certain class of random graphs. We work in the MPC model that captures popular parallel computing frameworks, such as MapReduce, Hadoop or Spark. On the practical side, we show that our algorithm outperforms the state-of-the-art MapReduce algorithms. To confirm its scalability, we report empirical results on graphs with several trillions of edges.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Distributed Parallel and Cluster Computing},
  annotation = {\{"size": 344578, "pages": 12, "previous": "arXiv:1807.10727 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/GXVNVFT6/cki et al. - 2018 - Connected Components at Scale via Local Contractions.pdf}
}

@article{berglund_,
  title = {The {{Reversal Curse}}: {{LLMs}} Trained on ``{{A}} Is {{B}}'' Fail to Learn ``{{B}} Is {{A}}''},
  author = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  abstract = {We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ``A is B'', it will not automatically generalize to the reverse direction ``B is A''. This is the Reversal Curse. For instance, if a model is trained on ``Olaf Scholz was the ninth Chancellor of Germany'', it will not automatically be able to answer the question, ``Who was the ninth Chancellor of Germany?''. Moreover, the likelihood of the correct answer (``Olaf Scholz'') will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if ``A is B'' occurs, ``B is A'' is more likely to occur).},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/VKV6MA44/Berglund et al. - The Reversal Curse LLMs trained on A is B fail to learn B is A.pdf}
}

@misc{_f,
  title = {Jobs -- {{Dataproc}} -- Huggingface-Science{\dots} -- {{Google Cloud}} Console - {{https://console.cloud.google.com/dataproc/jobs?project=huggingface-science-codeparrot}}},
  urldate = {2023-10-09},
  howpublished = {https://console.cloud.google.com/dataproc/jobs?project=huggingface-science-codeparrot}
}

@misc{_g,
  title = {Jb225an3grcw5c7ba5tojmivz4-Dot-Us-Central1.Dataproc.Googleusercontent.Com/Gateway/Default/Yarn/Proxy/Application\_1696879436608\_0029/Jobs/Job/?Id=3 - {{https://jb225an3grcw5c7ba5tojmivz4-dot-us-central1.dataproc.googleusercontent.com/gateway/default/yarn/proxy/application\_1696879436608\_0029/jobs/job/?id=3}}},
  urldate = {2023-10-11},
  howpublished = {https://jb225an3grcw5c7ba5tojmivz4-dot-us-central1.dataproc.googleusercontent.com/gateway/default/yarn/proxy/application\_1696879436608\_0029/jobs/job/?id=3}
}

@misc{shi_2023a,
  title = {Detecting {{Pretraining Data}} from {{Large Language Models}}},
  author = {Shi, Weijia and Ajith, Anirudh and Xia, Mengzhou and Huang, Yangsibo and Liu, Daogao and Blevins, Terra and Chen, Danqi and Zettlemoyer, Luke},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16789},
  eprint = {2310.16789},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.16789},
  urldate = {2023-11-02},
  abstract = {Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K\% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K\% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K\% Prob achieves a 7.4\% improvement on WIKIMIA over these previous methods. We apply Min-K\% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Shi et_al/2023/Shi_et_al_(2023)_Detecting_Pretraining_Data_from_Large_Language_Models.pdf}
}

@misc{elazar_2023,
  title = {What's {{In My Big Data}}?},
  author = {Elazar, Yanai and Bhagia, Akshita and Magnusson, Ian and Ravichander, Abhilasha and Schwenk, Dustin and Suhr, Alane and Walsh, Pete and Groeneveld, Dirk and Soldaini, Luca and Singh, Sameer and Hajishirzi, Hanna and Smith, Noah A. and Dodge, Jesse},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20707},
  eprint = {2310.20707},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.20707},
  urldate = {2023-11-02},
  abstract = {Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50\% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them: github.com/allenai/wimbd.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Elazar et_al/2023/Elazar_et_al_(2023)_What's_In_My_Big_Data.pdf}
}

@misc{milliere_2023,
  title = {The {{Alignment Problem}} in {{Context}}},
  author = {Milli{\`e}re, Rapha{\"e}l},
  year = {2023},
  month = nov,
  number = {arXiv:2311.02147},
  eprint = {2311.02147},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.02147},
  urldate = {2023-11-08},
  abstract = {A core challenge in the development of increasingly capable AI systems is to make them safe and reliable by ensuring their behaviour is consistent with human values. This challenge, known as the alignment problem, does not merely apply to hypothetical future AI systems that may pose catastrophic risks; it already applies to current systems, such as large language models, whose potential for harm is rapidly increasing. In this paper, I assess whether we are on track to solve the alignment problem for large language models, and what that means for the safety of future AI systems. I argue that existing strategies for alignment are insufficient, because large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I offer an explanation of this lingering vulnerability on which it is not simply a contingent limitation of current language models, but has deep technical ties to a crucial aspect of what makes these models useful and versatile in the first place -- namely, their remarkable aptitude to learn "in context" directly from user instructions. It follows that the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities. Furthermore, this assessment raises concerns about the prospect of ensuring the safety of future and more capable AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {\{"size": 473766, "pages": 25, "previous": "arXiv:2311.02147 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/FXD3AZPK/Millire_(2023)_The_Alignment_Problem_in_Context.pdf}
}

@misc{tunstall_2023,
  title = {Zephyr: {{Direct Distillation}} of {{LM Alignment}}},
  shorttitle = {Zephyr},
  author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and {von Werra}, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and Sarrazin, Nathan and Sanseviero, Omar and Rush, Alexander M. and Wolf, Thomas},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16944},
  eprint = {2310.16944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.16944},
  urldate = {2023-11-09},
  abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/WAV78WQD/Tunstall et al. - 2023 - Zephyr Direct Distillation of LM Alignment.pdf}
}

@misc{yu_2023a,
  title = {White-{{Box Transformers}} via {{Sparse Rate Reduction}}: {{Compression Is All There Is}}?},
  shorttitle = {White-{{Box Transformers}} via {{Sparse Rate Reduction}}},
  author = {Yu, Yaodong and Buchanan, Sam and Pai, Druv and Chu, Tianzhe and Wu, Ziyang and Tong, Shengbang and Bai, Hao and Zhai, Yuexiang and Haeffele, Benjamin D. and Ma, Yi},
  year = {2023},
  month = nov,
  number = {arXiv:2311.13110},
  eprint = {2311.13110},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.13110},
  urldate = {2023-11-24},
  abstract = {In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 44294368, "pages": 124, "previous": "arXiv:2311.13110 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/NTWD4LV6/Yu et al. - 2023 - White-Box Transformers via Sparse Rate Reduction Compression Is All There Is.pdf}
}

@misc{nasr_2023,
  title = {Scalable {{Extraction}} of {{Training Data}} from ({{Production}}) {{Language Models}}},
  author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and {Choquette-Choo}, Christopher A. and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  year = {2023},
  month = nov,
  number = {arXiv:2311.17035},
  eprint = {2311.17035},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.17035},
  urldate = {2023-11-29},
  abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  annotation = {\{"size": 1433005, "pages": 64, "previous": "arXiv:2311.17035 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/2AMQJPVX/Nasr et al. - 2023 - Scalable Extraction of Training Data from (Production) Language Models.pdf}
}

@misc{jones_2023,
  title = {Does {{GPT-4 Pass}} the {{Turing Test}}?},
  author = {Jones, Cameron and Bergen, Benjamin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20216},
  eprint = {2310.20216},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.20216},
  urldate = {2023-12-02},
  abstract = {We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41\% of games, outperforming baselines set by ELIZA (27\%) and GPT-3.5 (14\%), but falling short of chance and the baseline set by human participants (63\%). Participants' decisions were based mainly on linguistic style (35\%) and socio-emotional traits (27\%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/S8DLVSU3/Jones and Bergen - 2023 - Does GPT-4 Pass the Turing Test.pdf}
}

@misc{gu_2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  year = {2023},
  month = dec,
  number = {arXiv:2312.00752},
  eprint = {2312.00752},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.00752},
  urldate = {2023-12-04},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {\{"size": 1293868, "pages": 37, "previous": "arXiv:2312.00752 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/5WW2TPFN/Gu and Dao - 2023 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf}
}

@misc{hewitt_2023,
  title = {Backpack {{Language Models}}},
  author = {Hewitt, John and Thickstun, John and Manning, Christopher D. and Liang, Percy},
  year = {2023},
  month = may,
  number = {arXiv:2305.16765},
  eprint = {2305.16765},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16765},
  urldate = {2023-12-05},
  abstract = {We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/N4PKWVAQ/Hewitt et al. - 2023 - Backpack Language Models.pdf}
}

@misc{mitra_2023,
  title = {Orca 2: {{Teaching Small Language Models How}} to {{Reason}}},
  shorttitle = {Orca 2},
  author = {Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and Palangi, Hamid and Zheng, Guoqing and Rosset, Corby and Khanpour, Hamed and Awadallah, Ahmed},
  year = {2023},
  month = nov,
  number = {arXiv:2311.11045},
  eprint = {2311.11045},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.11045},
  urldate = {2023-12-09},
  abstract = {Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {\{"size": 1127197, "pages": 53, "previous": "arXiv:2311.11045 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/EM7X8KUB/Mitra_et_al_(2023)_Orca_2.pdf}
}

@misc{belcak_2023,
  title = {Exponentially {{Faster Language Modelling}}},
  author = {Belcak, Peter and Wattenhofer, Roger},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10770},
  eprint = {2311.10770},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.10770},
  urldate = {2023-12-09},
  abstract = {Language models only really need to use an exponential fraction of their neurons for individual inferences. As proof, we present UltraFastBERT, a BERT variant that uses 0.3\% of its neurons during inference while performing on par with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095 neurons for each layer inference. This is achieved by replacing feedforward networks with fast feedforward networks (FFFs). While no truly efficient implementation currently exists to unlock the full acceleration potential of conditional neural execution, we provide high-level CPU code achieving 78x speedup over the optimized baseline feedforward implementation, and a PyTorch implementation delivering 40x speedup over the equivalent batched feedforward inference. We publish our training code, benchmarking setup, and model weights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Belcak_Wattenhofer/2023/Belcak_Wattenhofer_(2023)_Exponentially_Faster_Language_Modelling.pdf}
}

@misc{he_2023,
  title = {Simplifying {{Transformer Blocks}}},
  author = {He, Bobby and Hofmann, Thomas},
  year = {2023},
  month = nov,
  number = {arXiv:2311.01906},
  eprint = {2311.01906},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01906},
  urldate = {2023-12-09},
  abstract = {A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections \& normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable. In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15\% faster training throughput, and using 15\% fewer parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {\{"size": 5303539, "pages": 27, "previous": "arXiv:2311.01906 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/UUXBH243/He_Hofmann_(2023)_Simplifying_Transformer_Blocks.pdf}
}

@misc{tamkin_2023,
  title = {Evaluating and {{Mitigating Discrimination}} in {{Language Model Decisions}}},
  author = {Tamkin, Alex and Askell, Amanda and Lovitt, Liane and Durmus, Esin and Joseph, Nicholas and Kravec, Shauna and Nguyen, Karina and Kaplan, Jared and Ganguli, Deep},
  year = {2023},
  month = dec,
  number = {arXiv:2312.03689},
  eprint = {2312.03689},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.03689},
  urldate = {2023-12-12},
  abstract = {As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {\{"size": 3169819, "pages": 28, "previous": "arXiv:2312.03689 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/2DMGIYY3/Tamkin_et_al_(2023)_Evaluating_and_Mitigating_Discrimination_in_Language_Model_Decisions.pdf}
}

@misc{magnusson_2023,
  title = {Paloma: {{A Benchmark}} for {{Evaluating Language Model Fit}}},
  shorttitle = {Paloma},
  author = {Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and Schwenk, Dustin and Walsh, Evan Pete and Elazar, Yanai and Lo, Kyle and Groeneveld, Dirk and Beltagy, Iz and Hajishirzi, Hannaneh and Smith, Noah A. and Richardson, Kyle and Dodge, Jesse},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10523},
  eprint = {2312.10523},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10523},
  urldate = {2023-12-19},
  abstract = {Language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains\${\textbackslash}unicode\{x2013\}\$varying distributions of language. Rather than assuming perplexity on one distribution extrapolates to others, Perplexity Analysis for Language Model Assessment (Paloma), measures LM fit to 585 text domains, ranging from nytimes.com to r/depression on Reddit. We invite submissions to our benchmark and organize results by comparability based on compliance with guidelines such as removal of benchmark contamination from pretraining. Submissions can also record parameter and training token count to make comparisons of Pareto efficiency for performance as a function of these measures of cost. We populate our benchmark with results from 6 baselines pretrained on popular corpora. In case studies, we demonstrate analyses that are possible with Paloma, such as finding that pretraining without data beyond Common Crawl leads to inconsistent fit to many domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 4782118, "pages": 36, "previous": "arXiv:2312.10523 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/QC7IGSAL/Magnusson_et_al_(2023)_Paloma2.pdf}
}

@misc{groeneveld_2023,
  title = {Catwalk: {{A Unified Language Model Evaluation Framework}} for {{Many Datasets}}},
  shorttitle = {Catwalk},
  author = {Groeneveld, Dirk and Awadalla, Anas and Beltagy, Iz and Bhagia, Akshita and Magnusson, Ian and Peng, Hao and Tafjord, Oyvind and Walsh, Pete and Richardson, Kyle and Dodge, Jesse},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10253},
  eprint = {2312.10253},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10253},
  urldate = {2023-12-19},
  abstract = {The success of large language models has shifted the evaluation paradigms in natural language processing (NLP). The community's interest has drifted towards comparing NLP models across many tasks, domains, and datasets, often at an extreme scale. This imposes new engineering challenges: efforts in constructing datasets and models have been fragmented, and their formats and interfaces are incompatible. As a result, it often takes extensive (re)implementation efforts to make fair and controlled comparisons at scale. Catwalk aims to address these issues. Catwalk provides a unified interface to a broad range of existing NLP datasets and models, ranging from both canonical supervised training and fine-tuning, to more modern paradigms like in-context learning. Its carefully-designed abstractions allow for easy extensions to many others. Catwalk substantially lowers the barriers to conducting controlled experiments at scale. For example, we finetuned and evaluated over 64 models on over 86 datasets with a single command, without writing any code. Maintained by the AllenNLP team at the Allen Institute for Artificial Intelligence (AI2), Catwalk is an ongoing open-source effort: https://github.com/allenai/catwalk.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {\{"size": 648839, "pages": 16, "previous": "arXiv:2312.10253 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/CHVKPIZX/Groeneveld_et_al_(2023)_Catwalk.pdf}
}

@misc{alizadeh_2023,
  title = {{{LLM}} in a Flash: {{Efficient Large Language Model Inference}} with {{Limited Memory}}},
  shorttitle = {{{LLM}} in a Flash},
  author = {Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C. and Rastegari, Mohammad and Farajtabar, Mehrdad},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11514},
  eprint = {2312.11514},
  publisher = {arXiv},
  urldate = {2023-12-20},
  abstract = {Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, "windowing'" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 1092689, "pages": 13, "previous": "arXiv:2312.11514 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/6B4NDQKQ/Alizadeh_et_al_(2023)_LLM_in_a_flash.pdf}
}

@article{song_,
  title = {{{PowerInfer}}: {{Fast Large Language Model Serving}} with a {{Consumer-grade GPU}}},
  author = {Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  abstract = {This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a powerlaw distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18\% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69{\texttimes} while retaining model accuracy.},
  langid = {english},
  annotation = {\{"size": 483962, "pages": 15, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/8Z8KK4J2/Song et al. - PowerInfer Fast Large Language Model Serving with a Consumer-grade GPU.pdf}
}

@article{davis_,
  title = {Using a Large Language Model to Generate Program Mutations for a Genetic Algorithm to Search for Solutions to Combinatorial Problems: {{Review}} of ({{Romera-Paredes}} et al., 2023).},
  author = {Davis, Ernest},
  abstract = {FunSearch (Romera-Paredes et al., 2023) uses a large language model (LLM) of a component of an AI system that has generated some difficult-to-find solutions to combinatorial problems that are larger than was previously known. However, the excitement that has greeted this has been unwarranted. First, as compared to other applications of AI to mathematical research, FunSearch does not seem particularly promising as a technique and its contribution to mathematics is not unusually great. Second, FunSearch uses the LLM as a subroutine in a genetic programming algorithm, to generate mutations of one particular subroutine in a larger program. The LLM is not told either what problem is being addressed or what the overall program is. Thus, as compared to other application of LLMs to mathematics, the LLM in FunSearch is remarkable for the shallowness of the mathematical understanding that it seems to exhibit.},
  langid = {english},
  annotation = {\{"size": 311134, "pages": 13, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/YW8IVZGM/Davis - Using a large language model to generate program mutations for a genetic algorithm to search for sol.pdf}
}

@misc{kwon_2023,
  title = {Efficient {{Memory Management}} for {{Large Language Model Serving}} with {{PagedAttention}}},
  author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06180},
  eprint = {2309.06180},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.06180},
  urldate = {2023-12-21},
  abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  annotation = {\{"size": 1459631, "pages": 16, "previous": "arXiv:2309.06180 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/JUIDZQDG/Kwon_et_al_(2023)_Efficient_Memory_Management_for_Large_Language_Model_Serving_with_PagedAttention.pdf}
}

@misc{zhang_2023,
  title = {Cached {{Transformers}}: {{Improving Transformers}} with {{Differentiable Memory Cache}}},
  shorttitle = {Cached {{Transformers}}},
  author = {Zhang, Zhaoyang and Shao, Wenqi and Ge, Yixiao and Wang, Xiaogang and Gu, Jinwei and Luo, Ping},
  year = {2023},
  month = dec,
  number = {arXiv:2312.12742},
  eprint = {2312.12742},
  publisher = {arXiv},
  urldate = {2023-12-21},
  abstract = {This work introduces a new Transformer model called Cached Transformer, which uses Gated Recurrent Cached (GRC) attention to extend the self-attention mechanism with a differentiable memory cache of tokens. GRC attention enables attending to both past and current tokens, increasing the receptive field of attention and allowing for exploring long-range dependencies. By utilizing a recurrent gating unit to continuously update the cache, our model achieves significant advancements in {\textbackslash}textbf\{six\} language and vision tasks, including language modeling, machine translation, ListOPs, image classification, object detection, and instance segmentation. Furthermore, our approach surpasses previous memory-based techniques in tasks such as language modeling and displays the ability to be applied to a broader range of situations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 1332635, "pages": 12, "previous": "arXiv:2312.12742 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/KA75MZUA/Zhang_et_al_(2023)_Cached_Transformers.pdf}
}

@misc{sucholutsky_2023,
  title = {Getting Aligned on Representational Alignment},
  author = {Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C. and Grant, Erin and Groen, Iris and Achterberg, Jascha and Tenenbaum, Joshua B. and Collins, Katherine M. and Hermann, Katherine L. and Oktar, Kerem and Greff, Klaus and Hebart, Martin N. and Jacoby, Nori and Zhang, Qiuyi and Marjieh, Raja and Geirhos, Robert and Chen, Sherol and Kornblith, Simon and Rane, Sunayana and Konkle, Talia and O'Connell, Thomas P. and Unterthiner, Thomas and Lampinen, Andrew K. and M{\"u}ller, Klaus-Robert and Toneva, Mariya and Griffiths, Thomas L.},
  year = {2023},
  month = nov,
  number = {arXiv:2310.13018},
  eprint = {2310.13018},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.13018},
  urldate = {2023-12-22},
  abstract = {Biological and artificial information processing systems form representations that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the extent to which the representations formed by these diverse systems agree? Do similarities in representations then translate into similar behavior? How can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most active research areas in cognitive science, neuroscience, and machine learning. For example, cognitive scientists measure the representational alignment of multiple individuals to identify shared cognitive priors, neuroscientists align fMRI responses from multiple individuals into a shared representational space for group-level analyses, and ML researchers distill knowledge from teacher models into student models by increasing their alignment. Unfortunately, there is limited knowledge transfer between research communities interested in representational alignment, so progress in one field often ends up being rediscovered independently in another. Thus, greater cross-field communication would be advantageous. To improve communication between these fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from all three fields and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  annotation = {\{"size": 12888323, "pages": 47, "previous": "arXiv:2310.13018 [cs, q-bio]"\}},
  file = {/Users/chenghao/Zotero/storage/CIFHXCV6/Sucholutsky_et_al_(2023)_Getting_aligned_on_representational_alignment.pdf}
}

@misc{samragh_2023,
  title = {Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones},
  shorttitle = {Weight Subcloning},
  author = {Samragh, Mohammad and Farajtabar, Mehrdad and Mehta, Sachin and Vemulapalli, Raviteja and Faghri, Fartash and Naik, Devang and Tuzel, Oncel and Rastegari, Mohammad},
  year = {2023},
  month = dec,
  number = {arXiv:2312.09299},
  eprint = {2312.09299},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09299},
  urldate = {2023-12-22},
  abstract = {Training large transformer models from scratch for a target task requires lots of data and is computationally demanding. The usual practice of transfer learning overcomes this challenge by initializing the model with weights of a pretrained model of the same size and specification to increase the convergence and training speed. However, what if no pretrained model of the required size is available? In this paper, we introduce a simple yet effective technique to transfer the knowledge of a pretrained model to smaller variants. Our approach called weight subcloning expedites the training of scaled-down transformers by initializing their weights from larger pretrained models. Weight subcloning involves an operation on the pretrained model to obtain the equivalent initialized scaled-down model. It consists of two key steps: first, we introduce neuron importance ranking to decrease the embedding dimension per layer in the pretrained model. Then, we remove blocks from the transformer model to match the number of layers in the scaled-down network. The result is a network ready to undergo training, which gains significant improvements in training speed compared to random initialization. For instance, we achieve 4x faster training for vision transformers in image classification and language models designed for next token prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 1132959, "pages": 10, "previous": "arXiv:2312.09299 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/YX7VDZZ2/Samragh et al. - 2023 - Weight subcloning direct initialization of transformers using larger pretrained ones.pdf}
}

@misc{nylund_2023,
  title = {Time Is {{Encoded}} in the {{Weights}} of {{Finetuned Language Models}}},
  author = {Nylund, Kai and Gururangan, Suchin and Smith, Noah A.},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13401},
  eprint = {2312.13401},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.13401},
  urldate = {2023-12-22},
  abstract = {We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/4NCTAMGU/Nylund et al. - 2023 - Time is Encoded in the Weights of Finetuned Language Models.pdf}
}

@misc{you_2023,
  title = {Ferret: {{Refer}} and {{Ground Anything Anywhere}} at {{Any Granularity}}},
  shorttitle = {Ferret},
  author = {You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07704},
  eprint = {2310.07704},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07704},
  urldate = {2023-12-23},
  abstract = {We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at https://github.com/apple/ml-ferret},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/chenghao/Zotero/storage/RYP9PV4X/You et al. - 2023 - Ferret Refer and Ground Anything Anywhere at Any Granularity.pdf}
}

@article{rasenberg_2023,
  title = {Reimagining Language: {{Towards}} a Better Understanding of Language by Including Our Interactions with Non-Humans},
  shorttitle = {Reimagining Language},
  author = {Rasenberg, Marlou and Amha, Azeb and Coler, Matt and van Koppen, Marjo and van Miltenburg, Emiel and de Rijk, Lynn and Stommel, Wyke and Dingemanse, Mark},
  year = {2023},
  month = nov,
  journal = {Linguistics in the Netherlands},
  volume = {40},
  number = {1},
  pages = {309--317},
  issn = {0929-7332, 1569-9919},
  doi = {10.1075/avt.00095.ras},
  urldate = {2023-12-25},
  abstract = {Abstract What is language and who or what can be said to have it? In this essay we consider this question in the context of interactions with non-humans, specifically: animals and computers. While perhaps an odd pairing at first glance, here we argue that these domains can offer contrasting perspectives through which we can explore and reimagine language. The interactions between humans and animals, as well as between humans and computers, reveal both the essence and the boundaries of language: from examining the role of sequence and contingency in human-animal interaction, to unravelling the challenges of natural interactions with ``smart'' speakers and language models. By bringing together disparate fields around foundational questions, we push the boundaries of linguistic inquiry and uncover new insights into what language is and how it functions in diverse non-human-exclusive contexts.},
  langid = {english},
  annotation = {\{"size": 280500, "pages": 9, "previous": "Publisher: John Benjamins"\}},
  file = {/Users/chenghao/Zotero/storage/BKEJM2EH/Rasenberg et al. - 2023 - Reimagining language Towards a better understanding of language by including our interactions with .pdf}
}

@misc{lu_2023,
  title = {Unified-{{IO}} 2: {{Scaling Autoregressive Multimodal Models}} with {{Vision}}, {{Language}}, {{Audio}}, and {{Action}}},
  shorttitle = {Unified-{{IO}} 2},
  author = {Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha},
  year = {2023},
  month = dec,
  number = {arXiv:2312.17172},
  eprint = {2312.17172},
  publisher = {arXiv},
  urldate = {2023-12-29},
  abstract = {We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 14617196, "pages": 38, "previous": "arXiv:2312.17172 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/48AILZ6P/Lu et al. - 2023 - Unified-IO 2 Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action.pdf}
}

@misc{rafailov_2023,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  howpublished = {https://arxiv.org/abs/2305.18290v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/J697QUWN/Rafailov et al. - 2023 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf}
}

@misc{dettmers_2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  howpublished = {https://arxiv.org/abs/2305.14314v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/9IXMUB7C/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf}
}

@misc{wu_2023,
  title = {{{BloombergGPT}}: {{A Large Language Model}} for {{Finance}}},
  shorttitle = {{{BloombergGPT}}},
  author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.},
  howpublished = {https://arxiv.org/abs/2303.17564v3},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/5QWYG467/Wu et al. - 2023 - BloombergGPT A Large Language Model for Finance.pdf}
}

@misc{touvron_2023a,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  howpublished = {https://arxiv.org/abs/2307.09288v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/MTG78EZ4/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf}
}

@misc{jiang_2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  howpublished = {https://arxiv.org/abs/2310.06825v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/PAA3EDR8/Jiang et al. - 2023 - Mistral 7B.pdf}
}

@misc{li_2023a,
  title = {Task {{Contamination}}: {{Language Models May Not Be Few-Shot Anymore}}},
  shorttitle = {Task {{Contamination}}},
  author = {Li, Changmao and Flanigan, Jeffrey},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-01-01},
  abstract = {Large language models (LLMs) offer impressive performance in various zero-shot and few-shot tasks. However, their success in zero-shot and few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined. This paper investigates how zero-shot and few-shot performance of LLMs has changed chronologically over time. Utilizing GPT-3 series models and several other recent open-sourced LLMs, and controlling for dataset difficulty, we find that on datasets released before the LLM training data creation date, LLMs perform surprisingly better than on datasets released after. This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets released prior to the LLMs' training data creation date. Additionally, we utilize training data inspection, task example extraction, and a membership inference attack, which reveal further evidence of task contamination. Importantly, we find that for classification tasks with no possibility of task contamination, LLMs rarely demonstrate statistically significant improvements over simple majority baselines, in both zero and few-shot settings.},
  howpublished = {https://arxiv.org/abs/2312.16337v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/JG62PR2J/Li and Flanigan - 2023 - Task Contamination Language Models May Not Be Few-Shot Anymore.pdf}
}

@misc{kalluri_2023,
  title = {The {{Surveillance AI Pipeline}}},
  author = {Kalluri, Pratyusha Ria and Agnew, William and Cheng, Myra and Owens, Kentrell and Soldaini, Luca and Birhane, Abeba},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-01-03},
  abstract = {A rapidly growing number of voices argue that AI research, and computer vision in particular, is powering mass surveillance. Yet the direct path from computer vision research to surveillance has remained obscured and difficult to assess. Here, we reveal the Surveillance AI pipeline by analyzing three decades of computer vision research papers and downstream patents, more than 40,000 documents. We find the large majority of annotated computer vision papers and patents self-report their technology enables extracting data about humans. Moreover, the majority of these technologies specifically enable extracting data about human bodies and body parts. We present both quantitative and rich qualitative analysis illuminating these practices of human data extraction. Studying the roots of this pipeline, we find that institutions that prolifically produce computer vision research, namely elite universities and "big tech" corporations, are subsequently cited in thousands of surveillance patents. Further, we find consistent evidence against the narrative that only these few rogue entities are contributing to surveillance. Rather, we expose the fieldwide norm that when an institution, nation, or subfield authors computer vision papers with downstream patents, the majority of these papers are used in surveillance patents. In total, we find the number of papers with downstream surveillance patents increased more than five-fold between the 1990s and the 2010s, with computer vision research now having been used in more than 11,000 surveillance patents. Finally, in addition to the high levels of surveillance we find documented in computer vision papers and patents, we unearth pervasive patterns of documents using language that obfuscates the extent of surveillance. Our analysis reveals the pipeline by which computer vision research has powered the ongoing expansion of surveillance.},
  howpublished = {https://arxiv.org/abs/2309.15084v2},
  langid = {english},
  annotation = {\{"size": 3273321, "pages": 23, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/ZB7U2SHI/Kalluri et al. - 2023 - The Surveillance AI Pipeline.pdf}
}

@misc{tonmoy_2024,
  title = {A {{Comprehensive Survey}} of {{Hallucination Mitigation Techniques}} in {{Large Language Models}}},
  author = {Tonmoy, S. M. Towhidul Islam and Zaman, S. M. Mehedi and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-05},
  abstract = {As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.},
  howpublished = {https://arxiv.org/abs/2401.01313v2},
  langid = {english},
  annotation = {\{"size": 328464, "pages": 19, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/CN253NKM/Tonmoy_et_al_(2024)_A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language2.pdf}
}

@misc{jiang_2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-09},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  howpublished = {https://arxiv.org/abs/2401.04088v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/AFY4K6W6/Jiang et al. - 2024 - Mixtral of Experts.pdf}
}

@misc{pioro_2024,
  title = {{{MoE-Mamba}}: {{Efficient Selective State Space Models}} with {{Mixture}} of {{Experts}}},
  shorttitle = {{{MoE-Mamba}}},
  author = {Pi{\'o}ro, Maciej and Ciebiera, Kamil and Kr{\'o}l, Krystian and Ludziejewski, Jan and Jaszczur, Sebastian},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-09},
  abstract = {State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based LLMs, including recent state-of-the-art open-source models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable, Transformer-like performance. Our model, MoE-Mamba, outperforms both Mamba and Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in 2.2x less training steps while preserving the inference performance gains of Mamba against the Transformer.},
  howpublished = {https://arxiv.org/abs/2401.04081v1},
  langid = {english},
  annotation = {\{"size": 721554, "pages": 9, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/P57GTS4B/Piro et al. - 2024 - MoE-Mamba Efficient Selective State Space Models with Mixture of Experts.pdf}
}

@misc{milliere_2024,
  title = {A {{Philosophical Introduction}} to {{Language Models}} -- {{Part I}}: {{Continuity With Classic Debates}}},
  shorttitle = {A {{Philosophical Introduction}} to {{Language Models}} -- {{Part I}}},
  author = {Milli{\`e}re, Rapha{\"e}l and Buckner, Cameron},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-09},
  abstract = {Large language models like GPT-4 have achieved remarkable proficiency in a broad spectrum of language-based tasks, some of which are traditionally associated with hallmarks of human intelligence. This has prompted ongoing disagreements about the extent to which we can meaningfully ascribe any kind of linguistic or cognitive competence to language models. Such questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. This article -- the first part of two companion papers -- serves both as a primer on language models for philosophers, and as an opinionated survey of their significance in relation to classic debates in the philosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as compositionality, language acquisition, semantic competence, grounding, world models, and the transmission of cultural knowledge. We argue that the success of language models challenges several long-held assumptions about artificial neural networks. However, we also highlight the need for further empirical investigation to better understand their internal mechanisms. This sets the stage for the companion paper (Part II), which turns to novel empirical methods for probing the inner workings of language models, and new philosophical questions prompted by their latest developments.},
  howpublished = {https://arxiv.org/abs/2401.03910v1},
  langid = {english},
  annotation = {\{"size": 1066034, "pages": 30, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/SGUKZSEN/Millire and Buckner - 2024 - A Philosophical Introduction to Language Models -- Part I Continuity With Classic Debates.pdf}
}

@misc{qin_2024,
  title = {Lightning {{Attention-2}}: {{A Free Lunch}} for {{Handling Unlimited Sequence Lengths}} in {{Large Language Models}}},
  shorttitle = {Lightning {{Attention-2}}},
  author = {Qin, Zhen and Sun, Weigao and Li, Dong and Shen, Xuyang and Sun, Weixuan and Zhong, Yiran},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-10},
  abstract = {Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.},
  howpublished = {https://arxiv.org/abs/2401.04658v1},
  langid = {english},
  annotation = {\{"size": 709518, "pages": 11, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/T9ZTJGCV/Qin et al. - 2024 - Lightning Attention-2 A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models.pdf}
}

@misc{liu_2023c,
  title = {{{LLM360}}: {{Towards Fully Transparent Open-Source LLMs}}},
  shorttitle = {{{LLM360}}},
  author = {Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and Fan, Richard and Gu, Yi and Miller, Victor and Zhuang, Yonghao and He, Guowei and Li, Haonan and Koto, Fajri and Tang, Liping and Ranjan, Nikhil and Shen, Zhiqiang and Ren, Xuguang and Iriondo, Roberto and Mu, Cun and Hu, Zhiting and Schulze, Mark and Nakov, Preslav and Baldwin, Tim and Xing, Eric P.},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-01-10},
  abstract = {The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.},
  howpublished = {https://arxiv.org/abs/2312.06550v1},
  langid = {english},
  annotation = {\{"size": 8826637, "pages": 15, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/HSQGT2A3/Liu et al. - 2023 - LLM360 Towards Fully Transparent Open-Source LLMs.pdf}
}

@misc{jin_2024,
  title = {{{LLM Maybe LongLM}}: {{Self-Extend LLM Context Window Without Tuning}}},
  shorttitle = {{{LLM Maybe LongLM}}},
  author = {Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-12},
  abstract = {This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.},
  howpublished = {https://arxiv.org/abs/2401.01325v1},
  langid = {english},
  annotation = {\{"size": -1, "pages": -1, "previous": ""\}},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Jin et_al/2024/Jin_et_al_(2024)_LLM_Maybe_LongLM.pdf}
}

@misc{qin_2023,
  title = {Scaling {{TransNormer}} to 175 {{Billion Parameters}}},
  author = {Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and Qiao, Yu and Zhong, Yiran},
  year = {2023},
  month = jul,
  number = {arXiv:2307.14995},
  eprint = {2307.14995},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.14995},
  urldate = {2024-01-13},
  abstract = {We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20\%. Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. Scalability is at the heart of our model's design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics. Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure data quality and relevance, we implement a new self-cleaning strategy to filter our collected data. Our pre-trained models will be released to foster community advancements in efficient LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Qin et_al/2023/Qin_et_al_(2023)_Scaling_TransNormer_to_175_Billion_Parameters.pdf}
}

@misc{hubinger_2024,
  title = {Sleeper {{Agents}}: {{Training Deceptive LLMs}} That {{Persist Through Safety Training}}},
  shorttitle = {Sleeper {{Agents}}},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-15},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  howpublished = {https://arxiv.org/abs/2401.05566v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/TVHG2NGZ/Hubinger et al. - 2024 - Sleeper Agents Training Deceptive LLMs that Persist Through Safety Training.pdf}
}

@misc{zhang_2024,
  title = {Extending {{LLMs}}' {{Context Window}} with 100 {{Samples}}},
  author = {Zhang, Yikai and Li, Junlong and Liu, Pengfei},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-17},
  abstract = {Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF.},
  howpublished = {https://arxiv.org/abs/2401.07004v1},
  langid = {english},
  annotation = {\{"size": -1, "pages": -1, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/P8ECKNGV/Zhang_et_al_(2024)_Extending_LLMs'_Context_Window_with_100_Samples.pdf}
}

@misc{wei_2023,
  title = {Larger Language Models Do In-Context Learning Differently},
  author = {Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and Ma, Tengyu},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-01-20},
  abstract = {We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.},
  howpublished = {https://arxiv.org/abs/2303.03846v2},
  langid = {english},
  annotation = {\{"size": 964880, "pages": 51, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/2HNEC44H/Wei et al. - 2023 - Larger language models do in-context learning differently.pdf}
}

@misc{sorensen_2023,
  title = {Value {{Kaleidoscope}}: {{Engaging AI}} with {{Pluralistic Human Values}}, {{Rights}}, and {{Duties}}},
  shorttitle = {Value {{Kaleidoscope}}},
  author = {Sorensen, Taylor and Jiang, Liwei and Hwang, Jena and Levine, Sydney and Pyatkin, Valentina and West, Peter and Dziri, Nouha and Lu, Ximing and Rao, Kavel and Bhagavatula, Chandra and Sap, Maarten and Tasioulas, John and Choi, Yejin},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-01-20},
  abstract = {Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91\% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.},
  howpublished = {https://arxiv.org/abs/2309.00779v1},
  langid = {english},
  annotation = {\{"size": 10978405, "pages": 50, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/3IR6P545/Sorensen et al. - 2023 - Value Kaleidoscope Engaging AI with Pluralistic Human Values, Rights, and Duties.pdf}
}

@inproceedings{talat_2022,
  title = {On the {{Machine Learning}} of {{Ethical Judgments}} from {{Natural Language}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Talat, Zeerak and Blix, Hagen and Valvoda, Josef and Ganesh, Maya Indira and Cotterell, Ryan and Williams, Adina},
  year = {2022},
  month = jul,
  pages = {769--779},
  doi = {10.18653/v1/2022.naacl-main.56},
  urldate = {2024-01-20},
  abstract = {Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, Adina Williams. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.},
  langid = {american},
  annotation = {\{"size": 311566, "pages": 11, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/MJAH4FJ5/Talat et al. - 2022 - On the Machine Learning of Ethical Judgments from Natural Language.pdf}
}

@misc{yuan_2024,
  title = {Self-{{Rewarding Language Models}}},
  author = {Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-21},
  abstract = {We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.},
  howpublished = {https://arxiv.org/abs/2401.10020v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/P3ZBWZMP/Yuan et al. - 2024 - Self-Rewarding Language Models.pdf}
}

@misc{chen_2024,
  title = {Orion-{{14B}}: {{Open-source Multilingual Large Language Models}}},
  shorttitle = {Orion-{{14B}}},
  author = {Chen, Du and Huang, Yi and Li, Xiaopu and Li, Yongqiang and Liu, Yongqiang and Pan, Haihui and Xu, Leichao and Zhang, Dacheng and Zhang, Zhipeng and Han, Kun},
  year = {2024},
  month = jan,
  number = {arXiv:2401.12246},
  eprint = {2401.12246},
  publisher = {arXiv},
  urldate = {2024-01-24},
  abstract = {In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 411856, "pages": 19, "previous": "arXiv:2401.12246 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/QFKMKFMV/Chen et al. - 2024 - Orion-14B Open-source Multilingual Large Language Models.pdf}
}

@misc{wadhawan_2024,
  title = {{{ConTextual}}: {{Evaluating Context-Sensitive Text-Rich Visual Reasoning}} in {{Large Multimodal Models}}},
  shorttitle = {{{ConTextual}}},
  author = {Wadhawan, Rohan and Bansal, Hritik and Chang, Kai-Wei and Peng, Nanyun},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8\% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In addition to human evaluations, we also employed automatic evaluation metrics using GPT-4, uncovering similar trends in performance disparities. We also perform a fine-grained evaluation across diverse visual contexts and provide qualitative analysis which provides a robust framework for future advancements in the LMM design. https://con-textual.github.io/},
  howpublished = {https://arxiv.org/abs/2401.13311v1},
  langid = {english},
  annotation = {\{"size": 32432151, "pages": 54, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/IA49T8GT/Wadhawan et al. - 2024 - ConTextual Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models.pdf}
}

@misc{wang_2024,
  title = {{{MambaByte}}: {{Token-free Selective State Space Model}}},
  shorttitle = {{{MambaByte}}},
  author = {Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M.},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.},
  howpublished = {https://arxiv.org/abs/2401.13660v1},
  langid = {english},
  annotation = {\{"size": 1535778, "pages": 22, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/8V4NAQSA/Wang et al. - 2024 - MambaByte Token-free Selective State Space Model.pdf}
}

@misc{ye_2024,
  title = {{{SpacTor-T5}}: {{Pre-training T5 Models}} with {{Span Corruption}} and {{Replaced Token Detection}}},
  shorttitle = {{{SpacTor-T5}}},
  author = {Ye, Ke and Jiang, Heinrich and Rostamizadeh, Afshin and Chakrabarti, Ayan and DeSalvo, Giulia and Kagy, Jean-Fran{\c c}ois and Karydas, Lazaros and Citovsky, Gui and Kumar, Sanjiv},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Pre-training large language models is known to be extremely resource intensive and often times inefficient, under-utilizing the information encapsulated in the training text sequences. In this paper, we present SpacTor, a new training procedure consisting of (1) a hybrid objective combining span corruption (SC) and token replacement detection (RTD), and (2) a two-stage curriculum that optimizes the hybrid objective over the initial \${\textbackslash}tau\$ iterations, then transitions to standard SC loss. We show empirically that the effectiveness of the hybrid objective is tied to the two-stage pre-training schedule, and provide extensive analysis on why this is the case. In our experiments with encoder-decoder architectures (T5) on a variety of NLP tasks, SpacTor-T5 yields the same downstream performance as standard SC pre-training, while enabling a 50\% reduction in pre-training iterations and 40\% reduction in total FLOPs. Alternatively, given the same amount of computing budget, we find that SpacTor results in significantly improved downstream benchmark performance.},
  howpublished = {https://arxiv.org/abs/2401.13160v1},
  langid = {english},
  annotation = {\{"size": 698028, "pages": 22, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/B3N6CHD8/Ye et al. - 2024 - SpacTor-T5 Pre-training T5 Models with Span Corruption and Replaced Token Detection.pdf}
}

@misc{lin_2024,
  title = {{{MaLA-500}}: {{Massive Language Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{MaLA-500}}},
  author = {Lin, Peiqin and Ji, Shaoxiong and Tiedemann, J{\"o}rg and Martins, Andr{\'e} F. T. and Sch{\"u}tze, Hinrich},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Large language models have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves state-of-the-art in-context learning results. We release MaLA-500 at https://huggingface.co/MaLA-LM},
  howpublished = {https://arxiv.org/abs/2401.13303v1},
  langid = {english},
  annotation = {\{"size": 412975, "pages": 14, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/8GRFS38Q/Lin et al. - 2024 - MaLA-500 Massive Language Adaptation of Large Language Models.pdf}
}

@misc{zhang_2024a,
  title = {{{MM-LLMs}}: {{Recent Advances}} in {{MultiModal Large Language Models}}},
  shorttitle = {{{MM-LLMs}}},
  author = {Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of \$26\$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.},
  howpublished = {https://arxiv.org/abs/2401.13601v1},
  langid = {english},
  annotation = {\{"size": 1973115, "pages": 22, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/X24UMU36/Zhang et al. - 2024 - MM-LLMs Recent Advances in MultiModal Large Language Models.pdf}
}

@article{kusupati_2022,
  title = {Matryoshka {{Representation Learning}}},
  author = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and {Howard-Snyder}, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {30233--30249},
  urldate = {2024-01-27},
  langid = {english},
  annotation = {\{"size": 6759513, "pages": 17, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/66D5DK63/Kusupati_et_al_(2022)_Matryoshka_Representation_Learning.pdf}
}

@misc{birhane_2024,
  title = {{{AI}} Auditing: {{The Broken Bus}} on the {{Road}} to {{AI Accountability}}},
  shorttitle = {{{AI}} Auditing},
  author = {Birhane, Abeba and Steed, Ryan and Ojewale, Victor and Vecchione, Briana and Raji, Inioluwa Deborah},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-31},
  abstract = {One of the most concrete measures to take towards meaningful AI accountability is to consequentially assess and report the systems' performance and impact. However, the practical nature of the "AI audit" ecosystem is muddled and imprecise, making it difficult to work through various concepts and map out the stakeholders involved in the practice. First, we taxonomize current AI audit practices as completed by regulators, law firms, civil society, journalism, academia, consulting agencies. Next, we assess the impact of audits done by stakeholders within each domain. We find that only a subset of AI audit studies translate to desired accountability outcomes. We thus assess and isolate practices necessary for effective AI audit results, articulating the observed connections between AI audit design, methodology and institutional context on its effectiveness as a meaningful mechanism for accountability.},
  howpublished = {https://arxiv.org/abs/2401.14462v1},
  langid = {english},
  annotation = {\{"size": 562029, "pages": 33, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/DFFGJHRG/Birhane_et_al_(2024)_AI_auditing.pdf}
}

@misc{gao_2024,
  title = {Efficient {{Tool Use}} with {{Chain-of-Abstraction Reasoning}}},
  author = {Gao, Silin and {Dwivedi-Yu}, Jane and Yu, Ping and Tan, Xiaoqing Ellen and Pasunuru, Ramakanth and Golovneva, Olga and Sinha, Koustuv and Celikyilmaz, Asli and Bosselut, Antoine and Wang, Tianlu},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-02-01},
  abstract = {To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average {\textasciitilde}6\% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average {\textasciitilde}1.4x faster than baseline tool-augmented LLMs.},
  howpublished = {https://arxiv.org/abs/2401.17464v1},
  langid = {english},
  annotation = {\{"size": 937097, "pages": 17, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/5PHDIAN6/Gao_et_al_(2024)_Efficient_Tool_Use_with_Chain-of-Abstraction_Reasoning.pdf}
}

@misc{groeneveld_2024,
  title = {{{OLMo}}: {{Accelerating}} the {{Science}} of {{Language Models}}},
  shorttitle = {{{OLMo}}},
  author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-02},
  abstract = {Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.},
  howpublished = {https://arxiv.org/abs/2402.00838v1},
  langid = {english},
  annotation = {\{"size": 518849, "pages": 21, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/4AEWMPLR/Groeneveld et al. - 2024 - OLMo Accelerating the Science of Language Models.pdf}
}

@misc{soldaini_2024,
  title = {Dolma: An {{Open Corpus}} of {{Three Trillion Tokens}} for {{Language Model Pretraining Research}}},
  shorttitle = {Dolma},
  author = {Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and Hofmann, Valentin and Jha, Ananya Harsh and Kumar, Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan and Magnusson, Ian and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Ravichander, Abhilasha and Richardson, Kyle and Shen, Zejiang and Strubell, Emma and Subramani, Nishant and Tafjord, Oyvind and Walsh, Pete and Zettlemoyer, Luke and Smith, Noah A. and Hajishirzi, Hannaneh and Beltagy, Iz and Groeneveld, Dirk and Dodge, Jesse and Lo, Kyle},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-02-02},
  abstract = {Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.},
  howpublished = {https://arxiv.org/abs/2402.00159v1},
  langid = {english},
  annotation = {\{"size": 10087885, "pages": 83, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/ZA54BQ79/Soldaini et al. - 2024 - Dolma an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.pdf}
}

@misc{bsharat_2023,
  title = {Principled {{Instructions Are All You Need}} for {{Questioning LLaMA-1}}/2, {{GPT-3}}.5/4},
  author = {Bsharat, Sondos Mahmoud and Myrzakhan, Aidar and Shen, Zhiqiang},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-02-05},
  abstract = {This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.},
  howpublished = {https://arxiv.org/abs/2312.16171v2},
  langid = {english},
  annotation = {\{"size": 1577522, "pages": 26, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/XSNKWPLE/Bsharat et al. - 2023 - Principled Instructions Are All You Need for Questioning LLaMA-12, GPT-3.54.pdf}
}

@misc{nussbaum_2024,
  title = {Nomic {{Embed}}: {{Training}} a {{Reproducible Long Context Text Embedder}}},
  shorttitle = {Nomic {{Embed}}},
  author = {Nussbaum, Zach and Morris, John X. and Duderstadt, Brandon and Mulyar, Andriy},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-05},
  abstract = {This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors},
  howpublished = {https://arxiv.org/abs/2402.01613v1},
  langid = {english},
  annotation = {\{"size": 356769, "pages": 12, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/7PE46NUF/Nussbaum et al. - 2024 - Nomic Embed Training a Reproducible Long Context Text Embedder.pdf}
}

@misc{tito_2022,
  title = {Hierarchical Multimodal Transformers for {{Multi-Page DocVQA}}},
  author = {Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  year = {2022},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-02-08},
  abstract = {Document Visual Question Answering (DocVQA) refers to the task of answering questions from document images. Existing work on DocVQA only considers single-page documents. However, in real scenarios documents are mostly composed of multiple pages that should be processed altogether. In this work we extend DocVQA to the multi-page scenario. For that, we first create a new dataset, MP-DocVQA, where questions are posed over multi-page documents instead of single pages. Second, we propose a new hierarchical method, Hi-VT5, based on the T5 architecture, that overcomes the limitations of current methods to process long multi-page documents. The proposed method is based on a hierarchical transformer architecture where the encoder summarizes the most relevant information of every page and then, the decoder takes this summarized information to generate the final answer. Through extensive experimentation, we demonstrate that our method is able, in a single stage, to answer the questions and provide the page that contains the relevant information to find the answer, which can be used as a kind of explainability measure.},
  howpublished = {https://arxiv.org/abs/2212.05935v2},
  langid = {english},
  annotation = {\{"size": 7354424, "pages": 14, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/JQ7DCCXM/Tito et al. - 2022 - Hierarchical multimodal transformers for Multi-Page DocVQA.pdf}
}

@misc{guo_2024,
  title = {Direct {{Language Model Alignment}} from {{Online AI Feedback}}},
  author = {Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and Ferret, Johan and Blondel, Mathieu},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-08},
  abstract = {Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.},
  howpublished = {https://arxiv.org/abs/2402.04792v1},
  langid = {english},
  annotation = {\{"size": 987022, "pages": 18, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/PHHW447R/Guo et al. - 2024 - Direct Language Model Alignment from Online AI Feedback.pdf}
}

@misc{shenoy_2024,
  title = {Lumos : {{Empowering Multimodal LLMs}} with {{Scene Text Recognition}}},
  shorttitle = {Lumos},
  author = {Shenoy, Ashish and Lu, Yichao and Jayakumar, Srihari and Chatterjee, Debojeet and Moslehpour, Mohsen and Chuang, Pierce and Harpale, Abhay and Bhardwaj, Vikas and Xu, Di and Zhao, Shicong and Zhao, Longfang and Ramchandani, Ankit and Dong, Xin Luna and Kumar, Anuj},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-14},
  abstract = {We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.},
  howpublished = {https://arxiv.org/abs/2402.08017v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/5WYTF3DY/Shenoy_et_al_(2024)_Lumos.pdf}
}

@misc{wang_2024a,
  title = {Chain-of-{{Thought Reasoning Without Prompting}}},
  author = {Wang, Xuezhi and Zhou, Denny},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-18},
  abstract = {In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the {\textbackslash}textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' {\textbackslash}textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.},
  howpublished = {https://arxiv.org/abs/2402.10200v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/WH2HHTMJ/Wang and Zhou - 2024 - Chain-of-Thought Reasoning Without Prompting.pdf}
}

@misc{nunes_2023,
  title = {{{DotHash}}: {{Estimating Set Similarity Metrics}} for {{Link Prediction}} and {{Document Deduplication}}},
  shorttitle = {{{DotHash}}},
  author = {Nunes, Igor and Heddes, Mike and Verg{\'e}s, Pere and Abraham, Danny and Veidenbaum, Alexander and Nicolau, Alexandru and Givargis, Tony},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  doi = {10.1145/3580305.3599314},
  urldate = {2024-02-24},
  abstract = {Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.},
  howpublished = {https://arxiv.org/abs/2305.17310v1},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/arXiv) [2024-02-24]},
  file = {/Users/chenghao/Zotero/storage/EY4HT2VW/Nunes et al. - 2023 - DotHash Estimating Set Similarity Metrics for Link Prediction and Document Deduplication.pdf}
}

@misc{ma_2024,
  title = {The {{Era}} of 1-Bit {{LLMs}}: {{All Large Language Models}} Are in 1.58 {{Bits}}},
  shorttitle = {The {{Era}} of 1-Bit {{LLMs}}},
  author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-28},
  abstract = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \{-1, 0, 1\}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  howpublished = {https://arxiv.org/abs/2402.17764v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/8S6LDR9V/Ma et al. - 2024 - The Era of 1-bit LLMs All Large Language Models are in 1.58 Bits.pdf}
}

@article{shah_2024,
  title = {Envisioning {{Information Access Systems}}: {{What Makes}} for {{Good Tools}} and a {{Healthy Web}}?},
  shorttitle = {Envisioning {{Information Access Systems}}},
  author = {Shah, Chirag and Bender, Emily M.},
  year = {2024},
  month = feb,
  journal = {ACM Transactions on the Web},
  pages = {3649468},
  issn = {1559-1131, 1559-114X},
  doi = {10.1145/3649468},
  urldate = {2024-02-28},
  abstract = {We observe a recent trend towards applying large language models (LLMs) in search and positioning them as efective information access systems. While the interfaces may look appealing and the apparent breadth of applicability is exciting, we are concerned that the ield is rushing ahead with a technology without suicient study of the uses it is meant to serve, how it would be used, and what its use would mean. We argue that it is important to reassert the central research focus of the ield of information retrieval, because information access is not merely an application to be solved by the so-called `AI' techniques du jour. Rather, it is a key human activity, with impacts on both individuals and society. As information scientists, we should be asking what do people and society want and need from information access systems and how do we design and build systems to meet those needs? With that goal, in this conceptual paper we investigate fundamental questions concerning information access from user and societal viewpoints. We revisit foundational work related to information behavior, information seeking, information retrieval, information iltering, and information access to resurface what we know about these fundamental questions and what may be missing. We then provide our conceptual framing about how we could ill this gap, focusing on methods as well as experimental and evaluation frameworks. We consider the Web as an information ecosystem and explore the ways in which synthetic media, produced by LLMs and otherwise, endangers that ecosystem. The primary goal of this conceptual paper is to shed light on what we still do not know about the potential impacts of LLM-based information access systems, how to advance our understanding of user behaviors, and where the next generations of students, scholars, and developers could fruitfully invest their energies. CCS Concepts: {$\cdot$} Information systems {$\rightarrow$} Web searching and information discovery; Users and interactive retrieval; {$\cdot$} Computing methodologies {$\rightarrow$} Natural language processing.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/LSUEXE2Z/Shah and Bender - 2024 - Envisioning Information Access Systems What Makes for Good Tools and a Healthy Web.pdf}
}

@article{desai_2024,
  title = {Between {{Copyright}} and {{Computer Science}}: {{The Law}} and {{Ethics}} of {{Generative AI}}},
  shorttitle = {Between {{Copyright}} and {{Computer Science}}},
  author = {Desai, Deven R. and Riedl, Mark},
  year = {2024},
  month = feb,
  urldate = {2024-02-28},
  abstract = {Copyright and computer science continue to intersect and clash, but they can coexist. The advent of new technologies such as digitization of visual and aural cr},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/VZ8UYSSE/Desai and Riedl - 2024 - Between Copyright and Computer Science The Law and Ethics of Generative AI.pdf}
}

@misc{shumailov_2023,
  title = {The {{Curse}} of {{Recursion}}: {{Training}} on {{Generated Data Makes Models Forget}}},
  shorttitle = {The {{Curse}} of {{Recursion}}},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-02-28},
  abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
  howpublished = {https://arxiv.org/abs/2305.17493v2},
  langid = {english},
  annotation = {83 citations (Semantic Scholar/arXiv) [2024-02-28]},
  file = {/Users/chenghao/Zotero/storage/3XWRCU8R/Shumailov et al. - 2023 - The Curse of Recursion Training on Generated Data Makes Models Forget.pdf}
}

@misc{de_2024,
  title = {Griffin: {{Mixing Gated Linear Recurrences}} with {{Local Attention}} for {{Efficient Language Models}}},
  shorttitle = {Griffin},
  author = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and {Cristian-Muraru}, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and De Freitas, Nando and Gulcehre, Caglar},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-03-04},
  abstract = {Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.},
  howpublished = {https://arxiv.org/abs/2402.19427v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/VKULWXTF/De_et_al_(2024)_Griffin.pdf}
}

@misc{arora_2024,
  title = {Simple Linear Attention Language Models Balance the Recall-Throughput Tradeoff},
  author = {Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-03-05},
  abstract = {Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.},
  howpublished = {https://arxiv.org/abs/2402.18668v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-03-05]},
  file = {/Users/chenghao/Zotero/storage/KFBTS9SX/Arora_et_al_(2024)_Simple_linear_attention_language_models_balance_the_recall-throughput_tradeoff.pdf}
}

@misc{_h,
  title = {{{SaulLM-7B}}: {{A}} Pioneering {{Large Language Model}} for {{Law}}},
  urldate = {2024-03-07},
  howpublished = {https://arxiv.org/html/2403.03883v1},
  file = {/Users/chenghao/Zotero/storage/Y5VF3F3L/2403.html}
}

@misc{zhao_2024,
  title = {{{GaLore}}: {{Memory-Efficient LLM Training}} by {{Gradient Low-Rank Projection}}},
  shorttitle = {{{GaLore}}},
  author = {Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03507},
  eprint = {2403.03507},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-09},
  abstract = {Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memoryefficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5\% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5\% and total training memory by 63.3\%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/LHL74CNT/Zhao et al. - 2024 - GaLore Memory-Efficient LLM Training by Gradient Low-Rank Projection.pdf}
}

@misc{reid_2024,
  title = {Gemini 1.5: {{Unlocking}} Multimodal Understanding across Millions of Tokens of Context},
  shorttitle = {Gemini 1.5},
  author = {Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and Antonoglou, Ioannis and Anil, Rohan and Borgeaud, Sebastian and Dai, Andrew and Millican, Katie and Dyer, Ethan and Glaese, Mia and Sottiaux, Thibault and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Molloy, James and Chen, Jilin and Isard, Michael and Barham, Paul and Hennigan, Tom and McIlroy, Ross and Johnson, Melvin and Schalkwyk, Johan and Collins, Eli and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Meyer, Clemens and Thornton, Gregory and Yang, Zhen and Michalewski, Henryk and Abbas, Zaheer and Schucher, Nathan and Anand, Ankesh and Ives, Richard and Keeling, James and Lenc, Karel and Haykal, Salem and Shakeri, Siamak and Shyam, Pranav and Chowdhery, Aakanksha and Ring, Roman and Spencer, Stephen and Sezener, Eren and Vilnis, Luke and Chang, Oscar and Morioka, Nobuyuki and Tucker, George and Zheng, Ce and Woodman, Oliver and Attaluri, Nithya and Kocisky, Tomas and Eltyshev, Evgenii and Chen, Xi and Chung, Timothy and Selo, Vittorio and Brahma, Siddhartha and Georgiev, Petko and Slone, Ambrose and Zhu, Zhenkai and Lottes, James and Qiao, Siyuan and Caine, Ben and Riedel, Sebastian and Tomala, Alex and Chadwick, Martin and Love, Juliette and Choy, Peter and Mittal, Sid and Houlsby, Neil and Tang, Yunhao and Lamm, Matthew and Bai, Libin and Zhang, Qiao and He, Luheng and Cheng, Yong and Humphreys, Peter and Li, Yujia and Brin, Sergey and Cassirer, Albin and Miao, Yingjie and Zilka, Lukas and Tobin, Taylor and Xu, Kelvin and Proleev, Lev and Sohn, Daniel and Magni, Alberto and Hendricks, Lisa Anne and Gao, Isabel and Onta{\~n}{\'o}n, Santiago and Bunyan, Oskar and Byrd, Nathan and Sharma, Abhanshu and Zhang, Biao and Pinto, Mario and Sinha, Rishika and Mehta, Harsh and Jia, Dawei and Caelles, Sergi and Webson, Albert and Morris, Alex and Roelofs, Becca and Ding, Yifan and Strudel, Robin and Xiong, Xuehan and Ritter, Marvin and Dehghani, Mostafa and Chaabouni, Rahma and Karmarkar, Abhijit and Lai, Guangda and Mentzer, Fabian and Xu, Bibo and Li, YaGuang and Zhang, Yujing and Paine, Tom Le and Goldin, Alex and Neyshabur, Behnam and Baumli, Kate and Levskaya, Anselm and Laskin, Michael and Jia, Wenhao and Rae, Jack W. and Xiao, Kefan and He, Antoine and Giordano, Skye and Yagati, Lakshman and Lespiau, Jean-Baptiste and Natsev, Paul and Ganapathy, Sanjay and Liu, Fangyu and Martins, Danilo and Chen, Nanxin and Xu, Yunhan and Barnes, Megan and May, Rhys and Vezer, Arpi and Oh, Junhyuk and Franko, Ken and Bridgers, Sophie and Zhao, Ruizhe and Wu, Boxi and Mustafa, Basil and Sechrist, Sean and Parisotto, Emilio and Pillai, Thanumalayan Sankaranarayana and Larkin, Chris and Gu, Chenjie and Sorokin, Christina and Krikun, Maxim and Guseynov, Alexey and Landon, Jessica and Datta, Romina and Pritzel, Alexander and Thacker, Phoebe and Yang, Fan and Hui, Kevin and Hauth, Anja and Yeh, Chih-Kuan and Barker, David and {Mao-Jones}, Justin and Austin, Sophia and Sheahan, Hannah and Schuh, Parker and Svensson, James and Jain, Rohan and Ramasesh, Vinay and Briukhov, Anton and Chung, Da-Woon and {von Glehn}, Tamara and Butterfield, Christina and Jhakra, Priya and Wiethoff, Matthew and Frye, Justin and Grimstad, Jordan and Changpinyo, Beer and Lan, Charline Le and Bortsova, Anna and Wu, Yonghui and Voigtlaender, Paul and Sainath, Tara and Smith, Charlotte and Hawkins, Will and Cao, Kris and Besley, James and Srinivasan, Srivatsan and Omernick, Mark and Gaffney, Colin and Surita, Gabriela and Burnell, Ryan and Damoc, Bogdan and Ahn, Junwhan and Brock, Andrew and Pajarskas, Mantas and Petrushkina, Anastasia and Noury, Seb and Blanco, Lorenzo and Swersky, Kevin and Ahuja, Arun and Avrahami, Thi and Misra, Vedant and {de Liedekerke}, Raoul and Iinuma, Mariko and Polozov, Alex and York, Sarah and van den Driessche, George and Michel, Paul and Chiu, Justin and Blevins, Rory and Gleicher, Zach and Recasens, Adri{\`a} and Rrustemi, Alban and Gribovskaya, Elena and Roy, Aurko and Gworek, Wiktor and Arnold, S{\'e}b and Lee, Lisa and {Lee-Thorp}, James and Maggioni, Marcello and Piqueras, Enrique and Badola, Kartikeya and Vikram, Sharad and Gonzalez, Lucas and Baddepudi, Anirudh and Senter, Evan and Devlin, Jacob and Qin, James and Azzam, Michael and Trebacz, Maja and Polacek, Martin and Krishnakumar, Kashyap and Chang, Shuo-yiin and Tung, Matthew and Penchev, Ivo and Joshi, Rishabh and Olszewska, Kate and Muir, Carrie and Wirth, Mateo and Hartman, Ale Jakse and Newlan, Josh and Kashem, Sheleem and Bolina, Vijay and Dabir, Elahe and {van Amersfoort}, Joost and Ahmed, Zafarali and {Cobon-Kerr}, James and Kamath, Aishwarya and Hrafnkelsson, Arnar Mar and Hou, Le and Mackinnon, Ian and Frechette, Alexandre and Noland, Eric and Si, Xiance and Taropa, Emanuel and Li, Dong and Crone, Phil and Gulati, Anmol and Cevey, S{\'e}bastien and Adler, Jonas and Ma, Ada and Silver, David and Tokumine, Simon and Powell, Richard and Lee, Stephan and Chang, Michael and Hassan, Samer and Mincu, Diana and Yang, Antoine and Levine, Nir and Brennan, Jenny and Wang, Mingqiu and Hodkinson, Sarah and Zhao, Jeffrey and Lipschultz, Josh and Pope, Aedan and Chang, Michael B. and Li, Cheng and Shafey, Laurent El and Paganini, Michela and Douglas, Sholto and Bohnet, Bernd and Pardo, Fabio and Odoom, Seth and Rosca, Mihaela and dos Santos, Cicero Nogueira and Soparkar, Kedar and Guez, Arthur and Hudson, Tom and Hansen, Steven and Asawaroengchai, Chulayuth and Addanki, Ravi and Yu, Tianhe and Stokowiec, Wojciech and Khan, Mina and Gilmer, Justin and Lee, Jaehoon and Bostock, Carrie Grimes and Rong, Keran and Caton, Jonathan and Pejman, Pedram and Pavetic, Filip and Brown, Geoff and Sharma, Vivek and Lu{\v c}i{\'c}, Mario and Samuel, Rajkumar and Djolonga, Josip and Mandhane, Amol and Sj{\"o}sund, Lars Lowe and Buchatskaya, Elena and White, Elspeth and Clay, Natalie and Jiang, Jiepu and Lim, Hyeontaek and Hemsley, Ross and Labanowski, Jane and De Cao, Nicola and Steiner, David and Hashemi, Sayed Hadi and Austin, Jacob and Gergely, Anita and Blyth, Tim and Stanton, Joe and Shivakumar, Kaushik and Siddhant, Aditya and Andreassen, Anders and Araya, Carlos and Sethi, Nikhil and Shivanna, Rakesh and Hand, Steven and Bapna, Ankur and Khodaei, Ali and Miech, Antoine and Tanzer, Garrett and Swing, Andy and Thakoor, Shantanu and Pan, Zhufeng and Nado, Zachary and Winkler, Stephanie and Yu, Dian and Saleh, Mohammad and Maggiore, Loren and Barr, Iain and Giang, Minh and Kagohara, Thais and Danihelka, Ivo and Marathe, Amit and Feinberg, Vladimir and Elhawaty, Mohamed and Ghelani, Nimesh and Horgan, Dan and Miller, Helen and Walker, Lexi and Tanburn, Richard and Tariq, Mukarram and Shrivastava, Disha and Xia, Fei and Chiu, Chung-Cheng and Ashwood, Zoe and Baatarsukh, Khuslen and Samangooei, Sina and Alcober, Fred and Stjerngren, Axel and Komarek, Paul and Tsihlas, Katerina and Boral, Anudhyan and Comanescu, Ramona and Chen, Jeremy and Liu, Ruibo and Bloxwich, Dawn and Chen, Charlie and Sun, Yanhua and Feng, Fangxiaoyu and Mauger, Matthew and Dotiwalla, Xerxes and Hellendoorn, Vincent and Sharman, Michael and Zheng, Ivy and Haridasan, Krishna and {Barth-Maron}, Gabe and Swanson, Craig and Rogozi{\'n}ska, Dominika and Andreev, Alek and Rubenstein, Paul Kishan and Sang, Ruoxin and Hurt, Dan and Elsayed, Gamaleldin and Wang, Renshen and Lacey, Dave and Ili{\'c}, Anastasija and Zhao, Yao and Aroyo, Lora and Iwuanyanwu, Chimezie and Nikolaev, Vitaly and Lakshminarayanan, Balaji and Jazayeri, Sadegh and Kaufman, Rapha{\"e}l Lopez and Varadarajan, Mani and Tekur, Chetan and Fritz, Doug and Khalman, Misha and Reitter, David and Dasgupta, Kingshuk and Sarcar, Shourya and Ornduff, Tina and Snaider, Javier and Huot, Fantine and Jia, Johnson and Kemp, Rupert and Trdin, Nejc and Vijayakumar, Anitha and Kim, Lucy and Angermueller, Christof and Lao, Li and Liu, Tianqi and Zhang, Haibin and Engel, David and Greene, Somer and White, Ana{\"i}s and Austin, Jessica and Taylor, Lilly and Ashraf, Shereen and Liu, Dangyi and Georgaki, Maria and Cai, Irene and Kulizhskaya, Yana and Goenka, Sonam and Saeta, Brennan and Vodrahalli, Kiran and Frank, Christian and {de Cesare}, Dario and Robenek, Brona and Richardson, Harry and Alnahlawi, Mahmoud and Yew, Christopher and Ponnapalli, Priya and Tagliasacchi, Marco and Korchemniy, Alex and Kim, Yelin and Li, Dinghua and Rosgen, Bill and Ashwood, Zoe and Levin, Kyle and Wiesner, Jeremy and Banzal, Praseem and Srinivasan, Praveen and Yu, Hongkun and {\"U}nl{\"u}, {\c C}a{\u g}lar and Reid, David and Tung, Zora and Finchelstein, Daniel and Kumar, Ravin and Elisseeff, Andre and Huang, Jin and Zhang, Ming and Zhu, Rui and Aguilar, Ricardo and Gim{\'e}nez, Mai and Xia, Jiawei and Dousse, Olivier and Gierke, Willi and Yeganeh, Soheil Hassas and Yates, Damion and Jalan, Komal and Li, Lu and {Latorre-Chimoto}, Eri and Nguyen, Duc Dung and Durden, Ken and Kallakuri, Praveen and Liu, Yaxin and Johnson, Matthew and Tsai, Tomy and Talbert, Alice and Liu, Jasmine and Neitz, Alexander and Elkind, Chen and Selvi, Marco and Jasarevic, Mimi and Soares, Livio Baldini and Cui, Albert and Wang, Pidong and Wang, Alek Wenjiao and Ye, Xinyu and Kallarackal, Krystal and Loher, Lucia and Lam, Hoi and Broder, Josef and {Holtmann-Rice}, Dan and Martin, Nina and Ramadhana, Bramandia and Toyama, Daniel and Shukla, Mrinal and Basu, Sujoy and Mohan, Abhi and Fernando, Nick and Fiedel, Noah and Paterson, Kim and Li, Hui and Garg, Ankush and Park, Jane and Choi, DongHyun and Wu, Diane and Singh, Sankalp and Zhang, Zhishuai and Globerson, Amir and Yu, Lily and Carpenter, John and Quitry, F{\'e}lix de Chaumont and Radebaugh, Carey and Lin, Chu-Cheng and Tudor, Alex and Shroff, Prakash and Garmon, Drew and Du, Dayou and Vats, Neera and Lu, Han and Iqbal, Shariq and Yakubovich, Alex and Tripuraneni, Nilesh and Manyika, James and Qureshi, Haroon and Hua, Nan and Ngani, Christel and Raad, Maria Abi and Forbes, Hannah and Bulanova, Anna and Stanway, Jeff and Sundararajan, Mukund and Ungureanu, Victor and Bishop, Colton and Li, Yunjie and Venkatraman, Balaji and Li, Bo and Thornton, Chloe and Scellato, Salvatore and Gupta, Nishesh and Wang, Yicheng and Tenney, Ian and Wu, Xihui and Shenoy, Ashish and Carvajal, Gabriel and Wright, Diana Gage and Bariach, Ben and Xiao, Zhuyun and Hawkins, Peter and Dalmia, Sid and Farabet, Clement and Valenzuela, Pedro and Yuan, Quan and Welty, Chris and Agarwal, Ananth and Chen, Mia and Kim, Wooyeol and Hulse, Brice and Dukkipati, Nandita and Paszke, Adam and Bolt, Andrew and Davoodi, Elnaz and Choo, Kiam and Beattie, Jennifer and Prendki, Jennifer and Vashisht, Harsha and {Santamaria-Fernandez}, Rebeca and Cobo, Luis C. and Wilkiewicz, Jarek and Madras, David and Elqursh, Ali and Uy, Grant and Ramirez, Kevin and Harvey, Matt and Liechty, Tyler and Zen, Heiga and Seibert, Jeff and Hu, Clara Huiyi and Elhawaty, Mohamed and Khorlin, Andrey and Le, Maigo and Aharoni, Asaf and Li, Megan and Wang, Lily and Kumar, Sandeep and Lince, Alejandro and Casagrande, Norman and Hoover, Jay and Badawy, Dalia El and Soergel, David and Vnukov, Denis and Miecnikowski, Matt and Simsa, Jiri and Koop, Anna and Kumar, Praveen and Sellam, Thibault and Vlasic, Daniel and Daruki, Samira and Shabat, Nir and Zhang, John and Su, Guolong and Zhang, Jiageng and Liu, Jeremiah and Sun, Yi and Palmer, Evan and Ghaffarkhah, Alireza and Xiong, Xi and Cotruta, Victor and Fink, Michael and Dixon, Lucas and Sreevatsa, Ashwin and Goedeckemeyer, Adrian and Dimitriev, Alek and Jafari, Mohsen and Crocker, Remi and FitzGerald, Nicholas and Kumar, Aviral and Ghemawat, Sanjay and Philips, Ivan and Liu, Frederick and Liang, Yannie and Sterneck, Rachel and Repina, Alena and Wu, Marcus and Knight, Laura and Georgiev, Marin and Lee, Hyo and Askham, Harry and Chakladar, Abhishek and Louis, Annie and Crous, Carl and Cate, Hardie and Petrova, Dessie and Quinn, Michael and {Owusu-Afriyie}, Denese and Singhal, Achintya and Wei, Nan and Kim, Solomon and Vincent, Damien and Nasr, Milad and {Choquette-Choo}, Christopher A. and Tojo, Reiko and Lu, Shawn and Casas, Diego de Las and Cheng, Yuchung and Bolukbasi, Tolga and Lee, Katherine and Fatehi, Saaber and Ananthanarayanan, Rajagopal and Patel, Miteyan and Kaed, Charbel and Li, Jing and Sygnowski, Jakub and Belle, Shreyas Rammohan and Chen, Zhe and Konzelmann, Jaclyn and P{\~o}der, Siim and Garg, Roopal and Koverkathu, Vinod and Brown, Adam and Dyer, Chris and Liu, Rosanne and Nova, Azade and Xu, Jun and Petrov, Slav and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-11},
  abstract = {In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({$>$}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
  howpublished = {https://arxiv.org/abs/2403.05530v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/UK4FR4GA/Reid et al. - 2024 - Gemini 1.5 Unlocking multimodal understanding across millions of tokens of context.pdf}
}

@misc{liao_2024,
  title = {Adding {{NVMe SSDs}} to {{Enable}} and {{Accelerate 100B Model Fine-tuning}} on a {{Single GPU}}},
  author = {Liao, Changyue and Sun, Mo and Yang, Zihan and Chen, Kaiqi and Yuan, Binhang and Wu, Fei and Wang, Zeke},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-12},
  abstract = {Recent advances in large language models have brought immense value to the world, with their superior capabilities stemming from the massive number of parameters they utilize. However, even the GPUs with the highest memory capacities, currently peaking at 80GB, are far from sufficient to accommodate these vast parameters and their associated optimizer states when conducting stochastic gradient descent-based optimization. One approach to hosting such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most academic researchers, who always have a limited budget for many high-end GPU servers. In this paper, we focus on huge model fine-tuning on a single, even low-end, GPU in a commodity server, which is accessible to most AI researchers. In such a scenario, the state-of-the-art work ZeRO-Infinity suffers from two severe issues when running in a commodity server: 1) low GPU utilization due to inefficient swapping, and 2) limited trainable model size due to CPU memory capacity. The underlying reason is that ZeRO-Infinity is optimized for running on high-end GPU servers. To this end, we present Fuyou, a low-cost training framework that enables efficient 100B huge model fine-tuning on a low-end server with a low-end GPU and limited CPU memory capacity. The key idea is to add the SSD-CPU communication as an optimization dimension and thus carefully co-optimize computation and data swapping from a systematic approach to maximize GPU utilization. The experimental results show that 1) Fuyou is able to fine-tune 175B GPT-3 on a consumer GPU RTX 4090 with high GPU utilization, while ZeRO-Infinity fails to fine-tune; and 2) when training a small GPT-3 13B model, Fuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves 45 TFLOPS.},
  howpublished = {https://arxiv.org/abs/2403.06504v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/9BUYR8DF/Liao et al. - 2024 - Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU.pdf}
}

@misc{mckinzie_2024,
  title = {{{MM1}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Multimodal LLM Pre-training}}},
  shorttitle = {{{MM1}}},
  author = {McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and Belyi, Anton and Zhang, Haotian and Singh, Karanjeet and Kang, Doug and H{\`e}, Hongyu and Schwarzer, Max and Gunter, Tom and Kong, Xiang and Zhang, Aonan and Wang, Jianyu and Wang, Chong and Du, Nan and Lei, Tao and Wiseman, Sam and Lee, Mark and Wang, Zirui and Pang, Ruoming and Grasch, Peter and Toshev, Alexander and Yang, Yinfei},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-15},
  abstract = {In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.},
  howpublished = {https://arxiv.org/abs/2403.09611v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-03-15]},
  file = {/Users/chenghao/Zotero/storage/BGIMCSJI/McKinzie et al. - 2024 - MM1 Methods, Analysis & Insights from Multimodal LLM Pre-training.pdf;/Users/chenghao/Zotero/storage/E5AZHVLU/McKinzie_et_al_(2024)_MM1.pdf}
}

@misc{liang_2024,
  title = {Monitoring {{AI-Modified Content}} at {{Scale}}: {{A Case Study}} on the {{Impact}} of {{ChatGPT}} on {{AI Conference Peer Reviews}}},
  shorttitle = {Monitoring {{AI-Modified Content}} at {{Scale}}},
  author = {Liang, Weixin and Izzo, Zachary and Zhang, Yaohui and Lepp, Haley and Cao, Hancheng and Zhao, Xuandong and Chen, Lingjiao and Ye, Haotian and Liu, Sheng and Huang, Zhi and McFarland, Daniel A. and Zou, James Y.},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-17},
  abstract = {We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5\% and 16.9\% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.},
  howpublished = {https://arxiv.org/abs/2403.07183v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/WQXFWXF2/Liang_et_al_(2024)_Monitoring_AI-Modified_Content_at_Scale.pdf}
}

@misc{hu_2024,
  title = {{{mPLUG-DocOwl}} 1.5: {{Unified Structure Learning}} for {{OCR-free Document Understanding}}},
  shorttitle = {{{mPLUG-DocOwl}} 1.5},
  author = {Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and Zhou, Jingren},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-20},
  abstract = {Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.},
  howpublished = {https://arxiv.org/abs/2403.12895v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-03-20]},
  file = {/Users/chenghao/Zotero/storage/893R83FC/Hu et al. - 2024 - mPLUG-DocOwl 1.5 Unified Structure Learning for OCR-free Document Understanding.pdf}
}

@article{zongjonathan_2024,
  title = {Data {{Refusal}} from {{Below}}: {{A Framework}} for {{Understanding}}, {{Evaluating}}, and {{Envisioning Refusal}} as {{Design}}},
  shorttitle = {Data {{Refusal}} from {{Below}}},
  author = {ZongJonathan and Nathan, MatiasJ},
  year = {2024},
  month = mar,
  journal = {ACM Journal on Responsible Computing},
  publisher = {ACMPUB27New York, NY},
  doi = {10.1145/3630107},
  urldate = {2024-03-27},
  abstract = {Amidst calls for public accountability over large data-driven systems, feminist and indigenous scholars have developed refusal as a practice that challenges the authority of data collectors. However, because data affect so many aspects of daily life, it ...},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2024-03-27]},
  file = {/Users/chenghao/Zotero/storage/3MH3U3N5/ZongJonathan and Nathan - 2024 - Data Refusal from Below A Framework for Understanding, Evaluating, and Envisioning Refusal as Desig.pdf}
}

@misc{wei_2024,
  title = {Long-Form Factuality in Large Language Models},
  author = {Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and Le, Quoc V.},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-29},
  abstract = {Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall). Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of {\textasciitilde}16k individual facts, SAFE agrees with crowdsourced human annotators 72\% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76\% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.},
  howpublished = {https://arxiv.org/abs/2403.18802v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/P4ABPD9T/Wei et al. - 2024 - Long-form factuality in large language models.pdf}
}

@misc{xu_2024,
  title = {Hallucination Is {{Inevitable}}: {{An Innate Limitation}} of {{Large Language Models}}},
  shorttitle = {Hallucination Is {{Inevitable}}},
  author = {Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-03-30},
  abstract = {Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.},
  howpublished = {https://arxiv.org/abs/2401.11817v1},
  langid = {english},
  annotation = {8 citations (Semantic Scholar/arXiv) [2024-03-30]},
  file = {/Users/chenghao/Zotero/storage/9QZNAFTK/Xu et al. - 2024 - Hallucination is Inevitable An Innate Limitation of Large Language Models.pdf}
}

@misc{lieber_2024,
  title = {Jamba: {{A Hybrid Transformer-Mamba Language Model}}},
  shorttitle = {Jamba},
  author = {Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and {Shalev-Shwartz}, Shai and Abend, Omri and Alon, Raz and Asida, Tomer and Bergman, Amir and Glozman, Roman and Gokhman, Michael and Manevich, Avashalom and Ratner, Nir and Rozen, Noam and Shwartz, Erez and Zusman, Mor and Shoham, Yoav},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-04-01},
  abstract = {We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.},
  howpublished = {https://arxiv.org/abs/2403.19887v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-01]},
  file = {/Users/chenghao/Zotero/storage/82BEJAZ7/Lieber et al. - 2024 - Jamba A Hybrid Transformer-Mamba Language Model.pdf}
}

@article{anil_,
  title = {Many-Shot {{Jailbreaking}}},
  author = {Anil, Cem and Durmus, Esin and Sharma, Mrinank and Benton, Joe and Kundu, Sandipan and Batson, Joshua and Rimsky, Nina and Tong, Meg and Mu, Jesse and Ford, Daniel and Mosconi, Francesco and Agrawal, Rajashree and Schaeffer, Rylan and Bashkansky, Naomi and Svenningsen, Samuel and Lambert, Mike and Radhakrishnan, Ansh and Denison, Carson and Hubinger, Evan J and Bai, Yuntao and Bricken, Trenton and Maxwell, Timothy and Schiefer, Nicholas and Sully, Jamie and Tamkin, Alex and Lanham, Tamera and Nguyen, Karina and Korbak, Tomasz and Kaplan, Jared and Ganguli, Deep and Bowman, Samuel R and Perez, Ethan and Grosse, Roger and Duvenaud, David},
  abstract = {We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This is newly feasible with the larger context windows recently deployed by Anthropic, OpenAI and Google DeepMind. We find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. We demonstrate the success of this attack on the most widely used state-of-the-art closedweight models, and across various tasks. Our results suggest very long contexts present a rich new attack surface for LLMs.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/45NW67Q6/Anil et al. - Many-shot Jailbreaking.pdf}
}

@misc{li_2024,
  title = {Long-Context {{LLMs Struggle}} with {{Long In-context Learning}}},
  author = {Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-03},
  abstract = {Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.},
  howpublished = {https://arxiv.org/abs/2404.02060v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/KT7B535H/Li et al. - 2024 - Long-context LLMs Struggle with Long In-context Learning.pdf}
}

@misc{raposo_2024,
  title = {Mixture-of-{{Depths}}: {{Dynamically}} Allocating Compute in Transformer-Based Language Models},
  shorttitle = {Mixture-of-{{Depths}}},
  author = {Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-06},
  abstract = {Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (\$k\$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-\$k\$ routing mechanism. Since \$k\$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the \$k\$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50{\textbackslash}\% faster to step during post-training sampling.},
  howpublished = {https://arxiv.org/abs/2404.02258v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/UGQLSE5R/Raposo et al. - 2024 - Mixture-of-Depths Dynamically allocating compute in transformer-based language models.pdf}
}

@misc{udandarao_2024,
  title = {No "{{Zero-Shot}}" {{Without Exponential Data}}: {{Pretraining Concept Frequency Determines Multimodal Model Performance}}},
  shorttitle = {No "{{Zero-Shot}}" {{Without Exponential Data}}},
  author = {Udandarao, Vishaal and Prabhu, Ameya and Ghosh, Adhiraj and Sharma, Yash and Torr, Philip H. S. and Bibi, Adel and Albanie, Samuel and Bethge, Matthias},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-08},
  abstract = {Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found.},
  howpublished = {https://arxiv.org/abs/2404.04125v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-08]},
  file = {/Users/chenghao/Zotero/storage/UH3BS6SI/Udandarao et al. - 2024 - No Zero-Shot Without Exponential Data Pretraining Concept Frequency Determines Multimodal Model P.pdf}
}

@misc{hsieh_2024,
  title = {{{RULER}}: {{What}}'s the {{Real Context Size}} of {{Your Long-Context Language Models}}?},
  shorttitle = {{{RULER}}},
  author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Ginsburg, Boris},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-11},
  abstract = {The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the "needle") from long distractor texts (the "haystack"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate ten long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only four models (GPT-4, Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.},
  howpublished = {https://arxiv.org/abs/2404.06654v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/7267MVT7/Hsieh et al. - 2024 - RULER What's the Real Context Size of Your Long-Context Language Models.pdf}
}

@misc{lin_2024a,
  title = {Rho-1: {{Not All Tokens Are What You Need}}},
  shorttitle = {Rho-1},
  author = {Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and Shen, Yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and Chen, Weizhu},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-12},
  abstract = {Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that "Not all tokens in a corpus are equally important for language model training". Our initial analysis delves into token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher excess loss. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30\% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6\% and 51.8\% on MATH dataset, respectively - matching DeepSeekMath with only 3\% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1 achieves 6.8\% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.},
  howpublished = {https://arxiv.org/abs/2404.07965v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/NVPA924U/Lin et al. - 2024 - Rho-1 Not All Tokens Are What You Need.pdf}
}

@misc{ma_2024a,
  title = {Megalodon: {{Efficient LLM Pretraining}} and {{Inference}} with {{Unlimited Context Length}}},
  shorttitle = {Megalodon},
  author = {Ma, Xuezhe and Yang, Xiaomeng and Xiong, Wenhan and Chen, Beidi and Yu, Lili and Zhang, Hao and May, Jonathan and Zettlemoyer, Luke and Levy, Omer and Zhou, Chunting},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-17},
  abstract = {The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: https://github.com/XuezheMax/megalodon},
  howpublished = {https://arxiv.org/abs/2404.08801v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/5C8QN2G7/Ma et al. - 2024 - Megalodon Efficient LLM Pretraining and Inference with Unlimited Context Length.pdf}
}

@misc{shao_2024,
  title = {Assisting in {{Writing Wikipedia-like Articles From Scratch}} with {{Large Language Models}}},
  author = {Shao, Yijia and Jiang, Yucheng and Kanell, Theodore A. and Xu, Peter and Khattab, Omar and Lam, Monica S.},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-04-19},
  abstract = {We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25\% absolute increase) and broad in coverage (by 10\%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.},
  howpublished = {https://arxiv.org/abs/2402.14207v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-19]},
  file = {/Users/chenghao/Zotero/storage/6ZUVVC4J/Shao et al. - 2024 - Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models.pdf}
}

@misc{hong_2024,
  title = {{{ORPO}}: {{Monolithic Preference Optimization}} without {{Reference Model}}},
  shorttitle = {{{ORPO}}},
  author = {Hong, Jiwoo and Lee, Noah and Thorne, James},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-04-22},
  abstract = {While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20\% on \${\textbackslash}text\{AlpacaEval\}\_\{2.0\}\$ (Figure 1), 66.19\% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-\${\textbackslash}alpha\$ (7B) and Mistral-ORPO-\${\textbackslash}beta\$ (7B).},
  howpublished = {https://arxiv.org/abs/2403.07691v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/KJU7R9L9/Hong et al. - 2024 - ORPO Monolithic Preference Optimization without Reference Model.pdf}
}

@misc{abdin_2024,
  title = {Phi-3 {{Technical Report}}: {{A Highly Capable Language Model Locally}} on {{Your Phone}}},
  shorttitle = {Phi-3 {{Technical Report}}},
  author = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, S{\'e}bastien and Cai, Martin and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chopra, Parul and Del Giorno, Allie and {de Rosa}, Gustavo and Dixon, Matthew and Eldan, Ronen and Iter, Dan and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liang, Chen and Liu, Weishung and Lin, Eric and Lin, Zeqi and Madan, Piyush and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and {Perez-Becker}, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Song, Xia and Ruwase, Olatunji and Wang, Xin and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wyatt, Michael and Xu, Can and Xu, Jiahang and Yadav, Sonali and Yang, Fan and Yang, Ziyi and Yu, Donghan and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yunan and Zhou, Xiren},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-23},
  abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\% and 78\% on MMLU, and 8.7 and 8.9 on MT-bench).},
  howpublished = {https://arxiv.org/abs/2404.14219v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/CXEDRUV3/Abdin et al. - 2024 - Phi-3 Technical Report A Highly Capable Language Model Locally on Your Phone.pdf}
}

@misc{huang_2024,
  title = {How {{Good Are Low-bit Quantized LLaMA3 Models}}? {{An Empirical Study}}},
  shorttitle = {How {{Good Are Low-bit Quantized LLaMA3 Models}}?},
  author = {Huang, Wei and Ma, Xudong and Qin, Haotong and Zheng, Xingyu and Lv, Chengtao and Chen, Hong and Luo, Jie and Qi, Xiaojuan and Liu, Xianglong and Magno, Michele},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-23},
  abstract = {Meta's LLaMA family has become one of the most powerful open-source Large Language Model (LLM) series. Notably, LLaMA3 models have recently been released and achieve impressive performance across various with super-large scale pre-training on over 15T tokens of data. Given the wide application of low-bit quantization for LLMs in resource-limited scenarios, we explore LLaMA3's capabilities when quantized to low bit-width. This exploration holds the potential to unveil new insights and challenges for low-bit quantization of LLaMA3 and other forthcoming LLMs, especially in addressing performance degradation problems that suffer in LLM compression. Specifically, we evaluate the 10 existing post-training quantization and LoRA-finetuning methods of LLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's low-bit quantization performance. Our experiment results indicate that LLaMA3 still suffers non-negligent degradation in these scenarios, especially in ultra-low bit-width. This highlights the significant performance gap under low bit-width that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, pushing the LLMs to lower bit-width with higher accuracy for being practical. Our project is released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized LLaMA3 models are released in https://huggingface.co/LLMQ.},
  howpublished = {https://arxiv.org/abs/2404.14047v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/DL8FD48R/Huang et al. - 2024 - How Good Are Low-bit Quantized LLaMA3 Models An Empirical Study.pdf}
}

@misc{xiang_2024,
  title = {How {{Far Can We Go}} with {{Practical Function-Level Program Repair}}?},
  author = {Xiang, Jiahong and Xu, Xiaoyang and Kong, Fanchu and Wu, Mingyuan and Zhang, Haotian and Zhang, Yuqun},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-23},
  abstract = {Recently, multiple Automated Program Repair (APR) techniques based on Large Language Models (LLMs) have been proposed to enhance the repair performance. While these techniques mainly focus on the single-line or hunk-level repair, they face significant challenges in real-world application due to the limited repair task scope and costly statement-level fault localization. However, the more practical function-level APR, which broadens the scope of APR task to fix entire buggy functions and requires only cost-efficient function-level fault localization, remains underexplored. In this paper, we conduct the first comprehensive study of LLM-based function-level APR including investigating the effect of the few-shot learning mechanism and the auxiliary repair-relevant information. Specifically, we adopt six widely-studied LLMs and construct a benchmark in both the Defects4J 1.2 and 2.0 datasets. Our study demonstrates that LLMs with zero-shot learning are already powerful function-level APR techniques, while applying the few-shot learning mechanism leads to disparate repair performance. Moreover, we find that directly applying the auxiliary repair-relevant information to LLMs significantly increases function-level repair performance. Inspired by our findings, we propose an LLM-based function-level APR technique, namely SRepair, which adopts a dual-LLM framework to leverage the power of the auxiliary repair-relevant information for advancing the repair performance. The evaluation results demonstrate that SRepair can correctly fix 300 single-function bugs in the Defects4J dataset, largely surpassing all previous APR techniques by at least 85\%, without the need for the costly statement-level fault location information. Furthermore, SRepair successfully fixes 32 multi-function bugs in the Defects4J dataset, which is the first time achieved by any APR technique ever to our best knowledge.},
  howpublished = {https://arxiv.org/abs/2404.12833v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/RXSAUPHT/Xiang et al. - 2024 - How Far Can We Go with Practical Function-Level Program Repair.pdf}
}

@misc{agarwal_2024,
  title = {Many-{{Shot In-Context Learning}}},
  author = {Agarwal, Rishabh and Singh, Avi and Zhang, Lei M. and Bohnet, Bernd and Chan, Stephanie and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and {Co-Reyes}, John D. and Chu, Eric and Behbahani, Feryal and Faust, Aleksandra and Larochelle, Hugo},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-25},
  abstract = {Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.},
  howpublished = {https://arxiv.org/abs/2404.11018v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-25]},
  file = {/Users/chenghao/Zotero/storage/DIUGG7PY/Agarwal et al. - 2024 - Many-Shot In-Context Learning.pdf}
}

@article{solove_2023,
  title = {Murky {{Consent}}: {{An Approach}} to the {{Fictions}} of {{Consent}} in {{Privacy Law}}},
  shorttitle = {Murky {{Consent}}},
  author = {Solove, Daniel J.},
  year = {2023},
  month = aug,
  doi = {10.2139/ssrn.4333743},
  urldate = {2024-04-25},
  abstract = {Consent plays a profound role in nearly all privacy laws. As Professor Heidi Hurd aptly said, consent works ``moral magic'' -- it transforms things that would be i},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/BYF7GAKY/Solove - 2023 - Murky Consent An Approach to the Fictions of Consent in Privacy Law.pdf}
}

@misc{qian_2024,
  title = {{{BASS}}: {{Batched Attention-optimized Speculative Sampling}}},
  shorttitle = {{{BASS}}},
  author = {Qian, Haifeng and Gonugondla, Sujan Kumar and Ha, Sungsoo and Shang, Mingyue and Gouda, Sanjay Krishna and Nallapati, Ramesh and Sengupta, Sudipta and Ma, Xiaofei and Deoras, Anoop},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-25},
  abstract = {Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43\% and Pass@All of 61\%, far exceeding what's feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8\%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.},
  howpublished = {https://arxiv.org/abs/2404.15778v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/S4JDBQEZ/Qian et al. - 2024 - BASS Batched Attention-optimized Speculative Sampling.pdf}
}

@misc{munkhdalai_2024,
  title = {Leave {{No Context Behind}}: {{Efficient Infinite Context Transformers}} with {{Infini-attention}}},
  shorttitle = {Leave {{No Context Behind}}},
  author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-25},
  abstract = {This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.},
  howpublished = {https://arxiv.org/abs/2404.07143v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-25]},
  file = {/Users/chenghao/Zotero/storage/J9BA9FI5/Munkhdalai et al. - 2024 - Leave No Context Behind Efficient Infinite Context Transformers with Infini-attention.pdf}
}

@misc{wu_2024,
  title = {How Faithful Are {{RAG}} Models? {{Quantifying}} the Tug-of-War between {{RAG}} and {{LLMs}}' Internal Prior},
  shorttitle = {How Faithful Are {{RAG}} Models?},
  author = {Wu, Kevin and Wu, Eric and Zou, James},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-25},
  abstract = {Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, we systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. We test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94\% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, we also find that the more the modified information deviates from the model's prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents.},
  howpublished = {https://arxiv.org/abs/2404.10198v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-25]},
  file = {/Users/chenghao/Zotero/storage/77KW7JCC/Wu et al. - 2024 - How faithful are RAG models Quantifying the tug-of-war between RAG and LLMs' internal prior.pdf}
}

@misc{dai_2024,
  title = {Sequence Can {{Secretly Tell You What}} to {{Discard}}},
  author = {Dai, Jincheng and Huang, Zhuowei and Jiang, Haiyun and Chen, Chen and Cai, Deng and Bi, Wei and Shi, Shuming},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-27},
  abstract = {Large Language Models (LLMs), despite their impressive performance on a wide range of tasks, require significant GPU memory and consume substantial computational resources. In addition to model weights, the memory occupied by KV cache increases linearly with sequence length, becoming a main bottleneck for inference. In this paper, we introduce a novel approach for optimizing the KV cache which significantly reduces its memory footprint. Through a comprehensive investigation, we find that on LLaMA2 series models, (i) the similarity between adjacent tokens' query vectors is remarkably high, and (ii) current query's attention calculation can rely solely on the attention information of a small portion of the preceding queries. Based on these observations, we propose CORM, a KV cache eviction policy that dynamically retains important key-value pairs for inference without finetuning the model. We validate that CORM reduces the inference memory usage of KV cache by up to 70\% without noticeable performance degradation across six tasks in LongBench.},
  howpublished = {https://arxiv.org/abs/2404.15949v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-27]},
  file = {/Users/chenghao/Zotero/storage/AM83TBJP/Dai et al. - 2024 - Sequence can Secretly Tell You What to Discard.pdf}
}

@misc{mehta_2024,
  title = {{{CatLIP}}: {{CLIP-level Visual Recognition Accuracy}} with 2.7x {{Faster Pre-training}} on {{Web-scale Image-Text Data}}},
  shorttitle = {{{CatLIP}}},
  author = {Mehta, Sachin and Horton, Maxwell and Faghri, Fartash and Sekhavat, Mohammad Hossein and Najibi, Mahyar and Farajtabar, Mehrdad and Tuzel, Oncel and Rastegari, Mohammad},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-27},
  abstract = {Contrastive learning has emerged as a transformative method for learning effective visual representations through the alignment of image and text embeddings. However, pairwise similarity computation in contrastive loss between image and text pairs poses computational challenges. This paper presents a novel weakly supervised pre-training of vision models on web-scale image-text data. The proposed method reframes pre-training on image-text data as a classification task. Consequently, it eliminates the need for pairwise similarity computations in contrastive loss, achieving a remarkable \$2.7{\textbackslash}times\$ acceleration in training speed compared to contrastive learning on web-scale data. Through extensive experiments spanning diverse vision tasks, including detection and segmentation, we demonstrate that the proposed method maintains high representation quality. Our source code along with pre-trained model weights and training recipes is available at {\textbackslash}url\{https://github.com/apple/corenet\}.},
  howpublished = {https://arxiv.org/abs/2404.15653v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/ERQJMP3Q/Mehta et al. - 2024 - CatLIP CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text.pdf}
}

@misc{lehnert_2024,
  title = {Beyond {{A}}*: {{Better Planning}} with {{Transformers}} via {{Search Dynamics Bootstrapping}}},
  shorttitle = {Beyond {{A}}*},
  author = {Lehnert, Lucas and Sukhbaatar, Sainbayar and Mcvay, Paul and Rabbat, Michael and Tian, Yuandong},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-04-27},
  abstract = {While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7\% of the time, while using up to 26.8\% fewer search steps than standard \$A{\textasciicircum}*\$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of \$A{\textasciicircum}*\$. This model is then fine-tuned via expert iterations to perform fewer search steps than \$A{\textasciicircum}*\$ search while still generating an optimal plan. In our training method, \$A{\textasciicircum}*\$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10\${\textbackslash}times\$ smaller model size and a 10\${\textbackslash}times\$ smaller training dataset. We also demonstrate how Searchformer scales to larger and more complex decision making tasks like Sokoban with improved percentage of solved tasks and shortened search dynamics.},
  howpublished = {https://arxiv.org/abs/2402.14083v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/8FCNNR43/Lehnert et al. - 2024 - Beyond A Better Planning with Transformers via Search Dynamics Bootstrapping.pdf}
}

@misc{_i,
  title = {A {{Visual Guide}} to {{Mamba}} and {{State Space Models}}},
  urldate = {2024-04-27},
  howpublished = {https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state},
  file = {/Users/chenghao/Zotero/storage/PH47YS8F/a-visual-guide-to-mamba-and-state.html}
}

@misc{_j,
  title = {Rotary {{Embeddings}}: {{A Relative Revolution}} {\textbar} {{EleutherAI Blog}}},
  urldate = {2024-04-27},
  howpublished = {https://blog.eleuther.ai/rotary-embeddings/},
  file = {/Users/chenghao/Zotero/storage/TBZJ3WT5/rotary-embeddings.html}
}

@misc{mehta_2024a,
  title = {{{OpenELM}}: {{An Efficient Language Model Family}} with {{Open-source Training}} and {{Inference Framework}}},
  shorttitle = {{{OpenELM}}},
  author = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-28},
  abstract = {The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36\% improvement in accuracy compared to OLMo while requiring \$2{\textbackslash}times\$ fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at {\textbackslash}url\{https://github.com/apple/corenet\}. Additionally, {\textbackslash}model models can be found on HuggingFace at: {\textbackslash}url\{https://huggingface.co/apple/OpenELM\}.},
  howpublished = {https://arxiv.org/abs/2404.14619v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/PU3CVTAK/Mehta et al. - 2024 - OpenELM An Efficient Language Model Family with Open-source Training and Inference Framework.pdf}
}

@misc{gabriel_2024,
  title = {The {{Ethics}} of {{Advanced AI Assistants}}},
  author = {Gabriel, Iason and Manzini, Arianna and Keeling, Geoff and Hendricks, Lisa Anne and Rieser, Verena and Iqbal, Hasan and Toma{\v s}ev, Nenad and Ktena, Ira and Kenton, Zachary and Rodriguez, Mikel and {El-Sayed}, Seliem and Brown, Sasha and Akbulut, Canfer and Trask, Andrew and Hughes, Edward and Bergman, A. Stevie and Shelby, Renee and Marchal, Nahema and Griffin, Conor and {Mateos-Garcia}, Juan and Weidinger, Laura and Street, Winnie and Lange, Benjamin and Ingerman, Alex and Lentz, Alison and Enger, Reed and Barakat, Andrew and Krakovna, Victoria and Siy, John Oliver and {Kurth-Nelson}, Zeb and McCroskery, Amanda and Bolina, Vijay and Law, Harry and Shanahan, Murray and Alberts, Lize and Balle, Borja and {de Haas}, Sarah and Ibitoye, Yetunde and Dafoe, Allan and Goldberg, Beth and Krier, S{\'e}bastien and Reese, Alexander and Witherspoon, Sims and Hawkins, Will and Rauh, Maribeth and Wallace, Don and Franklin, Matija and Goldstein, Josh A. and Lehman, Joel and Klenk, Michael and Vallor, Shannon and Biles, Courtney and Morris, Meredith Ringel and King, Helen and y Arcas, Blaise Ag{\"u}era and Isaac, William and Manyika, James},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-30},
  abstract = {This paper focuses on the opportunities and the ethical and societal risks posed by advanced AI assistants. We define advanced AI assistants as artificial agents with natural language interfaces, whose function is to plan and execute sequences of actions on behalf of a user, across one or more domains, in line with the user's expectations. The paper starts by considering the technology itself, providing an overview of AI assistants, their technical foundations and potential range of applications. It then explores questions around AI value alignment, well-being, safety and malicious uses. Extending the circle of inquiry further, we next consider the relationship between advanced AI assistants and individual users in more detail, exploring topics such as manipulation and persuasion, anthropomorphism, appropriate relationships, trust and privacy. With this analysis in place, we consider the deployment of advanced assistants at a societal scale, focusing on cooperation, equity and access, misinformation, economic impact, the environment and how best to evaluate advanced AI assistants. Finally, we conclude by providing a range of recommendations for researchers, developers, policymakers and public stakeholders.},
  howpublished = {https://arxiv.org/abs/2404.16244v2},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/arXiv) [2024-04-30]},
  file = {/Users/chenghao/Zotero/storage/DXUGZ9CQ/Gabriel et al. - 2024 - The Ethics of Advanced AI Assistants.pdf}
}

@misc{besiroglu_2024,
  title = {Chinchilla {{Scaling}}: {{A}} Replication Attempt},
  shorttitle = {Chinchilla {{Scaling}}},
  author = {Besiroglu, Tamay and Erdil, Ege and Barnett, Matthew and You, Josh},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-30},
  abstract = {Hoffmann et al. (2022) propose three methods for estimating a compute-optimal scaling law. We attempt to replicate their third estimation procedure, which involves fitting a parametric loss function to a reconstruction of data from their plots. We find that the reported estimates are inconsistent with their first two estimation methods, fail at fitting the extracted data, and report implausibly narrow confidence intervals--intervals this narrow would require over 600,000 experiments, while they likely only ran fewer than 500. In contrast, our rederivation of the scaling law using the third approach yields results that are compatible with the findings from the first two estimation procedures described by Hoffmann et al.},
  howpublished = {https://arxiv.org/abs/2404.10102v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-30]},
  file = {/Users/chenghao/Zotero/storage/REKAHWM6/Besiroglu et al. - 2024 - Chinchilla Scaling A replication attempt.pdf}
}

@misc{hayou_2024,
  title = {{{LoRA}}+: {{Efficient Low Rank Adaptation}} of {{Large Models}}},
  shorttitle = {{{LoRA}}+},
  author = {Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-04-30},
  abstract = {In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA\$+\$. In our extensive experiments, LoRA\$+\$ improves performance (1-2 \${\textbackslash}\%\$ improvements) and finetuning speed (up to \${\textbackslash}sim\$ 2X SpeedUp), at the same computational cost as LoRA.},
  howpublished = {https://arxiv.org/abs/2402.12354v1},
  langid = {english},
  annotation = {5 citations (Semantic Scholar/arXiv) [2024-04-30]},
  file = {/Users/chenghao/Zotero/storage/PXVCXM5P/Hayou et al. - 2024 - LoRA+ Efficient Low Rank Adaptation of Large Models.pdf}
}

@misc{pfau_2024,
  title = {Let's {{Think Dot}} by {{Dot}}: {{Hidden Computation}} in {{Transformer Language Models}}},
  shorttitle = {Let's {{Think Dot}} by {{Dot}}},
  author = {Pfau, Jacob and Merrill, William and Bowman, Samuel R.},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-30},
  abstract = {Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.},
  howpublished = {https://arxiv.org/abs/2404.15758v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-30]},
  file = {/Users/chenghao/Zotero/storage/65AFWU7Z/Pfau et al. - 2024 - Let's Think Dot by Dot Hidden Computation in Transformer Language Models.pdf}
}

@misc{liu_2024,
  title = {Best {{Practices}} and {{Lessons Learned}} on {{Synthetic Data}} for {{Language Models}}},
  author = {Liu, Ruibo and Wei, Jerry and Liu, Fangyu and Si, Chenglei and Zhang, Yanzhe and Rao, Jinmeng and Zheng, Steven and Peng, Daiyi and Yang, Diyi and Zhou, Denny and Dai, Andrew M.},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-04-30},
  abstract = {The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.},
  howpublished = {https://arxiv.org/abs/2404.07503v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-30]},
  file = {/Users/chenghao/Zotero/storage/E6CB2SG5/Liu et al. - 2024 - Best Practices and Lessons Learned on Synthetic Data for Language Models.pdf}
}

@misc{gloeckle_2024,
  title = {Better \& {{Faster Large Language Models}} via {{Multi-token Prediction}}},
  author = {Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\`e}re, Baptiste and {Lopez-Paz}, David and Synnaeve, Gabriel},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-01},
  abstract = {Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 \% more problems on HumanEval and 17 \% more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.},
  howpublished = {https://arxiv.org/abs/2404.19737v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-01]},
  file = {/Users/chenghao/Zotero/storage/GJ55ZZJB/Gloeckle et al. - 2024 - Better & Faster Large Language Models via Multi-token Prediction.pdf}
}

@misc{zhang_2024b,
  title = {A {{Careful Examination}} of {{Large Language Model Performance}} on {{Grade School Arithmetic}}},
  author = {Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Slack, Dylan and Lyu, Qin and Hendryx, Sean and Kaplan, Russell and Michele and Lunati and Yue, Summer},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-02},
  abstract = {Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 13\%, with several families of models (e.g., Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g., Gemini/GPT/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman's r{\textasciicircum}2=0.32) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that many models may have partially memorized GSM8k.},
  howpublished = {https://arxiv.org/abs/2405.00332v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/FSFH9EVZ/Zhang et al. - 2024 - A Careful Examination of Large Language Model Performance on Grade School Arithmetic.pdf}
}

@misc{wu_2024a,
  title = {Self-{{Play Preference Optimization}} for {{Language Model Alignment}}},
  author = {Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-02},
  abstract = {Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed {\textbackslash}textit\{Self-Play Preference Optimization\} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53\% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.},
  howpublished = {https://arxiv.org/abs/2405.00675v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-02]},
  file = {/Users/chenghao/Zotero/storage/ACKBPRAQ/Wu et al. - 2024 - Self-Play Preference Optimization for Language Model Alignment.pdf}
}

@misc{ravaut_2024,
  title = {How {{Much}} Are {{LLMs Contaminated}}? {{A Comprehensive Survey}} and the {{LLMSanitize Library}}},
  shorttitle = {How {{Much}} Are {{LLMs Contaminated}}?},
  author = {Ravaut, Mathieu and Ding, Bosheng and Jiao, Fangkai and Chen, Hailin and Li, Xingxuan and Zhao, Ruochen and Qin, Chengwei and Xiong, Caiming and Joty, Shafiq},
  year = {2024},
  month = mar,
  number = {arXiv:2404.00699},
  eprint = {2404.00699},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-02},
  abstract = {With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address contamination, or a clear consensus on prevention, mitigation and classification of contamination. In this paper, we survey all recent work on contamination with LLMs, and help the community track contamination levels of LLMs by releasing an open-source Python library named LLMSanitize implementing major contamination detection algorithms, which link is: https://github.com/ntunlp/LLMSanitize.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/R6KZ2YLJ/Ravaut et al. - 2024 - How Much are LLMs Contaminated A Comprehensive Survey and the LLMSanitize Library.pdf}
}

@misc{balne_2024,
  title = {Parameter {{Efficient Fine Tuning}}: {{A Comprehensive Analysis Across Applications}}},
  shorttitle = {Parameter {{Efficient Fine Tuning}}},
  author = {Balne, Charith Chandra Sai and Bhaduri, Sreyoshi and Roy, Tamoghna and Jain, Vinija and Chadha, Aman},
  year = {2024},
  month = apr,
  number = {arXiv:2404.13506},
  eprint = {2404.13506},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-02},
  abstract = {The rise of deep learning has marked significant progress in fields such as computer vision, natural language processing, and medical imaging, primarily through the adaptation of pre-trained models for specific tasks. Traditional fine-tuning methods, involving adjustments to all parameters, face challenges due to high computational and memory demands. This has led to the development of Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively update parameters to balance computational efficiency with performance. This review examines PEFT approaches, offering a detailed comparison of various strategies highlighting applications across different domains, including text generation, medical imaging, protein modeling, and speech synthesis. By assessing the effectiveness of PEFT methods in reducing computational load, speeding up training, and lowering memory usage, this paper contributes to making deep learning more accessible and adaptable, facilitating its wider application and encouraging innovation in model optimization. Ultimately, the paper aims to contribute towards insights into PEFT's evolving landscape, guiding researchers and practitioners in overcoming the limitations of conventional fine-tuning approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/XBU8V42T/Balne et al. - 2024 - Parameter Efficient Fine Tuning A Comprehensive Analysis Across Applications.pdf}
}

@misc{zhao_2024a,
  title = {{{LoRA Land}}: 310 {{Fine-tuned LLMs}} That {{Rival GPT-4}}, {{A Technical Report}}},
  shorttitle = {{{LoRA Land}}},
  author = {Zhao, Justin and Wang, Timothy and Abid, Wael and Angus, Geoffrey and Garg, Arnav and Kinnison, Jeffery and Sherstinsky, Alex and Molino, Piero and Addair, Travis and Rishi, Devvret},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-03},
  abstract = {Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.},
  howpublished = {https://arxiv.org/abs/2405.00732v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-03]},
  file = {/Users/chenghao/Zotero/storage/AH3PYU43/Zhao et al. - 2024 - LoRA Land 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report.pdf}
}

@misc{kim_2024,
  title = {Prometheus 2: {{An Open Source Language Model Specialized}} in {{Evaluating Other Language Models}}},
  shorttitle = {Prometheus 2},
  author = {Kim, Seungone and Suk, Juyoung and Longpre, Shayne and Lin, Bill Yuchen and Shin, Jamin and Welleck, Sean and Neubig, Graham and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-03},
  abstract = {Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.},
  howpublished = {https://arxiv.org/abs/2405.01535v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-03]},
  file = {/Users/chenghao/Zotero/storage/KTRK3EJC/Kim et al. - 2024 - Prometheus 2 An Open Source Language Model Specialized in Evaluating Other Language Models.pdf}
}

@misc{swayamdipta_2020,
  title = {Dataset {{Cartography}}: {{Mapping}} and {{Diagnosing Datasets}} with {{Training Dynamics}}},
  shorttitle = {Dataset {{Cartography}}},
  author = {Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A. and Choi, Yejin},
  year = {2020},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-05-03},
  abstract = {Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.},
  howpublished = {https://arxiv.org/abs/2009.10795v2},
  langid = {english},
  annotation = {319 citations (Semantic Scholar/arXiv) [2024-05-03]},
  file = {/Users/chenghao/Zotero/storage/P8PDK9FN/Swayamdipta et al. - 2020 - Dataset Cartography Mapping and Diagnosing Datasets with Training Dynamics.pdf}
}

@misc{ghorbani_2019,
  title = {Data {{Shapley}}: {{Equitable Valuation}} of {{Data}} for {{Machine Learning}}},
  shorttitle = {Data {{Shapley}}},
  author = {Ghorbani, Amirata and Zou, James},
  year = {2019},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-03},
  abstract = {As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on \$n\$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.},
  howpublished = {https://arxiv.org/abs/1904.02868v2},
  langid = {english},
  annotation = {527 citations (Semantic Scholar/arXiv) [2024-05-03]},
  file = {/Users/chenghao/Zotero/storage/TFGUT2I8/Ghorbani and Zou - 2019 - Data Shapley Equitable Valuation of Data for Machine Learning.pdf}
}

@misc{koh_2017,
  title = {Understanding {{Black-box Predictions}} via {{Influence Functions}}},
  author = {Koh, Pang Wei and Liang, Percy},
  year = {2017},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-05-03},
  abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  howpublished = {https://arxiv.org/abs/1703.04730v3},
  langid = {english},
  annotation = {2312 citations (Semantic Scholar/arXiv) [2024-05-03]},
  file = {/Users/chenghao/Zotero/storage/28NMWAAV/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence Functions.pdf}
}

@misc{mishra_2020,
  title = {{{DQI}}: {{Measuring Data Quality}} in {{NLP}}},
  shorttitle = {{{DQI}}},
  author = {Mishra, Swaroop and Arunkumar, Anjana and Sachdeva, Bhavdeep and Bryan, Chris and Baral, Chitta},
  year = {2020},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-03},
  abstract = {Neural language models have achieved human level performance across several NLP datasets. However, recent studies have shown that these models are not truly learning the desired task; rather, their high performance is attributed to overfitting using spurious biases, which suggests that the capabilities of AI systems have been over-estimated. We introduce a generic formula for Data Quality Index (DQI) to help dataset creators create datasets free of such unwanted biases. We evaluate this formula using a recently proposed approach for adversarial filtering, AFLite. We propose a new data creation paradigm using DQI to create higher quality data. The data creation paradigm consists of several data visualizations to help data creators (i) understand the quality of data and (ii) visualize the impact of the created data instance on the overall quality. It also has a couple of automation methods to (i) assist data creators and (ii) make the model more robust to adversarial attacks. We use DQI along with these automation methods to renovate biased examples in SNLI. We show that models trained on the renovated SNLI dataset generalize better to out of distribution tasks. Renovation results in reduced model performance, exposing a large gap with respect to human performance. DQI systematically helps in creating harder benchmarks using active learning. Our work takes the process of dynamic dataset creation forward, wherein datasets evolve together with the evolving state of the art, therefore serving as a means of benchmarking the true progress of AI.},
  howpublished = {https://arxiv.org/abs/2005.00816v1},
  langid = {english},
  annotation = {22 citations (Semantic Scholar/arXiv) [2024-05-03]},
  file = {/Users/chenghao/Zotero/storage/WF3N59RC/Mishra et al. - 2020 - DQI Measuring Data Quality in NLP.pdf}
}

@misc{elhoushi_2024,
  title = {{{LayerSkip}}: {{Enabling Early Exit Inference}} and {{Self-Speculative Decoding}}},
  shorttitle = {{{LayerSkip}}},
  author = {Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and Aly, Ahmed A. and Chen, Beidi and Wu, Carole-Jean},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-03},
  abstract = {We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.},
  howpublished = {https://arxiv.org/abs/2404.16710v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-03]},
  file = {/Users/chenghao/Zotero/storage/84T7EF9S/Elhoushi et al. - 2024 - LayerSkip Enabling Early Exit Inference and Self-Speculative Decoding.pdf}
}

@misc{liu_2024a,
  title = {{{KAN}}: {{Kolmogorov-Arnold Networks}}},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v c}i{\'c}, Marin and Hou, Thomas Y. and Tegmark, Max},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-04},
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
  howpublished = {https://arxiv.org/abs/2404.19756v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/8NZZ7Y75/Liu et al. - 2024 - KAN Kolmogorov-Arnold Networks.pdf}
}

@misc{chen_2024a,
  title = {Sequoia: {{Scalable}}, {{Robust}}, and {{Hardware-aware Speculative Decoding}}},
  shorttitle = {Sequoia},
  author = {Chen, Zhuoming and May, Avner and Svirschevski, Ruslan and Huang, Yuhsun and Ryabinin, Max and Jia, Zhihao and Chen, Beidi},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-05-05},
  abstract = {As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to \$4.04{\textbackslash}times\$, \$3.73{\textbackslash}times\$, and \$2.27{\textbackslash}times\$. For offloading setting on L40, Sequoia achieves as low as 0.56 s/token for exact Llama2-70B inference latency, which is \$9.96{\textbackslash}times\$ on our optimized offloading system (5.6 s/token), \$9.7{\textbackslash}times\$ than DeepSpeed-Zero-Inference, \$19.5{\textbackslash}times\$ than Huggingface Accelerate.},
  howpublished = {https://arxiv.org/abs/2402.12374v2},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/arXiv) [2024-05-05]},
  file = {/Users/chenghao/Zotero/storage/VHJ5KSSV/Chen et al. - 2024 - Sequoia Scalable, Robust, and Hardware-aware Speculative Decoding.pdf}
}

@misc{markovtsev_2018,
  title = {Splitting Source Code Identifiers Using {{Bidirectional LSTM Recurrent Neural Network}}},
  author = {Markovtsev, Vadim and Long, Waren and Bulychev, Egor and Keramitas, Romain and Slavnov, Konstantin and Markowski, Gabor},
  year = {2018},
  month = jul,
  number = {arXiv:1805.11651},
  eprint = {1805.11651},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.11651},
  urldate = {2024-05-06},
  abstract = {Programmers make rich use of natural language in the source code they write through identifiers and comments. Source code identifiers are selected from a pool of tokens which are strongly related to the meaning, naming conventions, and context. These tokens are often combined to produce more precise and obvious designations. Such multi-part identifiers count for 97\% of all naming tokens in the Public Git Archive - the largest dataset of Git repositories to date. We introduce a bidirectional LSTM recurrent neural network to detect subtokens in source code identifiers. We trained that network on 41.7 million distinct splittable identifiers collected from 182,014 open source projects in Public Git Archive, and show that it outperforms several other machine learning models. The proposed network can be used to improve the upstream models which are based on source code identifiers, as well as improving developer experience allowing writing code without switching the keyboard case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Programming Languages},
  file = {/Users/chenghao/Zotero/storage/5B4E8WHP/Markovtsev et al. - 2018 - Splitting source code identifiers using Bidirectional LSTM Recurrent Neural Network.pdf}
}

@misc{markovtsev_2018a,
  title = {Public {{Git Archive}}: A {{Big Code}} Dataset for All},
  shorttitle = {Public {{Git Archive}}},
  author = {Markovtsev, Vadim and Long, Waren},
  year = {2018},
  month = mar,
  journal = {arXiv.org},
  doi = {10.1145/3196398.3196464},
  urldate = {2024-05-06},
  abstract = {The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive -- dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for ``Big Code`` research.},
  howpublished = {https://arxiv.org/abs/1803.10144v1},
  langid = {english},
  annotation = {41 citations (Semantic Scholar/arXiv) [2024-05-06]},
  file = {/Users/chenghao/Zotero/storage/DBNTJQL3/Markovtsev and Long - 2018 - Public Git Archive a Big Code dataset for all.pdf}
}

@misc{long_2019,
  title = {Identifying Collaborators in Large Codebases},
  author = {Long, Waren and Markovtsev, Vadim and Mougard, Hugo and Bulychev, Egor and Hula, Jan},
  year = {2019},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-06},
  abstract = {The way developers collaborate inside and particularly across teams often escapes management's attention, despite a formal organization with designated teams being defined. Observability of the actual, organically formed engineering structure provides decision makers invaluable additional tools to manage their talent pool. To identify existing inter and intra-team interactions - and suggest relevant opportunities for suitable collaborations - this paper studies contributors' commit activity, usage of programming languages, and code identifier topics by embedding and clustering them. We evaluate our findings collaborating with the GitLab organization, analyzing 117 of their open source projects. We show that we are able to restore their engineering organization in broad strokes, and also reveal hidden coding collaborations as well as justify in-house technical decisions.},
  howpublished = {https://arxiv.org/abs/1905.06782v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/CB5ISPSA/Long et al. - 2019 - Identifying collaborators in large codebases.pdf}
}

@misc{markovtsev_2019,
  title = {{{STYLE-ANALYZER}}: Fixing Code Style Inconsistencies with Interpretable Unsupervised Algorithms},
  shorttitle = {{{STYLE-ANALYZER}}},
  author = {Markovtsev, Vadim and Long, Waren and Mougard, Hugo and Slavnov, Konstantin and Bulychev, Egor},
  year = {2019},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-06},
  abstract = {Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes. This paper introduces STYLE-ANALYZER, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. STYLE-ANALYZER is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. STYLE-ANALYZER can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of STYLE-ANALYZER by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. STYLE-ANALYZER includes a web application to visualize how the rules are triggered. We release STYLE-ANALYZER as a reusable and extendable open source software package on GitHub for the benefit of the community.},
  howpublished = {https://arxiv.org/abs/1904.00935v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/S7BA2E9C/Markovtsev et al. - 2019 - STYLE-ANALYZER fixing code style inconsistencies with interpretable unsupervised algorithms.pdf}
}

@misc{liu_2024b,
  title = {Infini-Gram: {{Scaling Unbounded}} n-Gram {{Language Models}} to a {{Trillion Tokens}}},
  shorttitle = {Infini-Gram},
  author = {Liu, Jiacheng and Min, Sewon and Zettlemoyer, Luke and Choi, Yejin and Hajishirzi, Hannaneh},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-05-07},
  abstract = {Are \$n\$-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we showcase their values in both text analysis and improving neural LLMs. This was done by modernizing \$n\$-gram LMs in two aspects. First, we train them at the same data scale as neural LLMs -- 5 trillion tokens. This is the largest \$n\$-gram LM ever built. Second, existing \$n\$-gram LMs use small \$n\$ which hinders their performance; we instead allow \$n\$ to be arbitrarily large, by introducing a new \${\textbackslash}infty\$-gram LM with backoff. Instead of pre-computing \$n\$-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute \${\textbackslash}infty\$-gram (as well as \$n\$-gram with arbitrary \$n\$) probabilities with millisecond-level latency. The \${\textbackslash}infty\$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the \${\textbackslash}infty\$-gram LM has fairly high accuracy for next-token prediction (47\%), and can complement neural LLMs to greatly reduce their perplexity. When analyzing machine-generated text, we also observe irregularities in the machine--\${\textbackslash}infty\$-gram agreement level with respect to the suffix length, which indicates deficiencies in neural LLM pretraining and the positional embeddings of Transformers.},
  howpublished = {https://arxiv.org/abs/2401.17377v3},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/UWPP576L/Liu et al. - 2024 - Infini-gram Scaling Unbounded n-gram Language Models to a Trillion Tokens.pdf}
}

@misc{beck_2024,
  title = {{{xLSTM}}: {{Extended Long Short-Term Memory}}},
  shorttitle = {{{xLSTM}}},
  author = {Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-08},
  abstract = {In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.},
  howpublished = {https://arxiv.org/abs/2405.04517v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/K7HY3CDQ/Beck et al. - 2024 - xLSTM Extended Long Short-Term Memory.pdf}
}

@misc{bogdanov_2024,
  title = {{{NuNER}}: {{Entity Recognition Encoder Pre-training}} via {{LLM-Annotated Data}}},
  shorttitle = {{{NuNER}}},
  author = {Bogdanov, Sergei and Constantin, Alexandre and Bernard, Timoth{\'e}e and Crabb{\'e}, Benoit and Bernard, Etienne},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-05-08},
  abstract = {Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.},
  howpublished = {https://arxiv.org/abs/2402.15343v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/QMM8ZISX/Bogdanov et al. - 2024 - NuNER Entity Recognition Encoder Pre-training via LLM-Annotated Data.pdf}
}

@misc{laurencon_2024,
  title = {What Matters When Building Vision-Language Models?},
  author = {Lauren{\c c}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-08},
  abstract = {The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.},
  howpublished = {https://arxiv.org/abs/2405.02246v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/TCD2HCIN/Laurenon et al. - 2024 - What matters when building vision-language models.pdf}
}

@article{koch_,
  title = {The {{Fault}} in {{Our Stars}}: {{An Analysis}} of {{GitHub Stars}} as an {{Importance Metric}} for {{Web Source Code}}},
  author = {Koch, Simon and Klein, David and Johns, Martin},
  abstract = {Are GitHub stars a good surrogate metric to assess the importance of open-source code? While security research frequently uses them as a proxy for importance, the reliability of this relationship has not been studied yet. Furthermore, its relationship to download numbers provided by code registries -- another commonly used metric -- has yet to be ascertained. We address this research gap by analyzing the correlation between both GitHub stars and download numbers as well as their correlation with detected deployments across websites. Our data set consists of 925 978 data points across three web programming languages: PHP, Ruby, and JavaScript. We assess deployment across websites using 58 hand-crafted fingerprints for JavaScript libraries. Our results reveal a weak relationship between GitHub Stars and download numbers ranging from a correlation of 0.47 for PHP down to 0.14 for JavaScript, as well as a high amount of low star and high download projects for PHP and Ruby and an opposite pattern for JavaScript with a noticeably higher count of high star and apparently low download libraries. Concerning the relationship for detected deployments, we discovered a correlation of 0.61 and 0.63 with stars and downloads, respectively. Our results indicate that both downloads and stars pose a moderately strong indicator of the importance of client-side deployed JavaScript libraries.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/8ZLXX7VZ/Koch et al. - The Fault in Our Stars An Analysis of GitHub Stars as an Importance Metric for Web Source Code.pdf}
}

@misc{land_2024,
  title = {Fishing for {{Magikarp}}: {{Automatically Detecting Under-trained Tokens}} in {{Large Language Models}}},
  shorttitle = {Fishing for {{Magikarp}}},
  author = {Land, Sander and Bartolo, Max},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-10},
  abstract = {The disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing. We present a comprehensive analysis of Large Language Model (LLM) tokenizers, specifically targeting this issue of detecting untrained and under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models.},
  howpublished = {https://arxiv.org/abs/2405.05417v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/SNCCJ74K/Land and Bartolo - 2024 - Fishing for Magikarp Automatically Detecting Under-trained Tokens in Large Language Models.pdf}
}

@misc{gekhman_2024,
  title = {Does {{Fine-Tuning LLMs}} on {{New Knowledge Encourage Hallucinations}}?},
  author = {Gekhman, Zorik and Yona, Gal and Aharoni, Roee and Eyal, Matan and Feder, Amir and Reichart, Roi and Herzig, Jonathan},
  year = {2024},
  month = may,
  number = {arXiv:2405.05904},
  eprint = {2405.05904},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-11},
  abstract = {When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas finetuning teaches them to use it more efficiently.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-11]},
  file = {/Users/chenghao/Zotero/storage/Y8CBCX59/Gekhman et al. - 2024 - Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations.pdf}
}

@misc{zhu_2023,
  title = {{{DyVal}}: {{Dynamic Evaluation}} of {{Large Language Models}} for {{Reasoning Tasks}}},
  shorttitle = {{{DyVal}}},
  author = {Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-05-11},
  abstract = {Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks. We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.},
  howpublished = {https://arxiv.org/abs/2309.17167v3},
  langid = {english},
  annotation = {5 citations (Semantic Scholar/arXiv) [2024-05-11]},
  file = {/Users/chenghao/Zotero/storage/U878JPXL/Zhu et al. - 2023 - DyVal Dynamic Evaluation of Large Language Models for Reasoning Tasks.pdf}
}

@misc{kou_2024,
  title = {{{CLLMs}}: {{Consistency Large Language Models}}},
  shorttitle = {{{CLLMs}}},
  author = {Kou, Siqi and Hu, Lanxiang and He, Zhezhi and Deng, Zhijie and Zhang, Hao},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-05-11},
  abstract = {Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4\${\textbackslash}times\$ to 3.4\${\textbackslash}times\$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.},
  howpublished = {https://arxiv.org/abs/2403.00835v3},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/WXFURS97/Kou et al. - 2024 - CLLMs Consistency Large Language Models.pdf}
}

@misc{deepseek-ai_2024,
  title = {{{DeepSeek-V2}}: {{A Strong}}, {{Economical}}, and {{Efficient Mixture-of-Experts Language Model}}},
  shorttitle = {{{DeepSeek-V2}}},
  author = {{DeepSeek-AI}},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-12},
  abstract = {We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5\% of training costs, reduces the KV cache by 93.3\%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.},
  howpublished = {https://arxiv.org/abs/2405.04434v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-12]},
  file = {/Users/chenghao/Zotero/storage/ECZC8WBB/DeepSeek-AI - 2024 - DeepSeek-V2 A Strong, Economical, and Efficient Mixture-of-Experts Language Model.pdf}
}

@misc{buehler_2024,
  title = {Accelerating {{Scientific Discovery}} with {{Generative Knowledge Extraction}}, {{Graph-Based Representation}}, and {{Multimodal Intelligent Graph Reasoning}}},
  author = {Buehler, Markus J.},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-05-12},
  abstract = {Leveraging generative Artificial Intelligence (AI), we have transformed a dataset comprising 1,000 scientific papers into an ontological knowledge graph. Through an in-depth structural analysis, we have calculated node degrees, identified communities and connectivities, and evaluated clustering coefficients and betweenness centrality of pivotal nodes, uncovering fascinating knowledge architectures. The graph has an inherently scale-free nature, is highly connected, and can be used for graph reasoning by taking advantage of transitive and isomorphic properties that reveal unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, propose never-before-seen material designs, and predict material behaviors. We compute deep node embeddings for combinatorial node similarity ranking for use in a path sampling strategy links dissimilar concepts that have previously not been related. One comparison revealed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. In another example, the algorithm proposed a hierarchical mycelium-based composite based on integrating path sampling with principles extracted from Kandinsky's 'Composition VII' painting. The resulting material integrates an innovative set of concepts that include a balance of chaos/order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across science, technology and art, revealing a nuanced ontology of immanence that reveal a context-dependent heterarchical interplay of constituents. Graph-based generative AI achieves a far higher degree of novelty, explorative capacity, and technical detail, than conventional approaches and establishes a widely useful framework for innovation by revealing hidden connections.},
  howpublished = {https://arxiv.org/abs/2403.11996v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-12]},
  file = {/Users/chenghao/Zotero/storage/8GMJK58V/Buehler - 2024 - Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation,.pdf}
}

@misc{ferrando_2024,
  title = {A {{Primer}} on the {{Inner Workings}} of {{Transformer-based Language Models}}},
  author = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and {Costa-juss{\`a}}, Marta R.},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-13},
  abstract = {The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.},
  howpublished = {https://arxiv.org/abs/2405.00208v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-13]},
  file = {/Users/chenghao/Zotero/storage/ZZCQ63VP/Ferrando et al. - 2024 - A Primer on the Inner Workings of Transformer-based Language Models.pdf}
}

@misc{ni_2024,
  title = {{{NExT}}: {{Teaching Large Language Models}} to {{Reason}} about {{Code Execution}}},
  shorttitle = {{{NExT}}},
  author = {Ni, Ansong and Allamanis, Miltiadis and Cohan, Arman and Deng, Yinlin and Shi, Kensen and Sutton, Charles and Yin, Pengcheng},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-05-13},
  abstract = {A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka. rubber duck debugging). However, large language models (LLMs) of code are typically trained on the surface textual form of programs, thus may lack a semantic understanding of how programs execute at run-time. To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses self-training to bootstrap a synthetic training set of execution-aware rationales that lead to correct task solutions (e.g., fixed programs) without laborious manual annotation. Experiments on program repair tasks based on MBPP and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by 26.1\% and 14.3\% absolute, respectively, with significantly improved rationale quality as verified by automated metrics and human raters. Our model can also generalize to scenarios where program traces are absent at test-time.},
  howpublished = {https://arxiv.org/abs/2404.14662v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-13]},
  file = {/Users/chenghao/Zotero/storage/VNQ4R93Q/Ni et al. - 2024 - NExT Teaching Large Language Models to Reason about Code Execution.pdf}
}

@misc{minixhofer_2024,
  title = {Zero-{{Shot Tokenizer Transfer}}},
  author = {Minixhofer, Benjamin and Ponti, Edoardo Maria and Vuli{\'c}, Ivan},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-14},
  abstract = {Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.},
  howpublished = {https://arxiv.org/abs/2405.07883v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/Y4K7MZFH/Minixhofer et al. - 2024 - Zero-Shot Tokenizer Transfer.pdf}
}

@misc{mccoy_2023,
  title = {Embers of {{Autoregression}}: {{Understanding Large Language Models Through}} the {{Problem They}} Are {{Trained}} to {{Solve}}},
  shorttitle = {Embers of {{Autoregression}}},
  author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L.},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-05-14},
  abstract = {The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low - even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4's accuracy at decoding a simple cipher is 51\% when the output is a high-probability word sequence but only 13\% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system - one that has been shaped by its own particular set of pressures.},
  howpublished = {https://arxiv.org/abs/2309.13638v1},
  langid = {english},
  annotation = {49 citations (Semantic Scholar/arXiv) [2024-05-14]},
  file = {/Users/chenghao/Zotero/storage/WZ52JXCG/McCoy et al. - 2023 - Embers of Autoregression Understanding Large Language Models Through the Problem They are Trained t.pdf}
}

@article{dingemanse_2024,
  title = {Generative {{AI}} and {{Research Integrity}}},
  author = {Dingemanse, Mark},
  year = {2024},
  month = may,
  publisher = {OSF},
  doi = {10.31219/osf.io/2c48n},
  urldate = {2024-05-15},
  abstract = {GPT based text generators like ChatGPT or Microsoft Copilot have rapidly become a "cultural sensation". This document provides scientific background  and guidance on how to think critically and mindfully about these tools in academic writing and research.},
  langid = {american},
  file = {/Users/chenghao/Zotero/storage/9D99P7QI/Dingemanse - 2024 - Generative AI and Research Integrity.pdf}
}

@misc{ge_2023,
  title = {Model {{Tells You What}} to {{Discard}}: {{Adaptive KV Cache Compression}} for {{LLMs}}},
  shorttitle = {Model {{Tells You What}} to {{Discard}}},
  author = {Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  urldate = {2024-05-15},
  abstract = {In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.},
  howpublished = {https://arxiv.org/abs/2310.01801v3},
  langid = {english},
  annotation = {20 citations (Semantic Scholar/arXiv) [2024-05-15]},
  file = {/Users/chenghao/Zotero/storage/SRGRG488/Ge et al. - 2023 - Model Tells You What to Discard Adaptive KV Cache Compression for LLMs.pdf}
}

@misc{lin_2024b,
  title = {Towards {{Accurate}} and {{Efficient Document Analytics}} with {{Large Language Models}}},
  author = {Lin, Yiming and Hulsebos, Madelon and Ma, Ruiying and Shankar, Shreya and Zeigham, Sepanta and Parameswaran, Aditya G. and Wu, Eugene},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-15},
  abstract = {Unstructured data formats account for over 80\% of the data currently stored, and extracting value from such formats remains a considerable challenge. In particular, current approaches for managing unstructured documents do not support ad-hoc analytical queries on document collections. Moreover, Large Language Models (LLMs) directly applied to the documents themselves, or on portions of documents through a process of Retrieval-Augmented Generation (RAG), fail to provide high accuracy query results, and in the LLM-only case, additionally incur high costs. Since many unstructured documents in a collection often follow similar templates that impart a common semantic structure, we introduce ZenDB, a document analytics system that leverages this semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document collections. ZenDB efficiently extracts semantic hierarchical structures from such templatized documents, and introduces a novel query engine that leverages these structures for accurate and cost-effective query execution. Users can impose a schema on their documents, and query it, all via SQL. Extensive experiments on three real-world document collections demonstrate ZenDB's benefits, achieving up to 30\% cost savings compared to LLM-based baselines, while maintaining or improving accuracy, and surpassing RAG-based baselines by up to 61\% in precision and 80\% in recall, at a marginally higher cost.},
  howpublished = {https://arxiv.org/abs/2405.04674v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-15]},
  file = {/Users/chenghao/Zotero/storage/4JX2FFQS/Lin et al. - 2024 - Towards Accurate and Efficient Document Analytics with Large Language Models.pdf}
}

@misc{biderman_2024,
  title = {{{LoRA Learns Less}} and {{Forgets Less}}},
  author = {Biderman, Dan and Ortiz, Jose Gonzalez and Portes, Jacob and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and Blakeney, Cody and Cunningham, John P.},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-17},
  abstract = {Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (\${\textbackslash}approx\$100K prompt-response pairs) and continued pretraining (\${\textbackslash}approx\$10B unstructured tokens) data regimes. Our results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model's performance on tasks outside the target domain. We show that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. We show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.},
  howpublished = {https://arxiv.org/abs/2405.09673v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/Q3SA3TT3/Biderman et al. - 2024 - LoRA Learns Less and Forgets Less.pdf}
}

@misc{team_2024,
  title = {Chameleon: {{Mixed-Modal Early-Fusion Foundation Models}}},
  shorttitle = {Chameleon},
  author = {Team, Chameleon},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-17},
  abstract = {We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.},
  howpublished = {https://arxiv.org/abs/2405.09818v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/TEE3WAP7/Team - 2024 - Chameleon Mixed-Modal Early-Fusion Foundation Models.pdf}
}

@misc{dong_2024,
  title = {{{RLHF Workflow}}: {{From Reward Modeling}} to {{Online RLHF}}},
  shorttitle = {{{RLHF Workflow}}},
  author = {Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-17},
  abstract = {We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information.},
  howpublished = {https://arxiv.org/abs/2405.07863v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/4J8TSARX/Dong et al. - 2024 - RLHF Workflow From Reward Modeling to Online RLHF.pdf}
}

@misc{ivanova_2024,
  title = {Elements of {{World Knowledge}} ({{EWOK}}): {{A}} Cognition-Inspired Framework for Evaluating Basic World Knowledge in Language Models},
  shorttitle = {Elements of {{World Knowledge}} ({{EWOK}})},
  author = {Ivanova, Anna A. and Sathe, Aalok and Lipkin, Benjamin and Kumar, Unnathi and Radkani, Setayesh and Clark, Thomas H. and Kauf, Carina and Hu, Jennifer and Pramod, R. T. and Grand, Gabriel and Paulun, Vivian and Ryskina, Maria and Akyurek, Ekin and Wilcox, Ethan and Rashid, Nafisa and Choshen, Leshem and Levy, Roger and Fedorenko, Evelina and Tenenbaum, Joshua and Andreas, Jacob},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-17},
  abstract = {The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities is hard, in part because the building blocks of world models are ill-defined. We present Elements of World Knowledge (EWOK), a framework for evaluating world modeling in language models by testing their ability to use knowledge of a concept to match a target text with a plausible/implausible context. EWOK targets specific concepts from multiple knowledge domains known to be vital for world modeling in humans. Domains range from social interactions (help/hinder) to spatial relations (left/right). Both, contexts and targets are minimal pairs. Objects, agents, and locations in the items can be flexibly filled in enabling easy generation of multiple controlled datasets. We then introduce EWOK-CORE-1.0, a dataset of 4,374 items covering 11 world knowledge domains. We evaluate 20 openweights large language models (1.3B--70B parameters) across a battery of evaluation paradigms along with a human norming study comprising 12,480 measurements. The overall performance of all tested models is worse than human performance, with results varying drastically across domains. These data highlight simple cases where even large models fail and present rich avenues for targeted research on LLM world modeling capabilities.},
  howpublished = {https://arxiv.org/abs/2405.09605v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-17]},
  file = {/Users/chenghao/Zotero/storage/3YP9DIDE/Ivanova et al. - 2024 - Elements of World Knowledge (EWOK) A cognition-inspired framework for evaluating basic world knowle.pdf}
}

@misc{_k,
  title = {Philosophy of Cognitive Science in the Age of Deep Learning},
  doi = {10.1002/wcs.1684},
  urldate = {2024-05-24},
  howpublished = {https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/wcs.1684},
  langid = {english}
}

@misc{lampinen_2024,
  title = {Learned Feature Representations Are Biased by Complexity, Learning Order, Position, and More},
  author = {Lampinen, Andrew Kyle and Chan, Stephanie C. Y. and Hermann, Katherine},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-24},
  abstract = {Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience. Both fields generally use representations as a means to understand or improve a system's computations. In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts. We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data. We train various deep learning architectures to compute these multiple abstract features about their inputs. We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs. For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well. We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly). Our results help to characterize the inductive biases of gradient-based representation learning. These results also highlight a key challenge for interpretability \$-\$ or for comparing the representations of models and brains \$-\$ disentangling extraneous biases from the computationally important aspects of a system's internal representations.},
  howpublished = {https://arxiv.org/abs/2405.05847v1},
  langid = {english}
}

@article{kim_2024a,
  title = {Financial {{Statement Analysis}} with {{Large Language Models}}},
  author = {Kim, Alex and Muhn, Maximilian and Nikolaev, Valeri V.},
  year = {2024},
  month = may,
  doi = {10.2139/ssrn.4835311},
  urldate = {2024-05-25},
  abstract = {We investigate whether an LLM can successfully perform financial statement analysis in a way similar to a professional human analyst. We provide standardized an},
  langid = {english}
}

@misc{jiang_2024a,
  title = {{{MoRA}}: {{High-Rank Updating}} for {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{MoRA}}},
  author = {Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-29},
  abstract = {Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.},
  howpublished = {https://arxiv.org/abs/2405.12130v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-29]},
  file = {/Users/chenghao/Zotero/storage/BKGIYAEX/Jiang et al. - 2024 - MoRA High-Rank Updating for Parameter-Efficient Fine-Tuning.pdf}
}

@misc{mcleish_2024,
  title = {Transformers {{Can Do Arithmetic}} with the {{Right Embeddings}}},
  author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-29},
  abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99\% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.},
  howpublished = {https://arxiv.org/abs/2405.17399v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-29]},
  file = {/Users/chenghao/Zotero/storage/VNDCT68L/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embeddings.pdf}
}

@misc{gutierrez_2024,
  title = {{{HippoRAG}}: {{Neurobiologically Inspired Long-Term Memory}} for {{Large Language Models}}},
  shorttitle = {{{HippoRAG}}},
  author = {Guti{\'e}rrez, Bernal Jim{\'e}nez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-29},
  abstract = {In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20\%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.},
  howpublished = {https://arxiv.org/abs/2405.14831v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-29]},
  file = {/Users/chenghao/Zotero/storage/59LQESUW/Gutirrez et al. - 2024 - HippoRAG Neurobiologically Inspired Long-Term Memory for Large Language Models.pdf}
}

@misc{wang_2024b,
  title = {Grokked {{Transformers}} Are {{Implicit Reasoners}}: {{A Mechanistic Journey}} to the {{Edge}} of {{Generalization}}},
  shorttitle = {Grokked {{Transformers}} Are {{Implicit Reasoners}}},
  author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-29},
  abstract = {We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.},
  howpublished = {https://arxiv.org/abs/2405.15071v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-29]},
  file = {/Users/chenghao/Zotero/storage/5WIS4TEH/Wang et al. - 2024 - Grokked Transformers are Implicit Reasoners A Mechanistic Journey to the Edge of Generalization.pdf}
}

@misc{hosseini_2024,
  title = {You {{Need}} to {{Pay Better Attention}}},
  author = {Hosseini, Mehran and Hosseini, Peyman},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-05-30},
  abstract = {We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and Amazon Reviews datasets.},
  howpublished = {https://arxiv.org/abs/2403.01643v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-05-30]},
  file = {/Users/chenghao/Zotero/storage/44ZMHPFQ/Hosseini and Hosseini - 2024 - You Need to Pay Better Attention.pdf}
}

@misc{rae_2024,
  title = {{{2BP}}: 2-{{Stage Backpropagation}}},
  shorttitle = {{{2BP}}},
  author = {Rae, Christopher and Lee, Joseph K. L. and Richings, James},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-05-31},
  abstract = {As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed the memory capacity of a single accelerator, necessitating the sharding of model parameters across multiple accelerators. Pipeline parallelism is a commonly used sharding strategy for training large DNNs. However, current implementations of pipeline parallelism are being unintentionally bottlenecked by the automatic differentiation tools provided by ML frameworks. This paper introduces 2-stage backpropagation (2BP). By splitting the backward propagation step into two separate stages, we can reduce idle compute time. We tested 2BP on various model architectures and pipelining schedules, achieving increases in throughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in throughput compared to traditional methods when training a LLaMa-like transformer with 7 billion parameters across 4 GPUs.},
  howpublished = {https://arxiv.org/abs/2405.18047v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/FQPWW72Q/Rae et al. - 2024 - 2BP 2-Stage Backpropagation.pdf}
}

@misc{cai_2024,
  title = {Matryoshka {{Multimodal Models}}},
  author = {Cai, Mu and Yang, Jianwei and Gao, Jianfeng and Lee, Yong Jae},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-06-01},
  abstract = {Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around {\textasciitilde}9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.},
  howpublished = {https://arxiv.org/abs/2405.17430v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/JVIZL7ML/Cai et al. - 2024 - Matryoshka Multimodal Models.pdf}
}

@misc{glorioso_2024,
  title = {Zamba: {{A Compact 7B SSM Hybrid Model}}},
  shorttitle = {Zamba},
  author = {Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-06-01},
  abstract = {In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. We open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.},
  howpublished = {https://arxiv.org/abs/2405.16712v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-01]},
  file = {/Users/chenghao/Zotero/storage/5EC4ZPKS/Glorioso et al. - 2024 - Zamba A Compact 7B SSM Hybrid Model.pdf}
}

@misc{dao_2024,
  title = {Transformers Are {{SSMs}}: {{Generalized Models}} and {{Efficient Algorithms Through Structured State Space Duality}}},
  shorttitle = {Transformers Are {{SSMs}}},
  author = {Dao, Tri and Gu, Albert},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-06-03},
  abstract = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
  howpublished = {https://arxiv.org/abs/2405.21060v1},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/arXiv) [2024-06-03]},
  file = {/Users/chenghao/Zotero/storage/AUBZYUG9/Dao and Gu - 2024 - Transformers are SSMs Generalized Models and Efficient Algorithms Through Structured State Space Du.pdf}
}

@misc{_e,
  title = {Scaling {{Monosemanticity}}: {{Extracting Interpretable Features}} from {{Claude}} 3 {{Sonnet}}},
  urldate = {2024-06-03},
  howpublished = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@misc{nezhurina_2024,
  title = {Alice in {{Wonderland}}: {{Simple Tasks Showing Complete Reasoning Breakdown}} in {{State-Of-the-Art Large Language Models}}},
  shorttitle = {Alice in {{Wonderland}}},
  author = {Nezhurina, Marianna and {Cipolina-Kun}, Lucia and Cherti, Mehdi and Jitsev, Jenia},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-05},
  abstract = {Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical "reasoning"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW},
  howpublished = {https://arxiv.org/abs/2406.02061v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-05]},
  file = {/Users/chenghao/Zotero/storage/IVVUTQA6/Nezhurina et al. - 2024 - Alice in Wonderland Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Lan.pdf}
}

@misc{tokpanov_2024,
  title = {Zyda: {{A}} 1.{{3T Dataset}} for {{Open Language Modeling}}},
  shorttitle = {Zyda},
  author = {Tokpanov, Yury and Millidge, Beren and Glorioso, Paolo and Pilault, Jonathan and Ibrahim, Adam and Whittington, James and Anthony, Quentin},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-07},
  abstract = {The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.},
  howpublished = {https://arxiv.org/abs/2406.01981v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/RCSW3VQL/Tokpanov et al. - 2024 - Zyda A 1.3T Dataset for Open Language Modeling.pdf}
}

@misc{shaikh_2024,
  title = {Show, {{Don}}'t {{Tell}}: {{Aligning Language Models}} with {{Demonstrated Feedback}}},
  shorttitle = {Show, {{Don}}'t {{Tell}}},
  author = {Shaikh, Omar and Lam, Michelle and Hejna, Joey and Shao, Yijia and Bernstein, Michael and Yang, Diyi},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-08},
  abstract = {Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (\${$<$}10\$) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (\$N=16\$). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19\% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.},
  howpublished = {https://arxiv.org/abs/2406.00888v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/NW6FKAC5/Shaikh et al. - 2024 - Show, Don't Tell Aligning Language Models with Demonstrated Feedback.pdf}
}

@misc{bhargava_2023,
  title = {What's the {{Magic Word}}? {{A Control Theory}} of {{LLM Prompting}}},
  shorttitle = {What's the {{Magic Word}}?},
  author = {Bhargava, Aman and Witkowski, Cameron and Shah, Manav and Thomson, Matt},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  urldate = {2024-06-08},
  abstract = {Prompt engineering is crucial for deploying LLMs but is poorly understood mathematically. We formalize LLM systems as a class of discrete stochastic dynamical systems to explore prompt engineering through the lens of control theory. We investigate the reachable set of output token sequences \$R\_y({\textbackslash}mathbf x\_0)\$ for which there exists a control input sequence \${\textbackslash}mathbf u\$ for each \${\textbackslash}mathbf y {\textbackslash}in R\_y({\textbackslash}mathbf x\_0)\$ that steers the LLM to output \${\textbackslash}mathbf y\$ from initial state sequence \${\textbackslash}mathbf x\_0\$. We offer analytic analysis on the limitations on the controllability of self-attention in terms of reachable set, where we prove an upper bound on the reachable set of outputs \$R\_y({\textbackslash}mathbf x\_0)\$ as a function of the singular values of the parameter matrices. We present complementary empirical analysis on the controllability of a panel of LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a lower bound on the reachable set of outputs \$R\_y({\textbackslash}mathbf x\_0)\$ w.r.t. initial state sequences \${\textbackslash}mathbf x\_0\$ sampled from the Wikitext dataset. We find that the correct next Wikitext token following sequence \${\textbackslash}mathbf x\_0\$ is reachable over 97\% of the time with prompts of \$k{\textbackslash}leq 10\$ tokens. We also establish that the top 75 most likely next tokens, as estimated by the LLM itself, are reachable at least 85\% of the time with prompts of \$k{\textbackslash}leq 10\$ tokens. Intriguingly, short prompt sequences can dramatically alter the likelihood of specific outputs, even making the least likely tokens become the most likely ones. This control-centric analysis of LLMs demonstrates the significant and poorly understood role of input sequences in steering output probabilities, offering a foundational perspective for enhancing language model system capabilities.},
  howpublished = {https://arxiv.org/abs/2310.04444v3},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/JGUBPFAM/Bhargava et al. - 2023 - What's the Magic Word A Control Theory of LLM Prompting.pdf}
}

@misc{magesh_2024,
  title = {Hallucination-{{Free}}? {{Assessing}} the {{Reliability}} of {{Leading AI Legal Research Tools}}},
  shorttitle = {Hallucination-{{Free}}?},
  author = {Magesh, Varun and Surani, Faiz and Dahl, Matthew and Suzgun, Mirac and Manning, Christopher D. and Ho, Daniel E.},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-06-14},
  abstract = {Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to "hallucinate," or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as "eliminating" (Casetext, 2023) or "avoid[ing]" hallucinations (Thomson Reuters, 2023), or guaranteeing "hallucination-free" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17\% and 33\% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.},
  howpublished = {https://arxiv.org/abs/2405.20362v1},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/arXiv) [2024-06-14]},
  file = {/Users/chenghao/Zotero/storage/KZKDJ2XU/Magesh et al. - 2024 - Hallucination-Free Assessing the Reliability of Leading AI Legal Research Tools.pdf}
}

@misc{simsa_2023,
  title = {{{DocILE Benchmark}} for {{Document Information Localization}} and {{Extraction}}},
  author = {{\v S}imsa, {\v S}t{\v e}p{\'a}n and {\v S}ulc, Milan and U{\v r}i{\v c}{\'a}{\v r}, Michal and Patel, Yash and Hamdi, Ahmed and Koci{\'a}n, Mat{\v e}j and Skalick{\'y}, Maty{\'a}{\v s} and Matas, Ji{\v r}{\'i} and Doucet, Antoine and Coustaty, Micka{\"e}l and Karatzas, Dimosthenis},
  year = {2023},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-06-16},
  abstract = {This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly{\textasciitilde}1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domain- and task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; applied to both tasks of the DocILE benchmark, with results shared in this paper, offering a quick starting point for future work. The dataset, baselines and supplementary material are available at https://github.com/rossumai/docile.},
  howpublished = {https://arxiv.org/abs/2302.05658v2},
  langid = {english},
  annotation = {14 citations (Semantic Scholar/arXiv) [2024-06-16]},
  file = {/Users/chenghao/Zotero/storage/CK766C4J/imsa et al. - 2023 - DocILE Benchmark for Document Information Localization and Extraction.pdf}
}

@misc{maini_2024,
  title = {{{LLM Dataset Inference}}: {{Did}} You Train on My Dataset?},
  shorttitle = {{{LLM Dataset Inference}}},
  author = {Maini, Pratyush and Jia, Hengrui and Papernot, Nicolas and Dziedzic, Adam},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-18},
  abstract = {The proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs). We demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful. However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time). Even when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions. Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models. This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph. While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values {$<$} 0.1, without any false positives.},
  howpublished = {https://arxiv.org/abs/2406.06443v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-18]},
  file = {/Users/chenghao/Zotero/storage/EEINZD5D/Maini et al. - 2024 - LLM Dataset Inference Did you train on my dataset.pdf}
}

@misc{schulhoff_2024,
  title = {The {{Prompt Report}}: {{A Systematic Survey}} of {{Prompting Techniques}}},
  shorttitle = {The {{Prompt Report}}},
  author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Da Costa, Hevander and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-18},
  abstract = {Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area's nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.},
  howpublished = {https://arxiv.org/abs/2406.06608v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-18]},
  file = {/Users/chenghao/Zotero/storage/MQLX4AH9/Schulhoff et al. - 2024 - The Prompt Report A Systematic Survey of Prompting Techniques.pdf}
}

@misc{_n,
  title = {Ethics and {{Information Technology}}},
  journal = {SpringerLink},
  urldate = {2024-06-18},
  abstract = {Ethics and Information Technology is a peer-reviewed journal dedicated to advancing the dialogue between moral philosophy and the field of information and ...},
  howpublished = {https://link.springer.com/journal/10676},
  langid = {english}
}

@article{hicks_2024,
  title = {{{ChatGPT}} Is Bullshit},
  author = {Hicks, Michael Townsen and Humphries, James and Slater, Joe},
  year = {2024},
  month = jun,
  journal = {Ethics and Information Technology},
  volume = {26},
  number = {2},
  pages = {1--10},
  publisher = {Springer Netherlands},
  issn = {1572-8439},
  doi = {10.1007/s10676-024-09775-5},
  urldate = {2024-06-18},
  abstract = {Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called ``AI hallucinations''. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.},
  copyright = {2024 The Author(s)},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2024-06-18]},
  file = {/Users/chenghao/Zotero/storage/GLMVV9IF/Hicks et al. - 2024 - ChatGPT is bullshit.pdf}
}

@misc{zhu_2024,
  title = {Scalable {{MatMul-free Language Modeling}}},
  author = {Zhu, Rui-Jie and Zhang, Yu and Sifferman, Ethan and Sheaves, Tyler and Wang, Yiqiao and Richmond, Dustin and Zhou, Peng and Eshraghian, Jason K.},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-18},
  abstract = {Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61\% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.},
  howpublished = {https://arxiv.org/abs/2406.02528v4},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/KPP5SSNK/Zhu et al. - 2024 - Scalable MatMul-free Language Modeling.pdf}
}

@misc{yang_2024,
  title = {{{CRAG}} -- {{Comprehensive RAG Benchmark}}},
  author = {Yang, Xiao and Sun, Kai and Xin, Hao and Sun, Yushi and Bhalla, Nikita and Chen, Xiangsen and Choudhary, Sajal and Gui, Rongze Daniel and Jiang, Ziran Will and Jiang, Ziyu and Kong, Lingkun and Moran, Brian and Wang, Jiaqi and Xu, Yifan Ethan and Yan, An and Yang, Chenyu and Yuan, Eting and Zha, Hanwen and Tang, Nan and Chen, Lei and Scheffer, Nicolas and Liu, Yue and Shah, Nirav and Wanga, Rakesh and Kumar, Anuj and Yih, Wen-tau and Dong, Xin Luna},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-18},
  abstract = {Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve {$<$}=34\% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44\%. State-of-the-art industry RAG solutions only answer 63\% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.},
  howpublished = {https://arxiv.org/abs/2406.04744v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-18]},
  file = {/Users/chenghao/Zotero/storage/Q3WAIBWQ/Yang et al. - 2024 - CRAG -- Comprehensive RAG Benchmark.pdf}
}

@article{solove_,
  title = {``{{I}}'{{VE GOT NOTHING TO HIDE}},'' {{AND OTHER MISUNDERSTANDINGS OF PRIVACY}}},
  author = {Solove, Daniel J},
  volume = {44},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/HNFQAQC2/Solove - IVE GOT NOTHING TO HIDE, AND OTHER MISUNDERSTANDINGS OF PRIVACY.pdf}
}

@misc{wang_2024c,
  title = {Multimodal {{Needle}} in a {{Haystack}}: {{Benchmarking Long-Context Capability}} of {{Multimodal Large Language Models}}},
  shorttitle = {Multimodal {{Needle}} in a {{Haystack}}},
  author = {Wang, Hengyi and Shi, Haizhou and Tan, Shiwei and Qin, Weiyi and Wang, Wenyuan and Zhang, Tunyu and Nambi, Akshay and Ganu, Tanuja and Wang, Hao},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-21},
  abstract = {Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images (haystack) based on textual instructions and descriptions of image contents. This setup necessitates an advanced understanding of extensive visual contexts and effective information retrieval within long-context image inputs. With this benchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and open-source models. The findings reveal that GPT-4o consistently surpasses other models in long-context scenarios, but suffers from hallucination problems in negative samples, i.e., when needles are not in the haystacks. Our comprehensive long-context evaluation of MLLMs also sheds lights on the considerable performance gap between API-based and open-source models. All the code, data, and instructions required to reproduce the main results are available at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.},
  howpublished = {https://arxiv.org/abs/2406.11230v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-21]},
  file = {/Users/chenghao/Zotero/storage/SNQ8SRER/Wang et al. - 2024 - Multimodal Needle in a Haystack Benchmarking Long-Context Capability of Multimodal Large Language M.pdf}
}

@misc{deepseek-ai_2024a,
  title = {{{DeepSeek-Coder-V2}}: {{Breaking}} the {{Barrier}} of {{Closed-Source Models}} in {{Code Intelligence}}},
  shorttitle = {{{DeepSeek-Coder-V2}}},
  author = {{DeepSeek-AI} and Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y. and Li, Yukun and Gao, Huazuo and Ma, Shirong and Zeng, Wangding and Bi, Xiao and Gu, Zihui and Xu, Hanwei and Dai, Damai and Dong, Kai and Zhang, Liyue and Piao, Yishi and Gou, Zhibin and Xie, Zhenda and Hao, Zhewen and Wang, Bingxuan and Song, Junxiao and Chen, Deli and Xie, Xin and Guan, Kang and You, Yuxiang and Liu, Aixin and Du, Qiushi and Gao, Wenjun and Lu, Xuan and Chen, Qinyu and Wang, Yaohui and Deng, Chengqi and Li, Jiashi and Zhao, Chenggang and Ruan, Chong and Luo, Fuli and Liang, Wenfeng},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-21},
  abstract = {We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.},
  howpublished = {https://arxiv.org/abs/2406.11931v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/BQM76Q6V/DeepSeek-AI et al. - 2024 - DeepSeek-Coder-V2 Breaking the Barrier of Closed-Source Models in Code Intelligence.pdf}
}

@misc{glm_2024,
  title = {{{ChatGLM}}: {{A Family}} of {{Large Language Models}} from {{GLM-130B}} to {{GLM-4 All Tools}}},
  shorttitle = {{{ChatGLM}}},
  author = {Glm, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and Lai, Hanyu and Yu, Hao and Wang, Hongning and Sun, Jiadai and Zhang, Jiajie and Cheng, Jiale and Gui, Jiayi and Tang, Jie and Zhang, Jing and Li, Juanzi and Zhao, Lei and Wu, Lindong and Zhong, Lucen and Liu, Mingdao and Huang, Minlie and Zhang, Peng and Zheng, Qinkai and Lu, Rui and Duan, Shuaiqi and Zhang, Shudan and Cao, Shulin and Yang, Shuxun and Tam, Weng Lam and Zhao, Wenyi and Liu, Xiao and Xia, Xiao and Zhang, Xiaohan and Gu, Xiaotao and Lv, Xin and Liu, Xinghan and Liu, Xinyi and Yang, Xinyue and Song, Xixuan and Zhang, Xunkai and An, Yifan and Xu, Yifan and Niu, Yilin and Yang, Yuantao and Li, Yueyan and Bai, Yushi and Dong, Yuxiao and Qi, Zehan and Wang, Zhaoyu and Yang, Zhen and Du, Zhengxiao and Hou, Zhenyu and Wang, Zihan},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-21},
  abstract = {We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/THUDM and https://huggingface.co/THUDM.},
  howpublished = {https://arxiv.org/abs/2406.12793v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/5JS9VSE8/Glm et al. - 2024 - ChatGLM A Family of Large Language Models from GLM-130B to GLM-4 All Tools.pdf}
}

@misc{li_2024a,
  title = {{{DataComp-LM}}: {{In}} Search of the next Generation of Training Sets for Language Models},
  shorttitle = {{{DataComp-LM}}},
  author = {Li, Jeffrey and Fang, Alex and Smyrnis, Georgios and Ivgi, Maor and Jordan, Matt and Gadre, Samir and Bansal, Hritik and Guha, Etash and Keh, Sedrick and Arora, Kushal and Garg, Saurabh and Xin, Rui and Muennighoff, Niklas and Heckel, Reinhard and Mercat, Jean and Chen, Mayee and Gururangan, Suchin and Wortsman, Mitchell and Albalak, Alon and Bitton, Yonatan and Nezhurina, Marianna and Abbas, Amro and Hsieh, Cheng-Yu and Ghosh, Dhruba and Gardner, Josh and Kilian, Maciej and Zhang, Hanlin and Shao, Rulin and Pratt, Sarah and Sanyal, Sunny and Ilharco, Gabriel and Daras, Giannis and Marathe, Kalyani and Gokaslan, Aaron and Zhang, Jieyu and Chandu, Khyathi and Nguyen, Thao and Vasiljevic, Igor and Kakade, Sham and Song, Shuran and Sanghavi, Sujay and Faghri, Fartash and Oh, Sewoong and Zettlemoyer, Luke and Lo, Kyle and {El-Nouby}, Alaaeldin and Pouransari, Hadi and Toshev, Alexander and Wang, Stephanie and Groeneveld, Dirk and Soldaini, Luca and Koh, Pang Wei and Jitsev, Jenia and Kollar, Thomas and Dimakis, Alexandros G. and Carmon, Yair and Dave, Achal and Schmidt, Ludwig and Shankar, Vaishaal},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-21},
  abstract = {We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64\% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40\% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63\% \& 66\%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.},
  howpublished = {https://arxiv.org/abs/2406.11794v2},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-21]},
  file = {/Users/chenghao/Zotero/storage/7AMJM9WX/Li et al. - 2024 - DataComp-LM In search of the next generation of training sets for language models.pdf}
}

@misc{wei_2024a,
  title = {Skywork-{{MoE}}: {{A Deep Dive}} into {{Training Techniques}} for {{Mixture-of-Experts Language Models}}},
  shorttitle = {Skywork-{{MoE}}},
  author = {Wei, Tianwen and Zhu, Bo and Zhao, Liang and Cheng, Cheng and Li, Biye and L{\"u}, Weiwei and Cheng, Peng and Zhang, Jianhao and Zhang, Xiaoyu and Zeng, Liang and Wang, Xiaokun and Ma, Yutuan and Hu, Rui and Yan, Shuicheng and Fang, Han and Zhou, Yahui},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-21},
  abstract = {In this technical report, we introduce the training methodologies implemented in the development of Skywork-MoE, a high-performance mixture-of-experts (MoE) large language model (LLM) with 146 billion parameters and 16 experts. It is initialized from the pre-existing dense checkpoints of our Skywork-13B model. We explore the comparative effectiveness of upcycling versus training from scratch initializations. Our findings suggest that the choice between these two approaches should consider both the performance of the existing dense checkpoints and the MoE training budget. We highlight two innovative techniques: gating logit normalization, which improves expert diversification, and adaptive auxiliary loss coefficients, allowing for layer-specific adjustment of auxiliary loss coefficients. Our experimental results validate the effectiveness of these methods. Leveraging these techniques and insights, we trained our upcycled Skywork-MoE on a condensed subset of our SkyPile corpus. The evaluation results demonstrate that our model delivers strong performance across a wide range of benchmarks.},
  howpublished = {https://arxiv.org/abs/2406.06563v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/K64HMV7G/Wei et al. - 2024 - Skywork-MoE A Deep Dive into Training Techniques for Mixture-of-Experts Language Models.pdf}
}

@misc{yuksekgonul_2024,
  title = {{{TextGrad}}: {{Automatic}} "{{Differentiation}}" via {{Text}}},
  shorttitle = {{{TextGrad}}},
  author = {Yuksekgonul, Mert and Bianchi, Federico and Boen, Joseph and Liu, Sheng and Huang, Zhi and Guestrin, Carlos and Zou, James},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-21},
  abstract = {AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges. Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from \$51{\textbackslash}\%\$ to \$55{\textbackslash}\%\$, yields \$20{\textbackslash}\%\$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.},
  howpublished = {https://arxiv.org/abs/2406.07496v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/R3QNYANQ/Yuksekgonul et al. - 2024 - TextGrad Automatic Differentiation via Text.pdf}
}

@misc{stepanov_2024,
  title = {{{GLiNER}} Multi-Task: {{Generalist Lightweight Model}} for {{Various Information Extraction Tasks}}},
  shorttitle = {{{GLiNER}} Multi-Task},
  author = {Stepanov, Ihor and Shtopko, Mykhailo},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-22},
  abstract = {Information extraction tasks require both accurate, efficient, and generalisable models. Classical supervised deep learning approaches can achieve the required performance, but they need large datasets and are limited in their ability to adapt to different tasks. On the other hand, large language models (LLMs) demonstrate good generalization, meaning that they can adapt to many different tasks based on user requests. However, LLMs are computationally expensive and tend to fail to generate structured outputs. In this article, we will introduce a new kind of GLiNER model that can be used for various information extraction tasks while being a small encoder model. Our model achieved SoTA performance on zero-shot NER benchmarks and leading performance on question-answering, summarization and relation extraction tasks. Additionally, in this article, we will cover experimental results on self-learning approaches for named entity recognition using GLiNER models.},
  howpublished = {https://arxiv.org/abs/2406.12925v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-22]},
  file = {/Users/chenghao/Zotero/storage/TCTYB77F/Stepanov and Shtopko - 2024 - GLiNER multi-task Generalist Lightweight Model for Various Information Extraction Tasks.pdf}
}

@misc{_m,
  title = {Introducing {{Idefics2}}: {{A Powerful 8B Vision-Language Model}} for the Community},
  urldate = {2024-06-22},
  howpublished = {https://huggingface.co/blog/idefics2},
  file = {/Users/chenghao/Zotero/storage/RCGH58TB/idefics2.html}
}

@article{fedorenko_2024,
  title = {Language Is Primarily a Tool for Communication Rather than Thought},
  author = {Fedorenko, Evelina and Piantadosi, Steven T. and Gibson, Edward A. F.},
  year = {2024},
  month = jun,
  journal = {Nature},
  volume = {630},
  number = {8017},
  pages = {575--586},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-024-07522-w},
  urldate = {2024-06-23},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/BB8KSBHP/Fedorenko et al. - 2024 - Language is primarily a tool for communication rather than thought.pdf}
}

@misc{li_2023b,
  title = {{{HaluEval}}: {{A Large-Scale Hallucination Evaluation Benchmark}} for {{Large Language Models}}},
  shorttitle = {{{HaluEval}}},
  author = {Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-06-23},
  abstract = {Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about \$19.5{\textbackslash}\%\$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.},
  howpublished = {https://arxiv.org/abs/2305.11747v3},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/P7LNI3RG/Li et al. - 2023 - HaluEval A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.pdf}
}

@misc{hu_2024a,
  title = {{{RefChecker}}: {{Reference-based Fine-grained Hallucination Checker}} and {{Benchmark}} for {{Large Language Models}}},
  shorttitle = {{{RefChecker}}},
  author = {Hu, Xiangkun and Ru, Dongyu and Qiu, Lin and Guo, Qipeng and Zhang, Tianhang and Xu, Yang and Luo, Yun and Liu, Pengfei and Zhang, Yue and Zhang, Zheng},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-06-23},
  abstract = {Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker},
  howpublished = {https://arxiv.org/abs/2405.14486v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/RYKZ4L6S/Hu et al. - 2024 - RefChecker Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Mode.pdf}
}

@article{farquhar_2024,
  title = {Detecting Hallucinations in Large Language Models Using Semantic Entropy},
  author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  year = {2024},
  month = jun,
  journal = {Nature},
  volume = {630},
  number = {8017},
  pages = {625--630},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07421-0},
  urldate = {2024-06-24},
  abstract = {Hallucinations (confabulations) in large language model systems can be tackled by measuring uncertainty about the meanings of generated responses rather than the text itself\&nbsp;to improve question-answering accuracy.},
  copyright = {2024 The Author(s)},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/XCS5NPQA/Farquhar et al. - 2024 - Detecting hallucinations in large language models using semantic entropy.pdf}
}

@misc{lv_2023,
  title = {Kosmos-2.5: {{A Multimodal Literate Model}}},
  shorttitle = {Kosmos-2.5},
  author = {Lv, Tengchao and Huang, Yupan and Chen, Jingye and Cui, Lei and Ma, Shuming and Chang, Yaoyao and Huang, Shaohan and Wang, Wenhui and Dong, Li and Luo, Weiyao and Wu, Shaoxiang and Wang, Guoxin and Zhang, Cha and Wei, Furu},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-06-24},
  abstract = {We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.},
  howpublished = {https://arxiv.org/abs/2309.11419v1},
  langid = {english},
  annotation = {28 citations (Semantic Scholar/arXiv) [2024-06-24]},
  file = {/Users/chenghao/Zotero/storage/9M3XU44P/Lv et al. - 2023 - Kosmos-2.5 A Multimodal Literate Model.pdf}
}

@misc{longpre_2024,
  title = {The {{Responsible Foundation Model Development Cheatsheet}}: {{A Review}} of {{Tools}} \& {{Resources}}},
  shorttitle = {The {{Responsible Foundation Model Development Cheatsheet}}},
  author = {Longpre, Shayne and Biderman, Stella and Albalak, Alon and Schoelkopf, Hailey and McDuff, Daniel and Kapoor, Sayash and Klyman, Kevin and Lo, Kyle and Ilharco, Gabriel and San, Nay and Rauh, Maribeth and Skowron, Aviya and Vidgen, Bertie and Weidinger, Laura and Narayanan, Arvind and Sanh, Victor and Adelani, David and Liang, Percy and Bommasani, Rishi and Henderson, Peter and Luccioni, Sasha and Jernite, Yacine and Soldaini, Luca},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-25},
  abstract = {Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.},
  howpublished = {https://arxiv.org/abs/2406.16746v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/Z2W5QJWN/Longpre et al. - 2024 - The Responsible Foundation Model Development Cheatsheet A Review of Tools & Resources.pdf}
}

@misc{longpre_2023,
  title = {A {{Pretrainer}}'s {{Guide}} to {{Training Data}}: {{Measuring}} the {{Effects}} of {{Data Age}}, {{Domain Coverage}}, {{Quality}}, \& {{Toxicity}}},
  shorttitle = {A {{Pretrainer}}'s {{Guide}} to {{Training Data}}},
  author = {Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and Ippolito, Daphne},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-06-25},
  abstract = {Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.},
  howpublished = {https://arxiv.org/abs/2305.13169v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/FAXVBA8C/Longpre et al. - 2023 - A Pretrainer's Guide to Training Data Measuring the Effects of Data Age, Domain Coverage, Quality,.pdf}
}

@misc{tong_2024,
  title = {Cambrian-1: {{A Fully Open}}, {{Vision-Centric Exploration}} of {{Multimodal LLMs}}},
  shorttitle = {Cambrian-1},
  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-26},
  abstract = {We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.},
  howpublished = {https://arxiv.org/abs/2406.16860v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-06-26]},
  file = {/Users/chenghao/Zotero/storage/SICBQZ2U/Tong et al. - 2024 - Cambrian-1 A Fully Open, Vision-Centric Exploration of Multimodal LLMs.pdf}
}

@misc{xu_2024a,
  title = {Benchmarking {{Benchmark Leakage}} in {{Large Language Models}}},
  author = {Xu, Ruijie and Wang, Zengzhi and Fan, Run-Ze and Liu, Pengfei},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-06-26},
  abstract = {Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages. By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the "Benchmark Transparency Card" to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.},
  howpublished = {https://arxiv.org/abs/2404.18824v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/RAUJRC99/Xu et al. - 2024 - Benchmarking Benchmark Leakage in Large Language Models.pdf}
}

@misc{penedo_2024,
  title = {The {{FineWeb Datasets}}: {{Decanting}} the {{Web}} for the {{Finest Text Data}} at {{Scale}}},
  shorttitle = {The {{FineWeb Datasets}}},
  author = {Penedo, Guilherme and Kydl{\'i}{\v c}ek, Hynek and {allal}, Loubna Ben and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2406.17557},
  urldate = {2024-06-27},
  abstract = {The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/chenghao/Zotero/storage/FITCGQBC/Penedo et al. - 2024 - The FineWeb Datasets Decanting the Web for the Finest Text Data at Scale.pdf}
}

@misc{frohmann_2024,
  title = {Segment {{Any Text}}: {{A Universal Approach}} for {{Robust}}, {{Efficient}} and {{Adaptable Sentence Segmentation}}},
  shorttitle = {Segment {{Any Text}}},
  author = {Frohmann, Markus and Sterner, Igor and Vuli{\'c}, Ivan and Minixhofer, Benjamin and Schedl, Markus},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-27},
  abstract = {Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are available at https://huggingface.co/segment-any-text under the MIT license.},
  howpublished = {https://arxiv.org/abs/2406.16678v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/H2B6GKAM/Frohmann et al. - 2024 - Segment Any Text A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation.pdf}
}

@misc{white_2024,
  title = {{{LiveBench}}: {{A Challenging}}, {{Contamination-Free LLM Benchmark}}},
  shorttitle = {{{LiveBench}}},
  author = {White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and {Shwartz-Ziv}, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and Hegde, Chinmay and LeCun, Yann and Goldstein, Tom and Neiswanger, Willie and Goldblum, Micah},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-06-29},
  abstract = {Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65\% accuracy. We release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and we will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.},
  howpublished = {https://arxiv.org/abs/2406.19314v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/53Y9UCC8/White et al. - 2024 - LiveBench A Challenging, Contamination-Free LLM Benchmark.pdf}
}

@article{mcaleese_,
  title = {{{LLM Critics Help Catch LLM Bugs}}},
  author = {McAleese, Nat and Pokorny, Michael and Uribe, Juan Felipe Cer{\'o}n and Nitishinskaya, Evgenia and Tre, Maja and Leike, Jan},
  abstract = {Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains ``critic'' models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63\% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as ``flawless'', even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/HBGYXZMQ/McAleese et al. - LLM Critics Help Catch LLM Bugs.pdf}
}

@misc{bavaresco_2024,
  title = {{{LLMs}} Instead of {{Human Judges}}? {{A Large Scale Empirical Study}} across 20 {{NLP Evaluation Tasks}}},
  shorttitle = {{{LLMs}} Instead of {{Human Judges}}?},
  author = {Bavaresco, Anna and Bernardi, Raffaella and Bertolazzi, Leonardo and Elliott, Desmond and Fern{\'a}ndez, Raquel and Gatt, Albert and Ghaleb, Esam and Giulianelli, Mario and Hanna, Michael and Koller, Alexander and Martins, Andr{\'e} F. T. and Mondorf, Philipp and Neplenbroek, Vera and Pezzelle, Sandro and Plank, Barbara and Schlangen, David and Suglia, Alessandro and Surikuchi, Aditya K. and Takmaz, Ece and Testoni, Alberto},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-07-02},
  abstract = {There is an increasing trend towards evaluating NLP models with LLM-generated judgments instead of human judgments. In the absence of a comparison against human data, this raises concerns about the validity of these evaluations; in case they are conducted with proprietary models, this also raises concerns over reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. We conclude that LLMs are not yet ready to systematically replace human judges in NLP.},
  howpublished = {https://arxiv.org/abs/2406.18403v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/5W6DBAT4/Bavaresco et al. - 2024 - LLMs instead of Human Judges A Large Scale Empirical Study across 20 NLP Evaluation Tasks.pdf}
}

@misc{chan_2024,
  title = {Scaling {{Synthetic Data Creation}} with 1,000,000,000 {{Personas}}},
  author = {Chan, Xin and Wang, Xiaoyang and Yu, Dian and Mi, Haitao and Yu, Dong},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-07-02},
  abstract = {We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas ({\textasciitilde}13\% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.},
  howpublished = {https://arxiv.org/abs/2406.20094v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/Y87WJRRL/Chan et al. - 2024 - Scaling Synthetic Data Creation with 1,000,000,000 Personas.pdf}
}

@misc{silcock_2024,
  title = {Newswire: {{A Large-Scale Structured Database}} of a {{Century}} of {{Historical News}}},
  shorttitle = {Newswire},
  author = {Silcock, Emily and Arora, Abhishek and {D'Amico-Wong}, Luca and Dell, Melissa},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-07-02},
  abstract = {In the U.S. historically, local newspapers drew their content largely from newswires like the Associated Press. Historians argue that newswires played a pivotal role in creating a national identity and shared understanding of the world, but there is no comprehensive archive of the content sent over newswires. We reconstruct such an archive by applying a customized deep learning pipeline to hundreds of terabytes of raw image scans from thousands of local newspapers. The resulting dataset contains 2.7 million unique public domain U.S. newswire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model. To construct the Newswire dataset, we first recognize newspaper layouts and transcribe around 138 millions structured article texts from raw image scans. We then use a customized neural bi-encoder model to de-duplicate reproduced articles, in the presence of considerable abridgement and noise, quantifying how widely each article was reproduced. A text classifier is used to ensure that we only include newswire articles, which historically are in the public domain. The structured data that accompany the texts provide rich information about the who (disambiguated individuals), what (topics), and where (georeferencing) of the news that millions of Americans read over the course of a century. We also include Library of Congress metadata information about the newspapers that ran the articles on their front pages. The Newswire dataset is useful both for large language modeling - expanding training data beyond what is available from modern web texts - and for studying a diversity of questions in computational linguistics, social science, and the digital humanities.},
  howpublished = {https://arxiv.org/abs/2406.09490v1},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/arXiv) [2024-07-02]},
  file = {/Users/chenghao/Zotero/storage/VBVDCXWZ/Silcock et al. - 2024 - Newswire A Large-Scale Structured Database of a Century of Historical News.pdf}
}

@misc{qiao_2024,
  title = {We-{{Math}}: {{Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning}}?},
  shorttitle = {We-{{Math}}},
  author = {Qiao, Runqi and Tan, Qiuna and Dong, Guanting and Wu, Minhui and Sun, Chong and Song, Xiaoshuai and GongQue, Zhuoma and Lei, Shanglin and Wei, Zhe and Zhang, Miaoxuan and Qiao, Runfeng and Zhang, Yifan and Zong, Xiao and Xu, Yida and Diao, Muxi and Bao, Zhimin and Li, Chen and Zhang, Honggang},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-03},
  abstract = {Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks, such as MathVista and MathVerse, focus more on the result-oriented performance but neglect the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond end-to-end performance. We meticulously collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and five layers of knowledge granularity. We decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM), to hierarchically assess inherent issues in LMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and problem-specific performance. We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategies. More notably, the primary challenge of GPT-4o has significantly transitioned from IK to IG, establishing it as the first LMM advancing towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorization - they correctly solve composite problems involving multiple knowledge concepts yet fail to answer sub-problems. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs. The WE-MATH data and evaluation code are available at https://github.com/We-Math/We-Math.},
  howpublished = {https://arxiv.org/abs/2407.01284v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/JBHW9M7H/Qiao et al. - 2024 - We-Math Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning.pdf}
}

@misc{liu_2024c,
  title = {{{RegMix}}: {{Data Mixture}} as {{Regression}} for {{Language Model Pre-training}}},
  shorttitle = {{{RegMix}}},
  author = {Liu, Qian and Zheng, Xiaosen and Muennighoff, Niklas and Zeng, Guangtao and Dou, Longxu and Pang, Tianyu and Jiang, Jing and Lin, Min},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-03},
  abstract = {The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10\% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6\%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at https://github.com/sail-sg/regmix.},
  howpublished = {https://arxiv.org/abs/2407.01492v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/69IXPREF/Liu et al. - 2024 - RegMix Data Mixture as Regression for Language Model Pre-training.pdf}
}

@misc{eskimez_2024,
  title = {E2 {{TTS}}: {{Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS}}},
  shorttitle = {E2 {{TTS}}},
  author = {Eskimez, Sefik Emre and Wang, Xiaofei and Thakker, Manthan and Li, Canrun and Tsai, Chung-Hsien and Xiao, Zhen and Yang, Hemin and Zhu, Zirun and Tang, Min and Tan, Xu and Liu, Yanqing and Zhao, Sheng and Kanda, Naoyuki},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-07-03},
  abstract = {This paper introduces Embarrassingly Easy Text-to-Speech (E2 TTS), a fully non-autoregressive zero-shot text-to-speech system that offers human-level naturalness and state-of-the-art speaker similarity and intelligibility. In the E2 TTS framework, the text input is converted into a character sequence with filler tokens. The flow-matching-based mel spectrogram generator is then trained based on the audio infilling task. Unlike many previous works, it does not require additional components (e.g., duration model, grapheme-to-phoneme) or complex techniques (e.g., monotonic alignment search). Despite its simplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that are comparable to or surpass previous works, including Voicebox and NaturalSpeech 3. The simplicity of E2 TTS also allows for flexibility in the input representation. We propose several variants of E2 TTS to improve usability during inference. See https://aka.ms/e2tts/ for demo samples.},
  howpublished = {https://arxiv.org/abs/2406.18009v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-03]},
  file = {/Users/chenghao/Zotero/storage/3ERVFBP2/Eskimez et al. - 2024 - E2 TTS Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS.pdf}
}

@misc{wang_2024d,
  title = {Searching for {{Best Practices}} in {{Retrieval-Augmented Generation}}},
  author = {Wang, Xiaohua and Wang, Zhenghua and Gao, Xuan and Zhang, Feiran and Wu, Yixin and Xu, Zhibo and Shi, Tianyuan and Wang, Zhengyuan and Li, Shizheng and Qian, Qi and Yin, Ruicheng and Lv, Changze and Zheng, Xiaoqing and Huang, Xuanjing},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-03},
  abstract = {Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a "retrieval as generation" strategy.},
  howpublished = {https://arxiv.org/abs/2407.01219v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/8HAFAICE/Wang et al. - 2024 - Searching for Best Practices in Retrieval-Augmented Generation.pdf}
}

@misc{defazio_2024,
  title = {The {{Road Less Scheduled}}},
  author = {Defazio, Aaron and Xingyu and Yang and Mehta, Harsh and Mishchenko, Konstantin and Khaled, Ahmed and Cutkosky, Ashok},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-07-05},
  abstract = {Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available (https://github.com/facebookresearch/schedule\_free).},
  howpublished = {https://arxiv.org/abs/2405.15682v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/C5CB93M6/Defazio et al. - 2024 - The Road Less Scheduled.pdf}
}

@misc{zhang_2024c,
  title = {{{InternLM-XComposer-2}}.5: {{A Versatile Large Vision Language Model Supporting Long-Contextual Input}} and {{Output}}},
  shorttitle = {{{InternLM-XComposer-2}}.5},
  author = {Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Cao, Yuhang and Qian, Rui and Chen, Lin and Guo, Qipeng and Duan, Haodong and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Zhang, Wenwei and Li, Yining and Gao, Yang and Sun, Peng and Zhang, Xinyue and Li, Wei and Li, Jingwen and Wang, Wenhai and Yan, Hang and He, Conghui and Zhang, Xingcheng and Chen, Kai and Dai, Jifeng and Qiao, Yu and Lin, Dahua and Wang, Jiaqi},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-05},
  abstract = {We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.},
  howpublished = {https://arxiv.org/abs/2407.03320v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-05]},
  file = {/Users/chenghao/Zotero/storage/JMM9Q94E/Zhang et al. - 2024 - InternLM-XComposer-2.5 A Versatile Large Vision Language Model Supporting Long-Contextual Input and.pdf}
}

@misc{evans_2024,
  title = {Data Curation via Joint Example Selection Further Accelerates Multimodal Learning},
  author = {Evans, Talfan and Parthasarathy, Nikhil and Merzic, Hamza and Henaff, Olivier J.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.17711},
  eprint = {2406.17711},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-07},
  abstract = {Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individuallyprioritized data points. As performance improves by selecting from larger superbatches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach---multimodal contrastive learning with joint example selection (JEST)---surpasses state-of-the-art models with up to 13{\texttimes} fewer iterations and 10{\texttimes} less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/SM3IBHJA/Evans et al. - 2024 - Data curation via joint example selection further accelerates multimodal learning.pdf}
}

@misc{_l,
  title = {Tara\_{{Backup}}}
}

@misc{_o,
  title = {Addon {{Item}}}
}

@misc{sun_2024,
  title = {Learning to ({{Learn}} at {{Test Time}}): {{RNNs}} with {{Expressive Hidden States}}},
  shorttitle = {Learning to ({{Learn}} at {{Test Time}})},
  author = {Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and Hashimoto, Tatsunori and Guestrin, Carlos},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-09},
  abstract = {Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.},
  howpublished = {https://arxiv.org/abs/2407.04620v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/MRB7ZF5N/Sun et al. - 2024 - Learning to (Learn at Test Time) RNNs with Expressive Hidden States.pdf}
}

@misc{chen_2024b,
  title = {{{VALL-E}} 2: {{Neural Codec Language Models}} Are {{Human Parity Zero-Shot Text}} to {{Speech Synthesizers}}},
  shorttitle = {{{VALL-E}} 2},
  author = {Chen, Sanyuan and Liu, Shujie and Zhou, Long and Liu, Yanqing and Tan, Xu and Li, Jinyu and Zhao, Sheng and Qian, Yao and Wei, Furu},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05370},
  eprint = {2406.05370},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-07-11},
  abstract = {This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  annotation = {2 citations (Semantic Scholar/arXiv) [2024-07-11]},
  file = {/Users/chenghao/Zotero/storage/43RD6B4W/Chen et al. - 2024 - VALL-E 2 Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers.pdf}
}

@misc{rahmanzadehgervi_2024,
  title = {Vision Language Models Are Blind},
  author = {Rahmanzadehgervi, Pooyan and Bolton, Logan and Taesiri, Mohammad Reza and Nguyen, Anh Totti},
  year = {2024},
  month = jul,
  number = {arXiv:2407.06581},
  eprint = {2407.06581},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-11},
  abstract = {Large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini 1.5 Pro are powering countless image-text applications and scoring high on many vision-understanding benchmarks. Yet, we find that VLMs fail on 7 visual tasks absurdly easy to humans such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting the number of circles in a Olympic-like logo. The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like of a person with myopia seeing fine details as blurry, and at worst, like an intelligent person that is blind making educated guesses. Code is available at: https://vlmsareblind.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/chenghao/Zotero/storage/4VGHBCHY/Rahmanzadehgervi et al. - 2024 - Vision language models are blind.pdf}
}

@misc{chuang_2023,
  title = {{{DoLa}}: {{Decoding}} by {{Contrasting Layers Improves Factuality}} in {{Large Language Models}}},
  shorttitle = {{{DoLa}}},
  author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-07-11},
  abstract = {Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17\% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.},
  howpublished = {https://arxiv.org/abs/2309.03883v2},
  langid = {english},
  annotation = {58 citations (Semantic Scholar/arXiv) [2024-07-11]},
  file = {/Users/chenghao/Zotero/storage/5NG8ACLZ/Chuang et al. - 2023 - DoLa Decoding by Contrasting Layers Improves Factuality in Large Language Models.pdf}
}

@misc{chu_2024,
  title = {Qwen2-{{Audio Technical Report}}},
  author = {Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-18},
  abstract = {We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.},
  howpublished = {https://arxiv.org/abs/2407.10759v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-18]},
  file = {/Users/chenghao/Zotero/storage/3K8G2VPQ/Chu et al. - 2024 - Qwen2-Audio Technical Report.pdf}
}

@misc{goldstein_2024,
  title = {{{GoldFinch}}: {{High Performance RWKV}}/{{Transformer Hybrid}} with {{Linear Pre-Fill}} and {{Extreme KV-Cache Compression}}},
  shorttitle = {{{GoldFinch}}},
  author = {Goldstein, Daniel and Obeid, Fares and Alcaide, Eric and Song, Guangyu and Cheah, Eugene},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-19},
  abstract = {We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model that uses a new technique to efficiently generate a highly compressed and reusable KV-Cache in linear time and space with respect to sequence length. GoldFinch stacks our new GOLD transformer on top of an enhanced version of the Finch (RWKV-6) architecture. We train up to 1.5B parameter class models of the Finch, Llama, and GoldFinch architectures, and find dramatically improved modeling performance relative to both Finch and Llama. Our cache size savings increase linearly with model layer count, ranging from 756-2550 times smaller than the traditional transformer cache for common sizes, enabling inference of extremely large context lengths even on limited hardware. Although autoregressive generation has O(n) time complexity per token because of attention, pre-fill computation of the entire initial cache state for a submitted context costs only O(1) time per token due to the use of a recurrent neural network (RNN) to generate this cache. We release our trained weights and training code under the Apache 2.0 license for community use.},
  howpublished = {https://arxiv.org/abs/2407.12077v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-19]},
  file = {/Users/chenghao/Zotero/storage/RBE3FB9P/Goldstein et al. - 2024 - GoldFinch High Performance RWKVTransformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compre.pdf}
}

@misc{kaushal_2024,
  title = {Spectra: {{A Comprehensive Study}} of {{Ternary}}, {{Quantized}}, and {{FP16 Language Models}}},
  shorttitle = {Spectra},
  author = {Kaushal, Ayush and Pandey, Tejas and Vaidhya, Tejas and Bhagat, Aaryan and Rish, Irina},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-19},
  abstract = {Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but unfortunately, it suffers from significant performance degradation below 4-bit precision. An alternative approach involves training compressed models directly at a low bitwidth (e.g., binary or ternary models). However, the performance, training dynamics, and scaling trends of such models are not yet well understood. To address this issue, we train and openly release the Spectra LLM suite consisting of 54 language models ranging from 99M to 3.9B parameters, trained on 300B tokens. Spectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8 bits), and ternary LLMs (TriLMs) - our improved architecture for ternary language modeling, which significantly outperforms previously proposed ternary models of a given size (in bits), matching half-precision models at scale. For example, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M, but matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge benchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM 3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind FloatLM in perplexity on validation splits and web-based corpora but performs better on less noisy datasets like Lambada and PennTreeBank. To enhance understanding of low-bitwidth models, we are releasing 500+ intermediate checkpoints of the Spectra suite at {\textbackslash}href\{https://github.com/NolanoOrg/SpectraSuite\}\{https://github.com/NolanoOrg/SpectraSuite\}.},
  howpublished = {https://arxiv.org/abs/2407.12327v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-19]},
  file = {/Users/chenghao/Zotero/storage/KHIYQ4QE/Kaushal et al. - 2024 - Spectra A Comprehensive Study of Ternary, Quantized, and FP16 Language Models.pdf}
}

@article{feynman_,
  title = {Personal {{Observations}} on the {{Reliability}} of the  {{Shuttle}}},
  author = {Feynman, Richard P},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/UNSWXE8W/Feynman - Appendix F Personal Observations on the Reliability of the Shule.pdf}
}

@misc{_p,
  title = {The {{Annotated Transformer}}},
  urldate = {2024-07-22},
  howpublished = {https://nlp.seas.harvard.edu/annotated-transformer/},
  file = {/Users/chenghao/Zotero/storage/LGT3R4TG/annotated-transformer.html}
}

@misc{_q,
  title = {Shtetl-{{Optimized}} >> {{Blog Archive}} >> {{The First Law}} of {{Complexodynamics}}},
  urldate = {2024-07-22},
  howpublished = {https://scottaaronson.blog/?p=762}
}

@misc{_r,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  urldate = {2024-07-22},
  howpublished = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/}
}

@misc{_s,
  title = {Understanding {{LSTM Networks}} -- Colah's Blog},
  urldate = {2024-07-22},
  howpublished = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/}
}

@misc{zaremba_2014,
  title = {Recurrent {{Neural Network Regularization}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  year = {2014},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-07-22},
  abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
  howpublished = {https://arxiv.org/abs/1409.2329v5},
  langid = {english},
  annotation = {2586 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/Z7WKA9RA/Zaremba et al. - 2014 - Recurrent Neural Network Regularization.pdf}
}

@misc{vinyals_2015,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2015},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-07-22},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  howpublished = {https://arxiv.org/abs/1506.03134v2},
  langid = {english},
  annotation = {2736 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/VCCVC2EK/Vinyals et al. - 2015 - Pointer Networks.pdf}
}

@inproceedings{krizhevsky_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-07-22},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/Users/chenghao/Zotero/storage/ICWP6435/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@misc{vinyals_2016,
  title = {Order {{Matters}}: {{Sequence}} to Sequence for Sets},
  shorttitle = {Order {{Matters}}},
  author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
  year = {2016},
  month = feb,
  number = {arXiv:1511.06391},
  eprint = {1511.06391},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-22},
  abstract = {Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {885 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/DXXR7BUR/Vinyals et al. - 2016 - Order Matters Sequence to sequence for sets.pdf}
}

@misc{santoro_2018,
  title = {Relational Recurrent Neural Networks},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  year = {2018},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-07-22},
  abstract = {Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a {\textbackslash}textit\{Relational Memory Core\} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.},
  howpublished = {https://arxiv.org/abs/1806.01822v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/NZNH9BX8/Santoro et al. - 2018 - Relational recurrent neural networks.pdf}
}

@misc{huang_2019,
  title = {{{GPipe}}: {{Efficient Training}} of {{Giant Neural Networks}} Using {{Pipeline Parallelism}}},
  shorttitle = {{{GPipe}}},
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  year = {2019},
  month = jul,
  number = {arXiv:1811.06965},
  eprint = {1811.06965},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.06965},
  urldate = {2024-07-22},
  abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {7 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/8TMFITN9/Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks using Pipeline Parallelism.pdf}
}

@misc{he_2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2024-07-22},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {166787 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/QP6Y4IQK/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@misc{yu_2016,
  title = {Multi-{{Scale Context Aggregation}} by {{Dilated Convolutions}}},
  author = {Yu, Fisher and Koltun, Vladlen},
  year = {2016},
  month = apr,
  number = {arXiv:1511.07122},
  eprint = {1511.07122},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.07122},
  urldate = {2024-07-22},
  abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {7738 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/H2L7NMPS/Yu and Koltun - 2016 - Multi-Scale Context Aggregation by Dilated Convolutions.pdf}
}

@misc{vaswani_2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-22},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {98298 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/ES877LW8/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@misc{bahdanau_2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2024-07-22},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {25489 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/JLGQRM29/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf}
}

@misc{he_2016,
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jul,
  number = {arXiv:1603.05027},
  eprint = {1603.05027},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.05027},
  urldate = {2024-07-22},
  abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {9318 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/AEFEZLQC/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf}
}

@misc{santoro_2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  year = {2017},
  month = jun,
  number = {arXiv:1706.01427},
  eprint = {1706.01427},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.01427},
  urldate = {2024-07-22},
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {1529 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/4YCF8JZZ/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf}
}

@misc{chen_2017,
  title = {Variational {{Lossy Autoencoder}}},
  author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2017},
  month = mar,
  number = {arXiv:1611.02731},
  eprint = {1611.02731},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.02731},
  urldate = {2024-07-22},
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x{\textbar}z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {632 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/DPY5WI9Q/Chen et al. - 2017 - Variational Lossy Autoencoder.pdf}
}

@misc{aaronson_2014,
  title = {Quantifying the {{Rise}} and {{Fall}} of {{Complexity}} in {{Closed Systems}}: {{The Coffee Automaton}}},
  shorttitle = {Quantifying the {{Rise}} and {{Fall}} of {{Complexity}} in {{Closed Systems}}},
  author = {Aaronson, Scott and Carroll, Sean M. and Ouellette, Lauren},
  year = {2014},
  month = may,
  number = {arXiv:1405.6903},
  eprint = {1405.6903},
  primaryclass = {cond-mat, physics:gr-qc, physics:nlin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1405.6903},
  urldate = {2024-07-22},
  abstract = {In contrast to entropy, which increases monotonically, the "complexity" or "interestingness" of closed systems seems intuitively to increase at first and then decrease as equilibrium is approached. For example, our universe lacked complex structures at the Big Bang and will also lack them after black holes evaporate and particles are dispersed. This paper makes an initial attempt to quantify this pattern. As a model system, we use a simple, two-dimensional cellular automaton that simulates the mixing of two liquids ("coffee" and "cream"). A plausible complexity measure is then the Kolmogorov complexity of a coarse-grained approximation of the automaton's state, which we dub the "apparent complexity." We study this complexity measure, and show analytically that it never becomes large when the liquid particles are non-interacting. By contrast, when the particles do interact, we give numerical evidence that the complexity reaches a maximum comparable to the "coffee cup's" horizontal dimension. We raise the problem of proving this behavior analytically.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Statistical Mechanics,General Relativity and Quantum Cosmology,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  annotation = {14 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/GEBQ88QF/Aaronson et al. - 2014 - Quantifying the Rise and Fall of Complexity in Closed Systems The Coffee Automaton.pdf}
}

@misc{graves_2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = dec,
  number = {arXiv:1410.5401},
  eprint = {1410.5401},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1410.5401},
  urldate = {2024-07-22},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/chenghao/Zotero/storage/L4DVSURL/Graves et al. - 2014 - Neural Turing Machines.pdf}
}

@misc{amodei_2015,
  title = {Deep {{Speech}} 2: {{End-to-End Speech Recognition}} in {{English}} and {{Mandarin}}},
  shorttitle = {Deep {{Speech}} 2},
  author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
  year = {2015},
  month = dec,
  number = {arXiv:1512.02595},
  eprint = {1512.02595},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.02595},
  urldate = {2024-07-22},
  abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {2811 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/US2XJAIR/Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in English and Mandarin.pdf}
}

@misc{grunwald_2004,
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  author = {Grunwald, Peter},
  year = {2004},
  month = jun,
  number = {arXiv:math/0406077},
  eprint = {math/0406077},
  publisher = {arXiv},
  doi = {10.48550/arXiv.math/0406077},
  urldate = {2024-07-22},
  abstract = {This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection "Advances in Minimum Description Length: Theory and Application" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).},
  archiveprefix = {arXiv},
  keywords = {6201,6801,68T05,68T10,9401,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory},
  annotation = {370 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/DA26VA2H/Grunwald - 2004 - A tutorial introduction to the minimum description length principle.pdf}
}

@book{shen_2017,
  title = {Kolmogorov {{Complexity}} and {{Algorithmic Randomness}}},
  author = {Shen, A. and Uspensky, V. and Vereshchagin, N.},
  year = {2017},
  month = nov,
  series = {Mathematical {{Surveys}} and                         {{Monographs}}},
  volume = {220},
  publisher = {American Mathematical                     Society},
  address = {Providence, Rhode                     Island},
  doi = {10.1090/surv/220},
  urldate = {2024-07-22},
  isbn = {978-1-4704-3182-2 978-1-4704-4083-1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/6SFE2ZXV/Shen et al. - 2017 - Kolmogorov Complexity and Algorithmic Randomness.pdf}
}

@misc{_t,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  urldate = {2024-07-22},
  howpublished = {https://cs231n.github.io/}
}

@misc{_u,
  title = {Stanford {{CS 224N}} {\textbar} {{Natural Language Processing}} with {{Deep Learning}}},
  urldate = {2024-07-22},
  howpublished = {https://web.stanford.edu/class/cs224n/}
}

@misc{qin_2023a,
  title = {{{OpenVoice}}: {{Versatile Instant Voice Cloning}}},
  shorttitle = {{{OpenVoice}}},
  author = {Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-07-22},
  abstract = {We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice can clone voices into a new language without any massive-speaker training data for that language. OpenVoice is also computationally efficient, costing tens of times less than commercially available APIs that offer even inferior performance. To foster further research in the field, we have made the source code and trained model publicly accessible. We also provide qualitative results in our demo website. Prior to its public release, our internal version of OpenVoice was used tens of millions of times by users worldwide between May and October 2023, serving as the backend of MyShell.},
  howpublished = {https://arxiv.org/abs/2312.01479v5},
  langid = {english},
  annotation = {5 citations (Semantic Scholar/arXiv) [2024-07-22]},
  file = {/Users/chenghao/Zotero/storage/XG4EYTMS/Qin et al. - 2023 - OpenVoice Versatile Instant Voice Cloning.pdf}
}

@misc{_v,
  title = {{{MARS5-TTS}}/Docs/Architecture.Md at Master {$\cdot$} {{Camb-ai}}/{{MARS5-TTS}}},
  urldate = {2024-07-22},
  howpublished = {https://github.com/Camb-ai/MARS5-TTS/blob/master/docs/architecture.md}
}

@misc{_w,
  title = {Real-Time {{Voice AI Agent}} - {{Cerebrium}}},
  urldate = {2024-07-22},
  howpublished = {https://docs.cerebrium.ai/v4/examples/realtime-voice-agents}
}

@misc{_x,
  title = {State {{Space Duality}} ({{Mamba-2}}) {{Part I}} - {{The Model}} {\textbar} {{Tri Dao}}},
  urldate = {2024-07-22},
  howpublished = {https://tridao.me/blog/2024/mamba2-part1-model/},
  file = {/Users/chenghao/Zotero/storage/LU5BUUNJ/mamba2-part1-model.html}
}

@misc{_y,
  title = {State {{Space Duality}} ({{Mamba-2}}) {{Part II}} - {{The Theory}} {\textbar} {{Tri Dao}}},
  urldate = {2024-07-22},
  howpublished = {https://tridao.me/blog/2024/mamba2-part2-theory/}
}

@misc{_z,
  title = {State {{Space Duality}} ({{Mamba-2}}) {{Part III}} - {{The Algorithm}} {\textbar} {{Tri Dao}}},
  urldate = {2024-07-22},
  howpublished = {https://tridao.me/blog/2024/mamba2-part3-algorithm/}
}

@misc{_aa,
  title = {State {{Space Duality}} ({{Mamba-2}}) {{Part IV}} - {{The Systems}} {\textbar} {{Tri Dao}}},
  urldate = {2024-07-22},
  howpublished = {https://tridao.me/blog/2024/mamba2-part4-systems/},
  file = {/Users/chenghao/Zotero/storage/9MPN7LY9/mamba2-part4-systems.html}
}

@misc{_ab,
  title = {{{GPUs Go Brrr}}},
  urldate = {2024-07-22},
  abstract = {how make gpu fast?},
  howpublished = {https://hazyresearch.stanford.edu/blog/2024-05-12-tk},
  langid = {english}
}

@misc{_ac,
  title = {{{FlashAttention-3}}: {{Fast}} and {{Accurate Attention}} with {{Asynchrony}} and {{Low-precision}}},
  urldate = {2024-07-22},
  howpublished = {https://www.together.ai/blog/flashattention-3}
}

@misc{fu_2024,
  title = {{{LazyLLM}}: {{Dynamic Token Pruning}} for {{Efficient Long Context LLM Inference}}},
  shorttitle = {{{LazyLLM}}},
  author = {Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar},
  year = {2024},
  month = jul,
  number = {arXiv:2407.14057},
  eprint = {2407.14057},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.14057},
  urldate = {2024-07-23},
  abstract = {The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-23]},
  file = {/Users/chenghao/Zotero/storage/AX9SFPHX/Fu et al. - 2024 - LazyLLM Dynamic Token Pruning for Efficient Long Context LLM Inference.pdf}
}

@misc{evans_2024a,
  title = {Stable {{Audio Open}}},
  author = {Evans, Zach and Parker, Julian D. and Carr, C. J. and Zukowski, Zack and Taylor, Josiah and Pons, Jordi},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-23},
  abstract = {Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.},
  howpublished = {https://arxiv.org/abs/2407.14358v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-23]},
  file = {/Users/chenghao/Zotero/storage/W8PLZV56/Evans et al. - 2024 - Stable Audio Open.pdf}
}

@misc{abramovich_2024,
  title = {{{VisFocus}}: {{Prompt-Guided Vision Encoders}} for {{OCR-Free Dense Document Understanding}}},
  shorttitle = {{{VisFocus}}},
  author = {Abramovich, Ofir and Nayman, Niv and Fogel, Sharon and Lavi, Inbal and Litman, Ron and Tsiper, Shahar and Tichauer, Royee and Appalaraju, Srikar and Mazor, Shai and Manmatha, R.},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-23},
  abstract = {In recent years, notable advancements have been made in the domain of visual document understanding, with the prevailing architecture comprising a cascade of vision and language models. The text component can either be extracted explicitly with the use of external OCR models in OCR-based approaches, or alternatively, the vision model can be endowed with reading capabilities in OCR-free approaches. Typically, the queries to the model are input exclusively to the language component, necessitating the visual features to encompass the entire document. In this paper, we present VisFocus, an OCR-free method designed to better exploit the vision encoder's capacity by coupling it directly with the language prompt. To do so, we replace the down-sampling layers with layers that receive the input prompt and allow highlighting relevant parts of the document, while disregarding others. We pair the architecture enhancements with a novel pre-training task, using language masking on a snippet of the document text fed to the visual encoder in place of the prompt, to empower the model with focusing capabilities. Consequently, VisFocus learns to allocate its attention to text patches pertinent to the provided prompt. Our experiments demonstrate that this prompt-guided visual encoding approach significantly improves performance, achieving state-of-the-art results on various benchmarks.},
  howpublished = {https://arxiv.org/abs/2407.12594v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/V7JBM77G/Abramovich et al. - 2024 - VisFocus Prompt-Guided Vision Encoders for OCR-Free Dense Document Understanding.pdf}
}

@article{kirchner_,
  title = {{{PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS}}},
  author = {Kirchner, Jan Hendrik and Chen, Yining and Edwards, Harri and Leike, Jan and McAleese, Nat and Burda, Yuri},
  abstract = {One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check --- a property we call legibility. We study legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, we propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). Our algorithm iteratively trains small verifiers to predict solution correctness, ``helpful'' provers to produce correct solutions that the verifier accepts, and ``sneaky'' provers to produce incorrect solutions that fool the verifier. We find that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, we show that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. Our results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/ZMB698P9/Kirchner et al. - PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS.pdf}
}

@misc{_ad,
  title = {A {{Gentle Introduction}} to {{Graph Neural Networks}}},
  urldate = {2024-07-24},
  howpublished = {https://distill.pub/2021/gnn-intro/}
}

@misc{_ae,
  title = {How to {{Use}} T-{{SNE Effectively}}},
  urldate = {2024-07-24},
  howpublished = {https://distill.pub/2016/misread-tsne/},
  file = {/Users/chenghao/Zotero/storage/LVMGTRJU/misread-tsne.html}
}

@misc{wang_2024e,
  title = {Knowledge {{Mechanisms}} in {{Large Language Models}}: {{A Survey}} and {{Perspective}}},
  shorttitle = {Knowledge {{Mechanisms}} in {{Large Language Models}}},
  author = {Wang, Mengru and Yao, Yunzhi and Xu, Ziwen and Qiao, Shuofei and Deng, Shumin and Wang, Peng and Chen, Xiang and Gu, Jia-Chen and Jiang, Yong and Xie, Pengjun and Huang, Fei and Chen, Huajun and Zhang, Ningyu},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-24},
  abstract = {Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.},
  howpublished = {https://arxiv.org/abs/2407.15017v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/6GCSD973/Wang et al. - 2024 - Knowledge Mechanisms in Large Language Models A Survey and Perspective.pdf}
}

@misc{awadalla_2024,
  title = {{{MINT-1T}}: {{Scaling Open-Source Multimodal Data}} by 10x: {{A Multimodal Dataset}} with {{One Trillion Tokens}}},
  shorttitle = {{{MINT-1T}}},
  author = {Awadalla, Anas and Xue, Le and Lo, Oscar and Shu, Manli and Lee, Hannah and Guha, Etash Kumar and Jordan, Matt and Shen, Sheng and Awadalla, Mohamed and Savarese, Silvio and Xiong, Caiming and Xu, Ran and Choi, Yejin and Schmidt, Ludwig},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-07-24},
  abstract = {Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, diverse open-source multimodal interleaved datasets. In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises one trillion text tokens and three billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. Our data and code will be released at https://github.com/mlfoundations/MINT-1T.},
  howpublished = {https://arxiv.org/abs/2406.11271v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/PJQ8P74R/Awadalla et al. - 2024 - MINT-1T Scaling Open-Source Multimodal Data by 10x A Multimodal Dataset with One Trillion Tokens.pdf}
}

@misc{antoniades_2024,
  title = {Generalization v.s. {{Memorization}}: {{Tracing Language Models}}' {{Capabilities Back}} to {{Pretraining Data}}},
  shorttitle = {Generalization v.s. {{Memorization}}},
  author = {Antoniades, Antonis and Wang, Xinyi and Elazar, Yanai and Amayuelas, Alfonso and Albalak, Alon and Zhang, Kexun and Wang, William Yang},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-07-25},
  abstract = {Despite the proven utility of large language models (LLMs) in real-world applications, there remains a lack of understanding regarding how they leverage their large-scale pretraining text corpora to achieve such capabilities. In this work, we investigate the interplay between generalization and memorization in pretrained LLMs at scale, through a comprehensive \$n\$-gram analysis of their training data. Our experiments focus on three general task types: translation, question-answering, and multiple-choice reasoning. With various sizes of open-source LLMs and their pretraining corpora, we observe that as the model size increases, the task-relevant \$n\$-gram pair data becomes increasingly important, leading to improved task performance, decreased memorization, stronger generalization, and emergent abilities. Our results support the hypothesis that LLMs' capabilities emerge from a delicate balance of memorization and generalization with sufficient task-related pretraining data, and point the way to larger-scale analyses that could further improve our understanding of these models.},
  howpublished = {https://arxiv.org/abs/2407.14985v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-07-25]},
  file = {/Users/chenghao/Zotero/storage/AS5PUSHY/Antoniades et al. - 2024 - Generalization v.s. Memorization Tracing Language Models' Capabilities Back to Pretraining Data.pdf}
}

@misc{pan_2023,
  title = {What {{In-Context Learning}} "{{Learns}}" {{In-Context}}: {{Disentangling Task Recognition}} and {{Task Learning}}},
  shorttitle = {What {{In-Context Learning}} "{{Learns}}" {{In-Context}}},
  author = {Pan, Jane and Gao, Tianyu and Chen, Howard and Chen, Danqi},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-07-25},
  abstract = {Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -- and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL's performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.},
  howpublished = {https://arxiv.org/abs/2305.09731v1},
  langid = {english},
  annotation = {72 citations (Semantic Scholar/arXiv) [2024-07-25]},
  file = {/Users/chenghao/Zotero/storage/AHN5VHAH/Pan et al. - 2023 - What In-Context Learning Learns In-Context Disentangling Task Recognition and Task Learning.pdf}
}

@misc{gerstgrasser_2024,
  title = {Is {{Model Collapse Inevitable}}? {{Breaking}} the {{Curse}} of {{Recursion}} by {{Accumulating Real}} and {{Synthetic Data}}},
  shorttitle = {Is {{Model Collapse Inevitable}}?},
  author = {Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and Roberts, Daniel A. and Yang, Diyi and Donoho, David L. and Koyejo, Sanmi},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-07-26},
  abstract = {The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops proposed that such loops would lead to a phenomenon termed model collapse, under which performance progressively degrades with each model-data feedback iteration until fitted models become useless. However, those studies largely assumed that new data replace old data over time, where an arguably more realistic assumption is that data accumulate over time. In this paper, we ask: what effect does accumulating data have on model collapse? We empirically study this question by pretraining sequences of language models on text corpora. We confirm that replacing the original real data by each generation's synthetic data does indeed tend towards model collapse, then demonstrate that accumulating the successive generations of synthetic data alongside the original real data avoids model collapse; these results hold across a range of model sizes, architectures, and hyperparameters. We obtain similar results for deep generative models on other types of real data: diffusion models for molecule conformation generation and variational autoencoders for image generation. To understand why accumulating data can avoid model collapse, we use an analytically tractable framework introduced by prior work in which a sequence of linear models are fit to the previous models' outputs. Previous work used this framework to show that if data are replaced, the test error increases with the number of model-fitting iterations; we extend this argument to prove that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations, meaning model collapse no longer occurs.},
  howpublished = {https://arxiv.org/abs/2404.01413v2},
  langid = {english},
  annotation = {9 citations (Semantic Scholar/arXiv) [2024-07-26]},
  file = {/Users/chenghao/Zotero/storage/NAG42KRC/Gerstgrasser et al. - 2024 - Is Model Collapse Inevitable Breaking the Curse of Recursion by Accumulating Real and Synthetic Dat.pdf}
}

@misc{kashani_2021,
  title = {Deep {{Learning Interviews}}: {{Hundreds}} of Fully Solved Job Interview Questions from a Wide Range of Key Topics in {{AI}}},
  shorttitle = {Deep {{Learning Interviews}}},
  author = {Kashani, Shlomo and Ivry, Amir},
  year = {2021},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-07-27},
  abstract = {The second edition of Deep Learning Interviews is home to hundreds of fully-solved problems, from a wide range of key topics in AI. It is designed to both rehearse interview or exam specific topics and provide machine learning MSc / PhD. students, and those awaiting an interview a well-organized overview of the field. The problems it poses are tough enough to cut your teeth on and to dramatically improve your skills-but they're framed within thought-provoking questions and engaging stories. That is what makes the volume so specifically valuable to students and job seekers: it provides them with the ability to speak confidently and quickly on any relevant topic, to answer technical questions clearly and correctly, and to fully understand the purpose and meaning of interview questions and answers. Those are powerful, indispensable advantages to have when walking into the interview room. The book's contents is a large inventory of numerous topics relevant to DL job interviews and graduate level exams. That places this work at the forefront of the growing trend in science to teach a core set of practical mathematical and computational skills. It is widely accepted that the training of every computer scientist must include the fundamental theorems of ML, and AI appears in the curriculum of nearly every university. This volume is designed as an excellent reference for graduates of such programs.},
  howpublished = {https://arxiv.org/abs/2201.00650v2},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/arXiv) [2024-07-27]},
  file = {/Users/chenghao/Zotero/storage/DSRY3KR5/Kashani and Ivry - 2021 - Deep Learning Interviews Hundreds of fully solved job interview questions from a wide range of key.pdf}
}

@article{_af,
  title = {Apple {{Intelligence Foundation Language Models}}},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/LKR83NME/Apple Intelligence Foundation Language Models.pdf}
}

@misc{dubey_2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and {van der Linde}, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and {Rantala-Yeary}, Lauren and {van der Maaten}, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and {de Oliveira}, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzm{\'a}n, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-08-02},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  howpublished = {https://arxiv.org/abs/2407.21783v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/2LDUCNJU/Dubey et al. - 2024 - The Llama 3 Herd of Models.pdf}
}

@misc{team_2024a,
  title = {Gemma 2: {{Improving Open Language Models}} at a {{Practical Size}}},
  shorttitle = {Gemma 2},
  author = {Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and Ferret, Johan and Liu, Peter and Tafti, Pouya and Friesen, Abe and Casbon, Michelle and Ramos, Sabela and Kumar, Ravin and Lan, Charline Le and Jerome, Sammy and Tsitsulin, Anton and Vieillard, Nino and Stanczyk, Piotr and Girgin, Sertan and Momchev, Nikola and Hoffman, Matt and Thakoor, Shantanu and Grill, Jean-Bastien and Neyshabur, Behnam and Walton, Alanna and Severyn, Aliaksei and Parrish, Alicia and Ahmad, Aliya and Hutchison, Allen and Abdagic, Alvin and Carl, Amanda and Shen, Amy and Brock, Andy and Coenen, Andy and Laforge, Anthony and Paterson, Antonia and Bastian, Ben and Piot, Bilal and Wu, Bo and Royal, Brandon and Chen, Charlie and Kumar, Chintu and Perry, Chris and Welty, Chris and {Choquette-Choo}, Christopher A. and Sinopalnikov, Danila and Weinberger, David and Vijaykumar, Dimple and Rogozi{\'n}ska, Dominika and Herbison, Dustin and Bandy, Elisa and Wang, Emma and Noland, Eric and Moreira, Erica and Senter, Evan and Eltyshev, Evgenii and Visin, Francesco and Rasskin, Gabriel and Wei, Gary and Cameron, Glenn and Martins, Gus and Hashemi, Hadi and {Klimczak-Pluci{\'n}ska}, Hanna and Batra, Harleen and Dhand, Harsh and Nardini, Ivan and Mein, Jacinda and Zhou, Jack and Svensson, James and Stanway, Jeff and Chan, Jetha and Zhou, Jin and Carrasqueira, Joana and Iljazi, Joana and Becker, Jocelyn and Fernandez, Joe and {van Amersfoort}, Joost and Gordon, Josh and Lipschultz, Josh and Newlan, Josh and Ji, Ju-yeong and Mohamed, Kareem and Badola, Kartikeya and Black, Kat and Millican, Katie and McDonell, Keelin and Nguyen, Kelvin and Sodhia, Kiranbir and Greene, Kish and Sjoesund, Lars Lowe and Usui, Lauren and Sifre, Laurent and Heuermann, Lena and Lago, Leticia and McNealus, Lilly and Soares, Livio Baldini and Kilpatrick, Logan and Dixon, Lucas and Martins, Luciano and Reid, Machel and Singh, Manvinder and Iverson, Mark and G{\"o}rner, Martin and Velloso, Mat and Wirth, Mateo and Davidow, Matt and Miller, Matt and Rahtz, Matthew and Watson, Matthew and Risdal, Meg and Kazemi, Mehran and Moynihan, Michael and Zhang, Ming and Kahng, Minsuk and Park, Minwoo and Rahman, Mofi and Khatwani, Mohit and Dao, Natalie and Bardoliwalla, Nenshad and Devanathan, Nesh and Dumai, Neta and Chauhan, Nilay and Wahltinez, Oscar and Botarda, Pankil and Barnes, Parker and Barham, Paul and Michel, Paul and Jin, Pengchong and Georgiev, Petko and Culliton, Phil and Kuppala, Pradeep and Comanescu, Ramona and Merhej, Ramona and Jana, Reena and Rokni, Reza Ardeshir and Agarwal, Rishabh and Mullins, Ryan and Saadat, Samaneh and Carthy, Sara Mc and Perrin, Sarah and Arnold, S{\'e}bastien and Krause, Sebastian and Dai, Shengyang and Garg, Shruti and Sheth, Shruti and Ronstrom, Sue and Chan, Susan and Jordan, Timothy and Yu, Ting and Eccles, Tom and Hennigan, Tom and Kocisky, Tomas and Doshi, Tulsee and Jain, Vihan and Yadav, Vikas and Meshram, Vilobh and Dharmadhikari, Vishal and Barkley, Warren and Wei, Wei and Ye, Wenming and Han, Woohyun and Kwon, Woosuk and Xu, Xiang and Shen, Zhe and Gong, Zhitao and Wei, Zichuan and Cotruta, Victor and Kirk, Phoebe and Rao, Anand and Giang, Minh and Peran, Ludovic and Warkentin, Tris and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Sculley, D. and Banks, Jeanine and Dragan, Anca and Petrov, Slav and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Borgeaud, Sebastian and Fiedel, Noah and Joulin, Armand and Kenealy, Kathleen and Dadashi, Robert and Andreev, Alek},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-08-03},
  abstract = {In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.},
  howpublished = {https://arxiv.org/abs/2408.00118v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/SCS8SXL2/Team et al. - 2024 - Gemma 2 Improving Open Language Models at a Practical Size.pdf}
}

@misc{song_2024,
  title = {The {{Good}}, {{The Bad}}, and {{The Greedy}}: {{Evaluation}} of {{LLMs Should Not Ignore Non-Determinism}}},
  shorttitle = {The {{Good}}, {{The Bad}}, and {{The Greedy}}},
  author = {Song, Yifan and Wang, Guoyin and Li, Sujian and Lin, Bill Yuchen},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-08-04},
  abstract = {Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks' consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance. Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation.},
  howpublished = {https://arxiv.org/abs/2407.10457v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/VKEQT6MI/Song et al. - 2024 - The Good, The Bad, and The Greedy Evaluation of LLMs Should Not Ignore Non-Determinism.pdf}
}

@misc{blecher_2023,
  title = {Nougat: {{Neural Optical Understanding}} for {{Academic Documents}}},
  shorttitle = {Nougat},
  author = {Blecher, Lukas and Cucurull, Guillem and Scialom, Thomas and Stojnic, Robert},
  year = {2023},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-04},
  abstract = {Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.},
  howpublished = {https://arxiv.org/abs/2308.13418v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/4ENUNTJM/Blecher et al. - 2023 - Nougat Neural Optical Understanding for Academic Documents.pdf}
}

@article{lampinen_,
  title = {Can Language Models Handle Recursively Nested Grammatical Structures? {{A}} Case Study on Comparing Models and Humans},
  shorttitle = {Can Language Models Handle Recursively Nested Grammatical Structures?},
  author = {Lampinen, Andrew},
  journal = {Computational Linguistics},
  pages = {1--35},
  doi = {10.1162/coli_a_00525},
  urldate = {2024-08-04},
  abstract = {Abstract. How should we compare the capabilities of language models (LMs) and humans? In this paper, I draw inspiration from comparative psychology to highlight challenges in these comparisons. I focus on a case study: processing of recursively nested grammatical structures. Prior work suggests that LMs cannot process these structures as reliably as humans can. However, the humans were provided with instructions and substantial training, while the LMs were evaluated zero-shot. I therefore match the evaluation more closely. Providing large LMs with a simple prompt---with substantially less content than the human training---allows the LMs to consistently outperform the human results, even in more deeply nested conditions than were tested with humans. Furthermore, the effects of prompting are robust to the particular structures and vocabulary used in the prompt. Finally, reanalyzing the existing human data suggests that the humans may not perform above chance at the difficult structures initially. Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans, when evaluated comparably. This case study highlights how discrepancies in the evaluation methods can confound comparisons of language models and humans. I conclude by reflecting on the broader challenge of comparing human and model capabilities, and highlight an important difference between evaluating cognitive models and foundation models.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/XKA4XTZT/Lampinen - Can language models handle recursively nested grammatical structures A case study on comparing mode.pdf}
}

@misc{ma_2024b,
  title = {Language {{Model Can Listen While Speaking}}},
  author = {Ma, Ziyang and Song, Yakun and Du, Chenpeng and Cong, Jian and Chen, Zhuo and Wang, Yuping and Wang, Yuxuan and Chen, Xie},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-06},
  abstract = {Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.},
  howpublished = {https://arxiv.org/abs/2408.02622v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/73W78VJF/Ma et al. - 2024 - Language Model Can Listen While Speaking.pdf}
}

@misc{yao_2024,
  title = {{{MiniCPM-V}}: {{A GPT-4V Level MLLM}} on {{Your Phone}}},
  shorttitle = {{{MiniCPM-V}}},
  author = {Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and Chen, Qianyu and Zhou, Huarong and Zou, Zhensheng and Zhang, Haoye and Hu, Shengding and Zheng, Zhi and Zhou, Jie and Cai, Jie and Han, Xu and Zeng, Guoyang and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-07},
  abstract = {The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain preventing MLLMs from being practical in real-world applications. The most notable challenge comes from the huge cost of running an MLLM with a massive number of parameters and extensive computation. As a result, most MLLMs need to be deployed on high-performing cloud servers, which greatly limits their application scopes such as mobile, offline, energy-sensitive, and privacy-protective scenarios. In this work, we present MiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By integrating the latest MLLM techniques in architecture, pretraining and alignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1) Strong performance, outperforming GPT-4V-1106, Gemini Pro and Claude 3 on OpenCompass, a comprehensive evaluation over 11 popular benchmarks, (2) strong OCR capability and 1.8M pixel high-resolution image perception at any aspect ratio, (3) trustworthy behavior with low hallucination rates, (4) multilingual support for 30+ languages, and (5) efficient deployment on mobile phones. More importantly, MiniCPM-V can be viewed as a representative example of a promising trend: The model sizes for achieving usable (e.g., GPT-4V) level performance are rapidly decreasing, along with the fast growth of end-side computation capacity. This jointly shows that GPT-4V level MLLMs deployed on end devices are becoming increasingly possible, unlocking a wider spectrum of real-world AI applications in the near future.},
  howpublished = {https://arxiv.org/abs/2408.01800v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-08-07]},
  file = {/Users/chenghao/Zotero/storage/VZVCTKT8/Yao et al. - 2024 - MiniCPM-V A GPT-4V Level MLLM on Your Phone.pdf}
}

@misc{_ag,
  title = {{{FlexAttention}}: {{The Flexibility}} of {{PyTorch}} with the {{Performance}} of {{FlashAttention}} {\textbar} {{PyTorch}}},
  urldate = {2024-08-07},
  howpublished = {https://pytorch.org/blog/flexattention/}
}

@misc{li_2023c,
  title = {Are {{ChatGPT}} and {{GPT-4 General-Purpose Solvers}} for {{Financial Text Analytics}}? {{A Study}} on {{Several Typical Tasks}}},
  shorttitle = {Are {{ChatGPT}} and {{GPT-4 General-Purpose Solvers}} for {{Financial Text Analytics}}?},
  author = {Li, Xianzhi and Chan, Samuel and Zhu, Xiaodan and Pei, Yulong and Ma, Zhiqiang and Liu, Xiaomo and Shah, Sameena},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-08-09},
  abstract = {The most recent large language models(LLMs) such as ChatGPT and GPT-4 have shown exceptional capabilities of generalist models, achieving state-of-the-art performance on a wide range of NLP tasks with little or no adaptation. How effective are such models in the financial domain? Understanding this basic question would have a significant impact on many downstream financial analytical tasks. In this paper, we conduct an empirical study and provide experimental evidences of their performance on a wide variety of financial text analytical problems, using eight benchmark datasets from five categories of tasks. We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domain-specific pretrained models. We hope our study can help understand the capability of the existing models in the financial domain and facilitate further improvements.},
  howpublished = {https://arxiv.org/abs/2305.05862v2},
  langid = {english},
  annotation = {20 citations (Semantic Scholar/arXiv) [2024-08-09]},
  file = {/Users/chenghao/Zotero/storage/Y83GQSJ5/Li et al. - 2023 - Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics A Study on Several Typic.pdf}
}

@misc{cho_2024,
  title = {Transformer {{Explainer}}: {{Interactive Learning}} of {{Text-Generative Models}}},
  shorttitle = {Transformer {{Explainer}}},
  author = {Cho, Aeree and Kim, Grace C. and Karpekov, Alexander and Helbling, Alec and Wang, Zijie J. and Lee, Seongmin and Hoover, Benjamin and Chau, Duen Horng},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-09},
  abstract = {Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at https://poloclub.github.io/transformer-explainer/. A video demo is available at https://youtu.be/ECR4oAwocjs.},
  howpublished = {https://arxiv.org/abs/2408.04619v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-08-09]},
  file = {/Users/chenghao/Zotero/storage/T3T33FYD/Cho et al. - 2024 - Transformer Explainer Interactive Learning of Text-Generative Models.pdf}
}

@misc{tai_2024,
  title = {{{PIXAR}}: {{Auto-Regressive Language Modeling}} in {{Pixel Space}}},
  shorttitle = {{{PIXAR}}},
  author = {Tai, Yintao and Liao, Xiyang and Suglia, Alessandro and Vergari, Antonio},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-08-10},
  abstract = {Recent work showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations. These models are implemented as autoencoders that reconstruct masked patches of rendered text. However, these pixel-based LLMs are limited to discriminative tasks (e.g., classification) and, similar to BERT, cannot be used to generate text. Therefore, they cannot be used for generative tasks such as free-form question answering. In this work, we introduce PIXAR, the first pixel-based autoregressive LLM that performs text generation. Consisting of only a decoder, PIXAR can perform free-form generative tasks while keeping the number of parameters on par with previous encoder-decoder models. Furthermore, we highlight the challenges of generating text as non-noisy images and show this is due to using a maximum likelihood objective. To overcome this problem, we propose an adversarial pretraining stage that improves the readability and accuracy of PIXAR by 8.1 on LAMBADA and 8.5 on bAbI -- making it comparable to GPT-2 on text generation tasks. This paves the way to build open-vocabulary LLMs that operate on perceptual input only and calls into question the necessity of the usual symbolic input representation, i.e., text as (sub)tokens.},
  howpublished = {https://arxiv.org/abs/2401.03321v2},
  langid = {english},
  annotation = {3 citations (Semantic Scholar/arXiv) [2024-08-10]},
  file = {/Users/chenghao/Zotero/storage/7S3TNLMF/Tai et al. - 2024 - PIXAR Auto-Regressive Language Modeling in Pixel Space.pdf}
}

@misc{desai_2020,
  title = {Calibration of {{Pre-trained Transformers}}},
  author = {Desai, Shrey and Durrett, Greg},
  year = {2020},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-08-11},
  abstract = {Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.},
  howpublished = {https://arxiv.org/abs/2003.07892v3},
  langid = {english},
  annotation = {238 citations (Semantic Scholar/arXiv) [2024-08-11]},
  file = {/Users/chenghao/Zotero/storage/YZCKEDDA/Desai and Durrett - 2020 - Calibration of Pre-trained Transformers.pdf}
}

@misc{shyam_2024,
  title = {Tree {{Attention}}: {{Topology-aware Decoding}} for {{Long-Context Attention}} on {{GPU}} Clusters},
  shorttitle = {Tree {{Attention}}},
  author = {Shyam, Vasudev and Pilault, Jonathan and Shepperd, Emily and Anthony, Quentin and Millidge, Beren},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-11},
  abstract = {Self-attention is the core mathematical operation of modern transformer architectures and is also a significant computational bottleneck due to its quadratic complexity in the sequence length. In this work, we derive the scalar energy function whose gradient computes the self-attention block, thus elucidating the theoretical underpinnings of self-attention, providing a Bayesian interpretation of the operation and linking it closely with energy-based models such as Hopfield Networks. Moreover, due to this formulation, we discover that we can use efficient and optimized automatic-differentiation techniques to derive a highly efficient Tree Attention algorithm to compute the gradient of the energy and hence self-attention. Our formulation reveals that the reduction across the sequence axis can be efficiently computed in parallel through a tree reduction. Our algorithm, for parallelizing attention computation across multiple GPUs, enables cross-device decoding to be performed asymptotically faster (up to 8x faster) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2x less peak memory. Our code is publicly available here: {\textbackslash}url\{https://github.com/Zyphra/tree\_attention\}},
  howpublished = {https://arxiv.org/abs/2408.04093v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-08-11]},
  file = {/Users/chenghao/Zotero/storage/2VNY8SHA/Shyam et al. - 2024 - Tree Attention Topology-aware Decoding for Long-Context Attention on GPU clusters.pdf}
}

@misc{_ah,
  title = {A Minimal {{Introduction}} to {{Quantization}} -- Hackerllama},
  urldate = {2024-08-11},
  howpublished = {https://osanseviero.github.io/hackerllama/blog/posts/minimal-quantize-intro/},
  file = {/Users/chenghao/Zotero/storage/Q23ZC4S2/minimal-quantize-intro.html}
}

@misc{_ai,
  title = {A {{Visual Guide}} to {{Quantization}} - by {{Maarten Grootendorst}}},
  urldate = {2024-08-11},
  howpublished = {https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization\#footnote-3-145531349},
  file = {/Users/chenghao/Zotero/storage/QLW3RHW5/a-visual-guide-to-quantization.html}
}

@misc{li_2023,
  title = {Internet {{Explorer}}: {{Targeted Representation Learning}} on the {{Open Web}}},
  shorttitle = {Internet {{Explorer}}},
  author = {Li, Alexander C. and Brown, Ellis and Efros, Alexei A. and Pathak, Deepak},
  year = {2023},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-08-11},
  abstract = {Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30--40 hours. Results, visualizations, and videos at https://internet-explorer-ssl.github.io/},
  howpublished = {https://arxiv.org/abs/2302.14051v2},
  langid = {english},
  annotation = {16 citations (Semantic Scholar/arXiv) [2024-08-11]},
  file = {/Users/chenghao/Zotero/storage/P5ENF5VX/Li et al. - 2023 - Internet Explorer Targeted Representation Learning on the Open Web.pdf}
}
