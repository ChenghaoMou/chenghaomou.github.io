@article{_,
  title = {[{{PDF}}] {{Noise-Robust De-Duplication}} at {{Scale}} {\textbar} {{Semantic Scholar}}},
  doi = {10.48550/arXiv.2210.04261},
  urldate = {2022-11-22},
  abstract = {It is shown that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours, and the neural approaches significantly outperform hashing and N -gram overlap. Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on N -grams. Limited efforts have been made to evaluate how well N -gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and N -gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a ``re-rank'' style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and N -gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. The public release of our NEWS-COPY de-duplication dataset will facilitate further research and applications.},
  langid = {english},
  keywords = {nosource},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-11-21]},
  file = {/Users/chenghao/Zotero/storage/NRPYCGI6/[PDF]_Noise-Robust_De-Duplication_at_Scale_Semantic_Scholar.pdf}
}

@misc{_2022,
  title = {Document {{AI}}: {{Fine-tuning LayoutLM}} for Document-Understanding Using {{Hugging Face Transformers}}},
  shorttitle = {Document {{AI}}},
  year = {2022},
  month = oct,
  journal = {philschmid blog},
  urldate = {2022-10-09},
  abstract = {Learn how to fine-tune LayoutLM for document-understand using Hugging Face Transformers. LayoutLM is a document image understanding and information extraction transformers.},
  howpublished = {https://www.philschmid.de/fine-tuning-layoutlm},
  langid = {english},
  keywords = {nosource}
}

@misc{_a,
  title = {No {{Language Left Behind}}: {{Scaling Human-Centered Machine Translation}} - {{Meta Research}}},
  shorttitle = {No {{Language Left Behind}}},
  journal = {Meta Research},
  urldate = {2022-07-09},
  abstract = {We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over...},
  howpublished = {https://research.facebook.com/publications/no-language-left-behind/},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/SQNN5PQ2/No_Language_Left_Behind.pdf}
}

@misc{_b,
  title = {From {{Deep}} to {{Long Learning}}?},
  urldate = {2023-04-09},
  howpublished = {https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/DA5WIHW2/From_Deep_to_Long_Learning.pdf;/Users/chenghao/Zotero/storage/CBLRFZ2D/2023-03-27-long-learning.html}
}

@misc{_c,
  title = {Natural {{Language Processing}} with {{Transformers}}, {{Revised Edition}} [{{Book}}]},
  urldate = {2023-03-18},
  abstract = {Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or {\dots} - Selection from Natural Language Processing with Transformers, Revised Edition [Book]},
  howpublished = {https://www.oreilly.com/library/view/natural-language-processing/9781098136789/},
  isbn = {9781098136796},
  langid = {english},
  keywords = {/unread}
}

@misc{_d,
  title = {Data {{Statements}}: {{From Technical Concept}} to {{Community Practice}} {\textbar} {{ACM Journal}} on {{Responsible Computing}}},
  urldate = {2023-05-12},
  howpublished = {https://dl.acm.org/doi/10.1145/3594737},
  annotation = {\{"size": 794925, "pages": 17, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/AJ3XXS2K/Data_Statements.pdf}
}

@misc{_e,
  title = {Addon {{Item}}},
  keywords = {/unread}
}

@misc{_f,
  title = {Jobs -- {{Dataproc}} -- Huggingface-Science{\dots} -- {{Google Cloud}} Console - {{https://console.cloud.google.com/dataproc/jobs?project=huggingface-science-codeparrot}}},
  urldate = {2023-10-09},
  howpublished = {https://console.cloud.google.com/dataproc/jobs?project=huggingface-science-codeparrot}
}

@misc{_g,
  title = {Jb225an3grcw5c7ba5tojmivz4-Dot-Us-Central1.Dataproc.Googleusercontent.Com/Gateway/Default/Yarn/Proxy/Application\_1696879436608\_0029/Jobs/Job/?Id=3 - {{https://jb225an3grcw5c7ba5tojmivz4-dot-us-central1.dataproc.googleusercontent.com/gateway/default/yarn/proxy/application\_1696879436608\_0029/jobs/job/?id=3}}},
  urldate = {2023-10-11},
  howpublished = {https://jb225an3grcw5c7ba5tojmivz4-dot-us-central1.dataproc.googleusercontent.com/gateway/default/yarn/proxy/application\_1696879436608\_0029/jobs/job/?id=3}
}

@misc{_h,
  title = {{{SaulLM-7B}}: {{A}} Pioneering {{Large Language Model}} for {{Law}}},
  urldate = {2024-03-07},
  howpublished = {https://arxiv.org/html/2403.03883v1},
  file = {/Users/chenghao/Zotero/storage/Y5VF3F3L/2403.html}
}

@misc{abbas_2023,
  title = {{{SemDeDup}}: {{Data-efficient}} Learning at Web-Scale through Semantic Deduplication},
  shorttitle = {{{SemDeDup}}},
  author = {Abbas, Amro and Tirumala, Kushal and Simig, D{\'a}niel and Ganguli, Surya and Morcos, Ari S.},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09540},
  eprint = {2303.09540},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-17},
  abstract = {Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50\% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-03-17]},
  file = {/Users/chenghao/Zotero/storage/HC4NWGKM/Abbas_et_al_(2023)_SemDeDup.pdf}
}

@misc{abercrombie_2023,
  title = {Mirages: {{On Anthropomorphism}} in {{Dialogue Systems}}},
  shorttitle = {Mirages},
  author = {Abercrombie, Gavin and Curry, Amanda Cercas and Dinkar, Tanvi and Talat, Zeerak},
  year = {2023},
  month = may,
  number = {arXiv:2305.09800},
  eprint = {2305.09800},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.09800},
  urldate = {2023-05-30},
  abstract = {Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-05-29] 0 citations (Semantic Scholar/DOI) [2023-05-29]},
  file = {/Users/chenghao/Zotero/storage/DJ92DHUC/Abercrombie_et_al_(2023)_Mirages.pdf}
}

@article{aghajanyan_2021,
  title = {{{HTLM}}: {{Hyper-Text Pre-Training}} and {{Prompting}} of {{Language Models}}},
  author = {Aghajanyan, Armen and Okhonko, Dmytro and Lewis, Mike and Joshi, Mandar and Xu, Hu and Ghosh, Gargi and Zettlemoyer, Luke},
  year = {2021},
  journal = {arXiv},
  abstract = {We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling title tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research.},
  keywords = {hypter-text language model,nlp,nosource,prompting}
}

@article{aghajanyan_2021a,
  title = {Muppet: {{Massive Multi-task Representations}} with {{Pre-Finetuning}}},
  author = {Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
  year = {2021},
  journal = {arXiv},
  abstract = {We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.{\textbackslash}textbackslashtextasciitildeRoBERTa) and generation models (e.g.{\textbackslash}textbackslashtextasciitildeBART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.},
  keywords = {nosource}
}

@misc{ainslie_2023,
  title = {{{CoLT5}}: {{Faster Long-Range Transformers}} with {{Conditional Computation}}},
  shorttitle = {{{CoLT5}}},
  author = {Ainslie, Joshua and Lei, Tao and {de Jong}, Michiel and Onta{\~n}{\'o}n, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and {Lee-Thorp}, James and Tay, Yi and Sung, Yun-Hsuan and Sanghai, Sumit},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09752},
  eprint = {2303.09752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.09752},
  urldate = {2023-03-20},
  abstract = {Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/XC3BHKTH/Ainslie et al. - 2023 - CoLT5 Faster Long-Range Transformers with Conditi.pdf}
}

@misc{akiki_2022,
  title = {{{BigScience}}: {{A Case Study}} in the {{Social Construction}} of a {{Multilingual Large Language Model}}},
  shorttitle = {{{BigScience}}},
  author = {Akiki, Christopher and Pistilli, Giada and Mieskes, Margot and Gall{\'e}, Matthias and Wolf, Thomas and Ili{\'c}, Suzana and Jernite, Yacine},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04960},
  eprint = {2212.04960},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.04960},
  urldate = {2022-12-12},
  abstract = {The BigScience Workshop was a value-driven initiative that spanned one and half years of interdisciplinary research and culminated in the creation of ROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the largest multilingual language models to date. In addition to the technical outcomes and artifacts, the workshop fostered multidisciplinary collaborations around large models, datasets, and their analysis. This in turn led to a wide range of research publications spanning topics from ethics to law, data governance, modeling choices and distributed training. This paper focuses on the collaborative research aspects of BigScience and takes a step back to look at the challenges of large-scale participatory research, with respect to participant diversity and the tasks required to successfully carry out such a project. Our main goal is to share the lessons we learned from this experience, what we could have done better and what we did well. We show how the impact of such a social approach to scientific research goes well beyond the technical artifacts that were the basis of its inception.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/chenghao/Zotero/storage/7MIQN5U7/Akiki_et_al_(2022)_BigScience.pdf}
}

@misc{alizadeh_2023,
  title = {{{LLM}} in a Flash: {{Efficient Large Language Model Inference}} with {{Limited Memory}}},
  shorttitle = {{{LLM}} in a Flash},
  author = {Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C. and Rastegari, Mohammad and Farajtabar, Mehrdad},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11514},
  eprint = {2312.11514},
  publisher = {arXiv},
  urldate = {2023-12-20},
  abstract = {Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, "windowing'" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 1092689, "pages": 13, "previous": "arXiv:2312.11514 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/6B4NDQKQ/Alizadeh_et_al_(2023)_LLM_in_a_flash.pdf}
}

@misc{allal_2023,
  title = {{{SantaCoder}}: Don't Reach for the Stars!},
  shorttitle = {{{SantaCoder}}},
  author = {Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and Umapathi, Logesh Kumar and Anderson, Carolyn Jane and Zi, Yangtian and Poirier, Joel Lamy and Schoelkopf, Hailey and Troshin, Sergey and Abulkhanov, Dmitry and Romero, Manuel and Lappert, Michael and De Toni, Francesco and {del R{\'i}o}, Bernardo Garc{\'i}a and Liu, Qian and Bose, Shamik and Bhattacharyya, Urvashi and Zhuo, Terry Yue and Yu, Ian and Villegas, Paulo and Zocca, Marco and Mangrulkar, Sourab and Lansky, David and Nguyen, Huu and Contractor, Danish and Villa, Luis and Li, Jia and Bahdanau, Dzmitry and Jernite, Yacine and Hughes, Sean and Fried, Daniel and Guha, Arjun and {de Vries}, Harm and {von Werra}, Leandro},
  year = {2023},
  month = feb,
  number = {arXiv:2301.03988},
  eprint = {2301.03988},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.03988},
  urldate = {2023-03-16},
  abstract = {The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Software Engineering},
  annotation = {4 citations (Semantic Scholar/arXiv) [2023-03-15] 4 citations (Semantic Scholar/DOI) [2023-03-15]},
  file = {/Users/chenghao/Zotero/storage/YE63DMSI/Allal_et_al_(2023)_SantaCoder.pdf}
}

@misc{andreas_2022,
  title = {Language {{Models}} as {{Agent Models}}},
  author = {Andreas, Jacob},
  year = {2022},
  month = dec,
  number = {arXiv:2212.01681},
  eprint = {2212.01681},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.01681},
  urldate = {2022-12-09},
  abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Multiagent Systems},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-09] 0 citations (Semantic Scholar/DOI) [2022-12-09]},
  file = {/Users/chenghao/Zotero/storage/FUVKUZJR/Andreas_(2022)_Language_Models_as_Agent_Models.pdf}
}

@article{anonymous_2022,
  title = {{{ERNIE-Layout}}: {{Layout-Knowledge Enhanced Multi-modal Pre-training}} for {{Document Understanding}}},
  shorttitle = {{{ERNIE-Layout}}},
  author = {Anonymous},
  year = {2022},
  month = jan,
  urldate = {2022-10-14},
  abstract = {We propose ERNIE-Layout, a knowledge enhanced pre-training approach for visual document understanding, which incorporates layout-knowledge into the pre-training of visual document understanding to learn a better joint multi-modal representation of text, layout and image. Previous works directly model serialized tokens from documents according to a raster-scan order, neglecting the importance of the reading order of documents, leading to sub-optimal performance. We incorporate layout-knowledge from Document-Parser into document pre-training, which is used to rearrange the tokens following an order more consistent with human reading habits. And we propose the Reading Order Prediction (ROP) task to enhance the interactions within segments and correlation between segments and a fine-grained cross-modal alignment pre-training task named Replaced Regions Prediction (RRP). ERNIE-Layout attempts to fuse textual and visual features in a unified Transformer model, which is based on our newly proposed spatial-aware disentangled attention mechanism. ERNIE-Layout achieves superior performance on various document understanding tasks, setting new SOTA for four tasks, including information extraction, document classification, document question answering.},
  langid = {english},
  annotation = {GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/TEGPI9JD/Anonymous_(2022)_ERNIE-Layout.pdf;/Users/chenghao/Zotero/storage/KM8IWTY8/forum.html}
}

@article{anonymous_2022a,
  title = {Layout-{{Aware Neural Model}} for {{Resolving Hierarchical Table Structure}}},
  author = {Anonymous},
  year = {2022},
  month = jan,
  urldate = {2022-11-04},
  abstract = {While many pipelines for extracting information from tables assume simple table structure, tables in the financial domain frequently have complex, hierarchical structure. The main example would be parent-child relationships between header cells. Most prior datasets of tables annotated from images or .pdf and most models for extracting table structure concentrate on the problems of table, cell, row, and column bounding box extraction. The area of fine-grained table structure remains relatively unexplored. In this study, we present a dataset of 887 tables, manually labeled for cell types and column hierarchy relations. The tables are selected from IBM FinTabNet, a much larger dataset of more than 100,000 financial tables having cell, row, and column bounding boxes extracted by deep learning, but not including semantic cell type or cell-to-cell relation labels, which we add. Selection of these 887 tables is performed using heuristics which result in a much larger proportion, roughly half, of the selected tables having complex hierarchical structure, than a random sample from FinTabNet. Further, we fine-tune models based on LayoutLM on the cell-type classification task and on the identification of hiearchical relations among column headers. We achieve F1 scores of 95\% and 70\% on the respective tasks. Finally, we use the trained model to create soft labels for the entirety of FinTabNet.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/4FWQSQA4/Anonymous_(2022)_Layout-Aware_Neural_Model_for_Resolving_Hierarchical_Table_Structure.pdf;/Users/chenghao/Zotero/storage/SWFQ2AFB/forum.html}
}

@article{appalaraju_2021,
  title = {{{DocFormer}}: {{End-to-End Transformer}} for {{Document Understanding}}},
  author = {Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R},
  year = {2021},
  journal = {arXiv},
  abstract = {We present DocFormer -- a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).},
  keywords = {document,nlp,nosource,transformer}
}

@misc{arora_2024,
  title = {Simple Linear Attention Language Models Balance the Recall-Throughput Tradeoff},
  author = {Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-03-05},
  abstract = {Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.},
  howpublished = {https://arxiv.org/abs/2402.18668v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-03-05]},
  file = {/Users/chenghao/Zotero/storage/KFBTS9SX/Arora_et_al_(2024)_Simple_linear_attention_language_models_balance_the_recall-throughput_tradeoff.pdf}
}

@misc{austin_2021,
  title = {Program {{Synthesis}} with {{Large Language Models}}},
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  year = {2021},
  month = aug,
  number = {arXiv:2108.07732},
  eprint = {2108.07732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07732},
  urldate = {2023-03-21},
  abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Machine Learning,Computer Science - Programming Languages},
  annotation = {177 citations (Semantic Scholar/arXiv) [2023-03-20]},
  file = {/Users/chenghao/Zotero/storage/LXCEL3YS/Austin et al. - 2021 - Program Synthesis with Large Language Models.pdf}
}

@article{bai_,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and {Telleen-Lawton}, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through selfimprovement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as `Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use `RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but nonevasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/4ULNGVNR/Bai_et_al_(Constitutional_AI.pdf}
}

@misc{bang_2023,
  title = {A {{Multitask}}, {{Multilingual}}, {{Multimodal Evaluation}} of {{ChatGPT}} on {{Reasoning}}, {{Hallucination}}, and {{Interactivity}}},
  author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04023},
  eprint = {2302.04023},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.04023},
  urldate = {2023-02-10},
  abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 21 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 64.33\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-02-09]},
  file = {/Users/chenghao/Zotero/storage/BZGLV6VK/Bang_et_al_(2023)_A_Multitask,_Multilingual,_Multimodal_Evaluation_of_ChatGPT_on_Reasoning,.pdf}
}

@article{banino_2021,
  title = {{{PonderNet}}: {{Learning}} to {{Ponder}}},
  shorttitle = {{{PonderNet}}},
  author = {Banino, Andrea and Balaguer, Jan and Blundell, Charles},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.05407 [cs]},
  eprint = {2107.05407},
  primaryclass = {cs},
  urldate = {2021-08-14},
  abstract = {In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Machine Learning,machine learning,training},
  annotation = {11 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/VFVDIQAN/Banino_et_al_(2021)_PonderNet.pdf}
}

@article{baydin_2022,
  title = {Gradients without {{Backpropagation}}},
  author = {Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Pearlmutter, Barak A. and Syme, Don and Wood, Frank and Torr, Philip},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.08587 [cs, stat]},
  eprint = {2202.08587},
  primaryclass = {cs, stat},
  urldate = {2022-02-21},
  abstract = {Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differentiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one can compute exactly and efficiently via the forward mode. We call this formulation the forward gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for backpropagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training up to twice as fast in some cases.},
  archiveprefix = {arxiv},
  keywords = {68T07,algorithms,backpropagation,Computer Science - Machine Learning,I.2.5,I.2.6,neural networks,Statistics - Machine Learning},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/XI5P88B6/Baydin_et_al_(2022)_Gradients_without_Backpropagation.pdf}
}

@misc{belcak_2023,
  title = {Exponentially {{Faster Language Modelling}}},
  author = {Belcak, Peter and Wattenhofer, Roger},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10770},
  eprint = {2311.10770},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.10770},
  urldate = {2023-12-09},
  abstract = {Language models only really need to use an exponential fraction of their neurons for individual inferences. As proof, we present UltraFastBERT, a BERT variant that uses 0.3\% of its neurons during inference while performing on par with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095 neurons for each layer inference. This is achieved by replacing feedforward networks with fast feedforward networks (FFFs). While no truly efficient implementation currently exists to unlock the full acceleration potential of conditional neural execution, we provide high-level CPU code achieving 78x speedup over the optimized baseline feedforward implementation, and a PyTorch implementation delivering 40x speedup over the equivalent batched feedforward inference. We publish our training code, benchmarking setup, and model weights.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Belcak_Wattenhofer/2023/Belcak_Wattenhofer_(2023)_Exponentially_Faster_Language_Modelling.pdf}
}

@inproceedings{bender_2020,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  shorttitle = {Climbing towards {{NLU}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bender, Emily M. and Koller, Alexander},
  year = {2020},
  month = jul,
  pages = {5185--5198},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.463},
  urldate = {2022-08-21},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as ``understanding'' language or capturing ``meaning''. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of ``Taking Stock of Where We've Been and Where We're Going'', we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  annotation = {267 citations (Semantic Scholar/DOI) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/MVXHHAGL/Bender_Koller_(2020)_Climbing_towards_NLU.pdf}
}

@inproceedings{bender_2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? \&\#x1f99c;},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  urldate = {2023-03-05},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  annotation = {958 citations (Semantic Scholar/DOI) [2023-03-05]},
  file = {/Users/chenghao/Zotero/storage/YU57XQY7/Bender_et_al_(2021)_On_the_Dangers_of_Stochastic_Parrots.pdf}
}

@article{berglund_,
  title = {The {{Reversal Curse}}: {{LLMs}} Trained on ``{{A}} Is {{B}}'' Fail to Learn ``{{B}} Is {{A}}''},
  author = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  abstract = {We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ``A is B'', it will not automatically generalize to the reverse direction ``B is A''. This is the Reversal Curse. For instance, if a model is trained on ``Olaf Scholz was the ninth Chancellor of Germany'', it will not automatically be able to answer the question, ``Who was the ninth Chancellor of Germany?''. Moreover, the likelihood of the correct answer (``Olaf Scholz'') will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if ``A is B'' occurs, ``B is A'' is more likely to occur).},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/VKV6MA44/Berglund et al. - The Reversal Curse LLMs trained on “A is B” fail to learn “B is A”.pdf}
}

@misc{bertsch_2023,
  title = {Unlimiformer: {{Long-Range Transformers}} with {{Unlimited Length Input}}},
  shorttitle = {Unlimiformer},
  author = {Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew R.},
  year = {2023},
  month = may,
  number = {arXiv:2305.01625},
  eprint = {2305.01625},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01625},
  urldate = {2023-05-03},
  abstract = {Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single \$k\$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-\$k\$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/2BNNLJIG/Bertsch_et_al_(2023)_Unlimiformer.pdf}
}

@article{biderman_2021,
  title = {Pitfalls in {{Machine Learning Research}}: {{Reexamining}} the {{Development Cycle}}},
  shorttitle = {Pitfalls in {{Machine Learning Research}}},
  author = {Biderman, Stella and Scheirer, Walter J.},
  year = {2021},
  month = aug,
  journal = {arXiv:2011.02832 [cs, stat]},
  eprint = {2011.02832},
  primaryclass = {cs, stat},
  urldate = {2021-08-27},
  abstract = {Machine learning has the potential to fuel further advances in data science, but it is greatly hindered by an ad hoc design process, poor data hygiene, and a lack of statistical rigor in model evaluation. Recently, these issues have begun to attract more attention as they have caused public and embarrassing issues in research and development. Drawing from our experience as machine learning researchers, we follow the machine learning process from algorithm design to data collection to model evaluation, drawing attention to common pitfalls and providing practical recommendations for improvements. At each step, case studies are introduced to highlight how these pitfalls occur in practice, and where things could be improved.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,development,machine learning,pitafalls,pitfalls,Statistics - Machine Learning,Statistics - Methodology},
  annotation = {5 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2011.02832 [cs, stat]/2021/biderman_scheirer_2021_pitfalls_in_machine_learning_research_-_reexamining_the_development_cycle2.pdf}
}

@misc{biderman_2023,
  title = {Pythia: {{A Suite}} for {{Analyzing Large Language Models Across Training}} and {{Scaling}}},
  shorttitle = {Pythia},
  author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and {van der Wal}, Oskar},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01373},
  eprint = {2304.01373},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.01373},
  urldate = {2023-04-25},
  abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/JP79AKQG/Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Model.pdf}
}

@misc{birhane_2023,
  title = {On {{Hate Scaling Laws For Data-Swamps}}},
  author = {Birhane, Abeba and Prabhu, Vinay and Han, Sang and Boddeti, Vishnu Naresh},
  year = {2023},
  month = jun,
  number = {arXiv:2306.13141},
  eprint = {2306.13141},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.13141},
  urldate = {2023-07-03},
  abstract = {`Scale the model, scale the data, scale the GPU-farms' is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts remain under explored. This is especially of critical importance in the context of visio-linguistic datasets whose main source is the World Wide Web, condensed and packaged as the CommonCrawl dump. This large scale data-dump, which is known to have numerous drawbacks, is repeatedly mined and serves as the data-motherlode for large generative models. In this paper, we: 1) investigate the effect of scaling datasets on hateful content through a comparative audit of the LAION-400M and LAION-2B-en, containing 400 million and 2 billion samples respectively, and 2) evaluate the downstream impact of scale on visio-linguistic models trained on these dataset variants by measuring racial bias of the models trained on them using the Chicago Face Dataset (CFD) as a probe. Our results show that 1) the presence of hateful content in datasets, when measured with a Hate Content Rate (HCR) metric on the inferences of the Pysentimiento hate-detection Natural Language Processing (NLP) model, increased by nearly \$12{\textbackslash}\%\$ and 2) societal biases and negative stereotypes were also exacerbated with scale on the models we evaluated. As scale increased, the tendency of the model to associate images of human faces with the `human being' class over 7 other offensive classes reduced by half. Furthermore, for the Black female category, the tendency of the model to associate their faces with the `criminal' class doubled, while quintupling for Black male faces. We present a qualitative and historical analysis of the model audit results, reflect on our findings and its implications for dataset curation practice, and close with a summary of our findings and potential future work to be done in this area.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computers and Society},
  annotation = {\{"size": 6239992, "pages": 27, "previous": "0 citations (Semantic Scholar/arXiv) [2023-07-03]{\textbackslash}n0 citations (Semantic Scholar/DOI) [2023-07-03]{\textbackslash}narXiv:2306.13141 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/CXGZHNFJ/Birhane_et_al_(2023)_On_Hate_Scaling_Laws_For_Data-Swamps.pdf}
}

@misc{birhane_2024,
  title = {{{AI}} Auditing: {{The Broken Bus}} on the {{Road}} to {{AI Accountability}}},
  shorttitle = {{{AI}} Auditing},
  author = {Birhane, Abeba and Steed, Ryan and Ojewale, Victor and Vecchione, Briana and Raji, Inioluwa Deborah},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-31},
  abstract = {One of the most concrete measures to take towards meaningful AI accountability is to consequentially assess and report the systems' performance and impact. However, the practical nature of the "AI audit" ecosystem is muddled and imprecise, making it difficult to work through various concepts and map out the stakeholders involved in the practice. First, we taxonomize current AI audit practices as completed by regulators, law firms, civil society, journalism, academia, consulting agencies. Next, we assess the impact of audits done by stakeholders within each domain. We find that only a subset of AI audit studies translate to desired accountability outcomes. We thus assess and isolate practices necessary for effective AI audit results, articulating the observed connections between AI audit design, methodology and institutional context on its effectiveness as a meaningful mechanism for accountability.},
  howpublished = {https://arxiv.org/abs/2401.14462v1},
  langid = {english},
  annotation = {\{"size": 562029, "pages": 33, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/DFFGJHRG/Birhane_et_al_(2024)_AI_auditing.pdf}
}

@article{biten_2022,
  title = {{{OCR-IDL}}: {{OCR Annotations}} for {{Industry Document Library Dataset}}},
  shorttitle = {{{OCR-IDL}}},
  author = {Biten, Ali Furkan and Tito, Rub{\`e}n P{\'e}rez and G{\'o}mez, Llu{\'i}s and Valveny, Ernest and Karatzas, Dimosthenis},
  year = {2022},
  journal = {ArXiv},
  abstract = {This work makes public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. Pretraining has proven successful in Document Intelligence tasks where deluge of documents are used to pretrain the models only later to be finetuned on downstream tasks. One of the problems of the pretraining approaches is the inconsistent usage of pretraining data with different OCR engines leading to incomparable results between models. In other words, it is not obvious whether the performance gain is coming from diverse usage of amount of data and distinct OCR engines or from the proposed models. To remedy the problem, we make public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. The contributed dataset (OCR-IDL) has an estimated monetary value over 20K US\$. It is our hope that OCR-IDL can be a starting point for future works on Document Intelligence. All of our data and its collection process with the annotations can be found in https://github.com/furkanbiten/idl\_data.},
  annotation = {GSCC: 0000003},
  file = {/Users/chenghao/Zotero/storage/6L39XAAA/Biten_et_al_(2022)_OCR-IDL.pdf}
}

@article{bohnet_2022,
  title = {Attributed {{Question Answering}}: {{Evaluation}} and {{Modeling}} for {{Attributed Large Language Models}}},
  shorttitle = {Attributed {{Question Answering}}},
  author = {Bohnet, Bernd and Tran, Vinh Q. and Verga, Pat and Aharoni, Roee and Andor, Daniel and Soares, Livio Baldini and Eisenstein, Jacob and Ganchev, Kuzman and Herzig, Jonathan and Hui, Kai and Kwiatkowski, Tom and Ma, Ji and Ni, Jianmo and Schuster, Tal and Cohen, William W. and Collins, Michael and Das, Dipanjan and Metzler, Donald and Petrov, Slav and Webster, Kellie},
  year = {2022},
  month = dec,
  doi = {10.48550/arXiv.2212.08037},
  urldate = {2022-12-18},
  abstract = {Large language models (LLMs) have shown impressive results across a variety of tasks while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial for both system developers and users in this setting. We propose and study Attributed QA as a key first step in the development of attributed LLMs. We develop a reproducable evaluation framework for the task, using human annotations as a gold standard and a correlated automatic metric that we show is suitable for development settings. We describe and benchmark a broad set of architectures for the task. Our contributions give some concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third key question (How to build LLMs with attribution?).},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-17]},
  file = {/Users/chenghao/Zotero/storage/UX9BWF25/Bohnet_et_al_(2022)_Attributed_Question_Answering.pdf}
}

@article{borchmann_2021,
  title = {{{DUE}}: {{End-to-End Document Understanding Benchmark}}},
  shorttitle = {{{DUE}}},
  author = {Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Stanislawek, Tomasz and Jurkiewicz, Dawid and Turski, Micha{\l} and Szyndler, Karolina and Grali{\'n}ski, Filip},
  year = {2021},
  month = dec,
  journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  volume = {1},
  urldate = {2022-10-07},
  langid = {english},
  annotation = {GSCC: 0000079},
  file = {/Users/chenghao/Zotero/storage/CDD73SFD/Borchmann_et_al_(2021)_DUE.pdf}
}

@inproceedings{bowman_2021,
  title = {What {{Will}} It {{Take}} to {{Fix Benchmarking}} in {{Natural Language Understanding}}?},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Bowman, Samuel R. and Dahl, George},
  year = {2021},
  month = jun,
  pages = {4843--4855},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.385},
  urldate = {2022-02-20},
  abstract = {Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.},
  keywords = {benchmark,bias,dataset,nlp},
  annotation = {35 citations (Semantic Scholar/DOI) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/Association for Computational Linguistics/2021/Bowman_Dahl_2021_What Will it Take to Fix Benchmarking in Natural Language Understanding.pdf}
}

@misc{bowman_2022,
  title = {The {{Dangers}} of {{Underclaiming}}: {{Reasons}} for {{Caution When Reporting How NLP Systems Fail}}},
  shorttitle = {The {{Dangers}} of {{Underclaiming}}},
  author = {Bowman, Samuel R.},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08300},
  eprint = {2110.08300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08300},
  urldate = {2023-02-13},
  abstract = {Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/DCDGCVID/Bowman - 2022 - The Dangers of Underclaiming Reasons for Caution .pdf}
}

@misc{bowman_2023,
  title = {Eight {{Things}} to {{Know}} about {{Large Language Models}}},
  author = {Bowman, Samuel R.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.00612},
  eprint = {2304.00612},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.00612},
  urldate = {2023-04-11},
  abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/MVAMVATB/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf}
}

@article{breiman_,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  author = {Breiman, Leo},
  journal = {THE TWO CULTURES},
  pages = {33},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  langid = {english},
  annotation = {\{"size": 300414, "pages": 33, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/PXPNIHWQ/Breiman_(Statistical_Modeling.pdf}
}

@misc{bsharat_2023,
  title = {Principled {{Instructions Are All You Need}} for {{Questioning LLaMA-1}}/2, {{GPT-3}}.5/4},
  author = {Bsharat, Sondos Mahmoud and Myrzakhan, Aidar and Shen, Zhiqiang},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-02-05},
  abstract = {This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.},
  howpublished = {https://arxiv.org/abs/2312.16171v2},
  langid = {english},
  annotation = {\{"size": 1577522, "pages": 26, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/XSNKWPLE/Bsharat et al. - 2023 - Principled Instructions Are All You Need for Questioning LLaMA-12, GPT-3.54.pdf}
}

@misc{bulatov_2022,
  title = {Recurrent {{Memory Transformer}}},
  author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S.},
  year = {2022},
  month = dec,
  number = {arXiv:2207.06881},
  eprint = {2207.06881},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.06881},
  urldate = {2023-04-30},
  abstract = {Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {4 citations (Semantic Scholar/arXiv) [2023-04-30] 4 citations (Semantic Scholar/DOI) [2023-04-30]},
  file = {/Users/chenghao/Zotero/storage/XPWWA6U3/Bulatov_et_al_(2022)_Recurrent_Memory_Transformer.pdf}
}

@misc{bulatov_2023,
  title = {Scaling {{Transformer}} to {{1M}} Tokens and beyond with {{RMT}}},
  author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11062},
  eprint = {2304.11062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11062},
  urldate = {2023-04-25},
  abstract = {This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/YNBRHW5E/Bulatov et al. - 2023 - Scaling Transformer to 1M tokens and beyond with R.pdf}
}

@misc{burns_2023,
  title = {A {{Suite}} of {{Generative Tasks}} for {{Multi-Level Multimodal Webpage Understanding}}},
  author = {Burns, Andrea and Srinivasan, Krishna and Ainslie, Joshua and Brown, Geoff and Plummer, Bryan A. and Saenko, Kate and Ni, Jianmo and Guo, Mandy},
  year = {2023},
  month = may,
  number = {arXiv:2305.03668},
  eprint = {2305.03668},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03668},
  urldate = {2023-05-12},
  abstract = {Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Experiments show that the new annotations from WikiWeb2M improve task performance compared to data from prior work. We also include ablations on sequence length, input features, and model size.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 7675000, "pages": 25, "previous": "arXiv:2305.03668 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/HV9P5NZM/Burns et al. - 2023 - A Suite of Generative Tasks for Multi-Level Multim.pdf}
}

@misc{carlini_2022,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  year = {2022},
  month = feb,
  number = {arXiv:2202.07646},
  eprint = {2202.07646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.07646},
  urldate = {2022-08-21},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {28 citations (Semantic Scholar/arXiv) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/NIMTPH9M/Carlini_et_al_(2022)_Quantifying_Memorization_Across_Neural_Language_Models.pdf}
}

@article{chalkidis_2022,
  title = {{{LexGLUE}}: {{A Benchmark Dataset}} for {{Legal Language Understanding}} in {{English}}},
  shorttitle = {{{LexGLUE}}},
  author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel Martin and Aletras, Nikolaos},
  year = {2022},
  month = mar,
  journal = {arXiv:2110.00976 [cs]},
  eprint = {2110.00976},
  primaryclass = {cs},
  urldate = {2022-03-26},
  abstract = {Laws and their interpretations, legal arguments and agreements{\textbackslash} are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.},
  archiveprefix = {arxiv},
  keywords = {benchmark,Computer Science - Computation and Language,dataset,legal,nlp},
  annotation = {7 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/ZW7HUSWM/Chalkidis_et_al_(2022)_LexGLUE.pdf}
}

@article{chen_2022,
  title = {{{XDoc}}: {{Unified Pre-training}} for {{Cross-Format Document Understanding}}},
  shorttitle = {{{XDoc}}},
  author = {Chen, Jingye and Lv, Tengchao and Cui, Lei and Zhang, Changrong and Wei, Furu},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2210.02849},
  abstract = {XDoc is proposed, a unified pre-trained model which deals with different document formats in a single model, which is cost effective for real-world deployment and shares backbone parameters for different formats such as the word embedding layer and the Transformer layers. The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance, existing pre-trained models usually target one specific document format at one time, making it difficult to combine knowledge from multiple document formats. To address this, we propose XDoc, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7\% parameters, XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. The code and pre-trained models will be publicly available at https://aka.ms/xdoc.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/H7WN7YIJ/Chen_et_al_(2022)_XDoc.pdf;/Users/chenghao/Zotero/storage/B4Y6AKRX/e5b03c3165d3a056dc55ce18835be04f5f817f4b.html}
}

@misc{chen_2023,
  title = {Extending {{Context Window}} of {{Large Language Models}} via {{Positional Interpolation}}},
  author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  year = {2023},
  month = jun,
  number = {arXiv:2306.15595},
  doi = {10.48550/arXiv.2306.15595},
  urldate = {2023-07-17},
  abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least \${\textbackslash}sim 600 {\textbackslash}times\$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 751217, "pages": 18, "previous": "1 citations (Semantic Scholar/arXiv) [2023-07-17]{\textbackslash}n1 citations (Semantic Scholar/DOI) [2023-07-17]{\textbackslash}narXiv:2306.15595 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/LEMWM4WI/Chen et al. - 2023 - Extending Context Window of Large Language Models via Positional Interpolation.pdf}
}

@misc{chen_2024,
  title = {Orion-{{14B}}: {{Open-source Multilingual Large Language Models}}},
  shorttitle = {Orion-{{14B}}},
  author = {Chen, Du and Huang, Yi and Li, Xiaopu and Li, Yongqiang and Liu, Yongqiang and Pan, Haihui and Xu, Leichao and Zhang, Dacheng and Zhang, Zhipeng and Han, Kun},
  year = {2024},
  month = jan,
  number = {arXiv:2401.12246},
  eprint = {2401.12246},
  publisher = {arXiv},
  urldate = {2024-01-24},
  abstract = {In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 411856, "pages": 19, "previous": "arXiv:2401.12246 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/QFKMKFMV/Chen et al. - 2024 - Orion-14B Open-source Multilingual Large Language Models.pdf}
}

@article{cheng_2022,
  title = {{{TRIE}}++: {{Towards End-to-End Information Extraction}} from {{Visually Rich Documents}}},
  shorttitle = {{{TRIE}}++},
  author = {Cheng, Zhanzhan and Zhang, Peng and Li, Can and Liang, Qiao and Xu, Yunlu and Li, Pengfei and Pu, Shiliang and Niu, Yi and Wu, Fei},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2207.06744},
  abstract = {A end-to-end information extraction framework from visually rich documents, where text reading and information extraction can reinforce each other via a well-designed multi-modal context block is proposed. ---Recently, automatically extracting information from visually rich documents ( e.g., tickets and resumes) has become a hot and vital research topic due to its widespread commercial value. Most existing methods divide this task into two subparts: the text reading part for obtaining the plain text from the original document images and the information extraction part for extracting key contents. These methods mainly focus on improving the second, while neglecting that the two parts are highly correlated. This paper proposes a unified end-to-end information extraction framework from visually rich documents, where text reading and information extraction can reinforce each other via a well-designed multi-modal context block. Specifically, the text reading part provides multi-modal features like visual, textual and layout features. The multi-modal context block is developed to fuse the generated multi-modal features and even the prior knowledge from the pre-trained language model for better semantic representation. The information extraction part is responsible for generating key contents with the fused context features. The framework can be trained in an end-to-end trainable manner, achieving global optimization. What is more, we define and group visually rich documents into four categories across two dimensions, the layout and text type. For each document category, we provide or recommend the corresponding benchmarks, experimental settings and strong baselines for remedying the problem that this research area lacks the uniform evaluation standard. Extensive experiments on four kinds of benchmarks (from fixed layout to variable layout, from full-structured text to semi-unstructured text) are reported, demonstrating the proposed method's effectiveness. Data, source code and models are available.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/6AHMIQYY/Cheng_et_al_(2022)_TRIE++.pdf}
}

@article{choshen_2022,
  title = {Fusing Finetuned Models for Better Pretraining},
  author = {Choshen, Leshem and Venezian, Elad and Slonim, Noam and Katz, Yoav},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.03044 [cs]},
  eprint = {2204.03044},
  primaryclass = {cs},
  urldate = {2022-04-12},
  abstract = {Pretrained models are the standard starting point for training. This approach consistently outperforms the use of a random initialization. However, pretraining is a costly endeavour that few can undertake. In this paper, we create better base models at hardly any cost, by fusing multiple existing fine tuned models into one. Specifically, we fuse by averaging the weights of these models. We show that the fused model results surpass the pretrained model ones. We also show that fusing is often better than intertraining. We find that fusing is less dependent on the target task. Furthermore, weight decay nullifies intertraining effects but not those of fusing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,finetuning,nlp,pretraining},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-12]},
  file = {/Users/chenghao/Zotero/storage/P2QF3RNC/Choshen_et_al_(2022)_Fusing_finetuned_models_for_better_pretraining.pdf}
}

@misc{chowdhery_2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and {Gur-Ari}, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and {Meier-Hellstern}, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  year = {2022},
  month = oct,
  number = {arXiv:2204.02311},
  eprint = {2204.02311},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.02311},
  urldate = {2023-03-18},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {642 citations (Semantic Scholar/arXiv) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/K8IZGNU6/Chowdhery_et_al_(2022)_PaLM.pdf}
}

@misc{chuang_2023,
  title = {{{DoLa}}: {{Decoding}} by {{Contrasting Layers Improves Factuality}} in {{Large Language Models}}},
  shorttitle = {{{DoLa}}},
  author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2023-11-28},
  abstract = {Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17\% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.},
  howpublished = {https://arxiv.org/abs/2309.03883v1},
  langid = {english},
  annotation = {\{"size": 671214, "pages": 23, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/4INC6AZ8/Chuang et al. - 2023 - DoLa Decoding by Contrasting Layers Improves Factuality in Large Language Models.pdf}
}

@article{clark_2021,
  title = {{{CANINE}}: {{Pre-training}} an {{Efficient Tokenization-Free Encoder}} for {{Language Representation}}},
  author = {Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
  year = {2021},
  journal = {arXiv},
  abstract = {Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences, without explicit tokenization or vocabulary, and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28\% fewer model parameters.},
  keywords = {nosource}
}

@article{csanyi_2021,
  title = {Challenges and {{Open Problems}} of {{Legal Document Anonymization}}},
  author = {Cs{\'a}nyi, Gergely M{\'a}rk and Nagy, D{\'a}niel and V{\'a}gi, Ren{\'a}t{\'o} and Vad{\'a}sz, J{\'a}nos P{\'a}l and Orosz, Tam{\'a}s},
  year = {2021},
  month = aug,
  journal = {Symmetry},
  volume = {13},
  number = {8},
  pages = {1490},
  issn = {2073-8994},
  doi = {10.3390/sym13081490},
  urldate = {2023-07-05},
  abstract = {Data sharing is a central aspect of judicial systems. The openly accessible documents can make the judiciary system more transparent. On the other hand, the published legal documents can contain much sensitive information about the involved persons or companies. For this reason, the anonymization of these documents is obligatory to prevent privacy breaches. General Data Protection Regulation (GDPR) and other modern privacy-protecting regulations have strict definitions of private data containing direct and indirect identifiers. In legal documents, there is a wide range of attributes regarding the involved parties. Moreover, legal documents can contain additional information about the relations between the involved parties and rare events. Hence, the personal data can be represented by a sparse matrix of these attributes. The application of Named Entity Recognition methods is essential for a fair anonymization process but is not enough. Machine learning-based methods should be used together with anonymization models, such as differential privacy, to reduce re-identification risk. On the other hand, the information content (utility) of the text should be preserved. This paper aims to summarize and highlight the open and symmetrical problems from the fields of structured and unstructured text anonymization. The possible methods for anonymizing legal documents discussed and illustrated by case studies from the Hungarian legal practice.},
  langid = {english},
  keywords = {/unread},
  annotation = {\{"size": 1209150, "pages": 25, "previous": "18 citations (Semantic Scholar/DOI) [2023-07-06]"\}},
  file = {/Users/chenghao/Zotero/storage/ZUMCQBD6/Csányi_et_al_(2021)_Challenges_and_Open_Problems_of_Legal_Document_Anonymization.pdf}
}

@article{cui_2021,
  title = {Document {{AI}}: {{Benchmarks}}, {{Models}} and {{Applications}}},
  shorttitle = {Document {{AI}}},
  author = {Cui, Lei and Xu, Yiheng and Lv, Tengchao and Wei, Furu},
  year = {2021},
  journal = {ArXiv},
  abstract = {Early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods are introduced, and future directions for Document AI research are looked into. Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.},
  annotation = {GSCC: 0000873},
  file = {/Users/chenghao/Zotero/storage/6LPU7CZK/Cui_et_al_(2021)_Document_AI.pdf}
}

@article{dahlgaard_2017,
  title = {Fast {{Similarity Sketching}}},
  author = {Dahlgaard, S{\o}ren and Knudsen, Mathias B{\ae}k Tejs and Thorup, Mikkel},
  year = {2017},
  month = apr,
  doi = {10.48550/arXiv.1704.04370},
  urldate = {2022-10-10},
  abstract = {We consider the Similarity Sketching problem: Given a universe \$[u]= {\textbackslash}\{0,{\textbackslash}ldots,u-1{\textbackslash}\}\$ we want a random function \$S\$ mapping subsets \$A{\textbackslash}subseteq [u]\$ into vectors \$S(A)\$ of size \$t\$, such that similarity is preserved. More precisely: Given sets \$A,B{\textbackslash}subseteq [u]\$, define \$X\_i=[S(A)[i]= S(B)[i]]\$ and \$X={\textbackslash}sum\_\{i{\textbackslash}in [t]\}X\_i\$. We want to have \$E[X]=t{\textbackslash}cdot J(A,B)\$, where \$J(A,B)={\textbar}A{\textbackslash}cap B{\textbar}/{\textbar}A{\textbackslash}cup B{\textbar}\$ and furthermore to have strong concentration guarantees (i.e. Chernoff-style bounds) for \$X\$. This is a fundamental problem which has found numerous applications in data mining, large-scale classification, computer vision, similarity search, etc. via the classic MinHash algorithm. The vectors \$S(A)\$ are also called sketches. The seminal \$t{\textbackslash}times\$MinHash algorithm uses \$t\$ random hash functions \$h\_1,{\textbackslash}ldots, h\_t\$, and stores \${\textbackslash}left({\textbackslash}min\_\{a{\textbackslash}in A\}h\_1(A),{\textbackslash}ldots, {\textbackslash}min\_\{a{\textbackslash}in A\}h\_t(A){\textbackslash}right)\$ as the sketch of \$A\$. The main drawback of MinHash is, however, its \$O(t{\textbackslash}cdot {\textbar}A{\textbar})\$ running time, and finding a sketch with similar properties and faster running time has been the subject of several papers. Addressing this, Li et al. [NIPS'12] introduced one permutation hashing (OPH), which creates a sketch of size \$t\$ in \$O(t + {\textbar}A{\textbar})\$ time, but with the drawback that possibly some of the \$t\$ entries are "empty" when \${\textbar}A{\textbar} = O(t)\$. One could argue that sketching is not necessary in this case, however the desire in most applications is to have one sketching procedure that works for sets of all sizes. Therefore, filling out these empty entries is the subject of several follow-up papers initiated by Shrivastava and Li [ICML'14]. However, these "densification" schemes fail to provide good concentration bounds exactly in the case \${\textbar}A{\textbar} = O(t)\$, where they are needed. (continued...)},
  langid = {english},
  annotation = {24 citations (Semantic Scholar/arXiv) [2022-10-09]},
  file = {/Users/chenghao/Zotero/storage/7QXV6QUI/Dahlgaard_et_al_(2017)_Fast_Similarity_Sketching.pdf}
}

@article{davis_,
  title = {Using a Large Language Model to Generate Program Mutations for a Genetic Algorithm to Search for Solutions to Combinatorial Problems: {{Review}} of ({{Romera-Paredes}} et al., 2023).},
  author = {Davis, Ernest},
  abstract = {FunSearch (Romera-Paredes et al., 2023) uses a large language model (LLM) of a component of an AI system that has generated some difficult-to-find solutions to combinatorial problems that are larger than was previously known. However, the excitement that has greeted this has been unwarranted. First, as compared to other applications of AI to mathematical research, FunSearch does not seem particularly promising as a technique and its contribution to mathematics is not unusually great. Second, FunSearch uses the LLM as a subroutine in a genetic programming algorithm, to generate mutations of one particular subroutine in a larger program. The LLM is not told either what problem is being addressed or what the overall program is. Thus, as compared to other application of LLMs to mathematics, the LLM in FunSearch is remarkable for the shallowness of the mathematical understanding that it seems to exhibit.},
  langid = {english},
  annotation = {\{"size": 311134, "pages": 13, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/YW8IVZGM/Davis - Using a large language model to generate program mutations for a genetic algorithm to search for sol.pdf}
}

@article{davis_2022,
  title = {End-to-End {{Document Recognition}} and {{Understanding}} with {{Dessurt}}},
  author = {Davis, Brian L. and Morse, B. and Price, Bryan and Tensmeyer, Chris and Wigington, Curtis and Morariu, Vlad I.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2203.16618},
  abstract = {Dessurt is an end-to-end architecture that performs text recognition in addition to the document understanding, it does not require an external recognition model as prior methods do and is able to handle a variety of document domains and tasks. . We introduce Dessurt, a relatively simple document understanding transformer capable of being fine-tuned on a greater variety of document tasks than prior methods. It receives a document image and task string as input and generates arbitrary text autoregressively as output. Because Dessurt is an end-to-end architecture that performs text recognition in addition to the document understanding, it does not require an external recognition model as prior methods do. Dessurt is a more flexible model than prior methods and is able to handle a variety of document domains and tasks. We show that this model is effective at 9 different dataset-task combinations.},
  annotation = {GSCC: 0000001},
  file = {/Users/chenghao/Zotero/storage/IF2BZV6H/Davis_et_al_(2022)_End-to-end_Document_Recognition_and_Understanding_with_Dessurt.pdf}
}

@misc{de_2024,
  title = {Griffin: {{Mixing Gated Linear Recurrences}} with {{Local Attention}} for {{Efficient Language Models}}},
  shorttitle = {Griffin},
  author = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and {Cristian-Muraru}, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and De Freitas, Nando and Gulcehre, Caglar},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-03-04},
  abstract = {Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.},
  howpublished = {https://arxiv.org/abs/2402.19427v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/VKULWXTF/De_et_al_(2024)_Griffin.pdf}
}

@article{decurto_2022,
  title = {Learning with {{Signatures}}},
  author = {{de Curt{\`o}}, J. and {de Zarz{\`a}}, I. and Calafate, Carlos T. and Yan, Hong},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.07953 [cs]},
  eprint = {2204.07953},
  primaryclass = {cs},
  urldate = {2022-04-24},
  abstract = {In this work we investigate the use of the Signature Transform in the context of Learning. Under this assumption, we advance a supervised framework that provides state-of-the-art classification accuracy with the use of very few labels without the need of credit assignment and with minimal or no overfitting. We leverage tools from harmonic analysis by the use of the signature and log-signature and use as a score function RMSE and MAE Signature and log-signature. We develop a closed-form equation to compute probably good optimal scale factors. Classification is performed at the CPU level orders of magnitude faster than other methods. We report results on AFHQ dataset, Four Shapes, MNIST and CIFAR10 achieving 100\% accuracy on all tasks.},
  archiveprefix = {arxiv},
  keywords = {classification,Computer Science - Computer Vision and Pattern Recognition,cv,supervised learning},
  file = {/Users/chenghao/Zotero/storage/4D7HTFVD/de_Curtò_et_al_(2022)_Learning_with_Signatures.pdf}
}

@article{dehghani_2021,
  title = {The {{Benchmark Lottery}}},
  author = {Dehghani, Mostafa and Tay, Yi and Gritsenko, Alexey A and Zhao, Zhe and Houlsby, Neil and Diaz, Fernando and Metzler, Donald and Vinyals, Oriol},
  year = {2021},
  journal = {arXiv},
  abstract = {The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of "a benchmark lottery" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.},
  keywords = {nosource}
}

@article{dejean_2022,
  title = {{{LayoutXLM}} vs. {{GNN}}: {{An Empirical Evaluation}} of {{Relation Extraction}} for {{Documents}}},
  shorttitle = {{{LayoutXLM}} vs. {{GNN}}},
  author = {D'ejean, Herv'e and Clinchant, S. and Meunier, J.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2206.10304},
  abstract = {This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). For this benchmark, we use the XFUND dataset, released along with LayoutXLM. While both models reach similar results, they both exhibit very different charac-teristics. This raises the question on how to integrate various modalities in a neural network: by merging all modalities thanks to additional pretraining (LayoutXLM), or in a cascaded way (ECN). We conclude by discussing some methodological issues that must be considered for new datasets and task definition in the do-main of Information Extraction with complex documents.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/YMUZRP7Q/D'ejean_et_al_(2022)_LayoutXLM_vs.pdf}
}

@article{desai_2024,
  title = {Between {{Copyright}} and {{Computer Science}}: {{The Law}} and {{Ethics}} of {{Generative AI}}},
  shorttitle = {Between {{Copyright}} and {{Computer Science}}},
  author = {Desai, Deven R. and Riedl, Mark},
  year = {2024},
  month = feb,
  urldate = {2024-02-28},
  abstract = {Copyright and computer science continue to intersect and clash, but they can coexist. The advent of new technologies such as digitization of visual and aural cr},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/VZ8UYSSE/Desai and Riedl - 2024 - Between Copyright and Computer Science The Law and Ethics of Generative AI.pdf}
}

@misc{dettmers_2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  howpublished = {https://arxiv.org/abs/2305.14314v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/9IXMUB7C/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf}
}

@misc{diliello_2022,
  title = {Paragraph-Based {{Transformer Pre-training}} for {{Multi-Sentence Inference}}},
  author = {Di Liello, Luca and Garg, Siddhant and Soldaini, Luca and Moschitti, Alessandro},
  year = {2022},
  month = may,
  number = {arXiv:2205.01228},
  eprint = {2205.01228},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-04},
  abstract = {Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models the paragraph-level semantics across multiple input sentences. Our evaluation on three AS2 and one fact verification datasets demonstrates the superiority of our pre-training technique over the traditional ones for transformers used as joint models for multi-candidate inference tasks, as well as when used as cross-encoders for sentence-pair formulations of these tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-06-04]},
  file = {/Users/chenghao/Zotero/storage/9VEMW9NJ/Di_Liello_et_al_(2022)_Paragraph-based_Transformer_Pre-training_for_Multi-Sentence_Inference.pdf}
}

@misc{ding_2023,
  title = {{{LongNet}}: {{Scaling Transformers}} to 1,000,000,000 {{Tokens}}},
  shorttitle = {{{LongNet}}},
  author = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Wei, Furu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02486},
  eprint = {2307.02486},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.02486},
  urldate = {2023-07-06},
  abstract = {Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 483820, "pages": 15, "previous": "arXiv:2307.02486 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/C22FMD8K/Ding_et_al_(2023)_LongNet.pdf}
}

@article{du_2021,
  title = {{{GLaM}}: {{Efficient Scaling}} of {{Language Models}} with {{Mixture-of-Experts}}},
  shorttitle = {{{GLaM}}},
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and {Meier-Hellstern}, Kathy and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.06905 [cs]},
  eprint = {2112.06905},
  primaryclass = {cs},
  urldate = {2021-12-16},
  abstract = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,moe,nlp,scaling},
  annotation = {20 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2112.06905 [cs]/2021/du_2021_glam_-_efficient_scaling_of_language_models_with_mixture-of-experts.pdf}
}

@misc{ehsan_2023,
  title = {Charting the {{Sociotechnical Gap}} in {{Explainable AI}}: {{A Framework}} to {{Address}} the {{Gap}} in {{XAI}}},
  shorttitle = {Charting the {{Sociotechnical Gap}} in {{Explainable AI}}},
  author = {Ehsan, Upol and Saha, Koustuv and De Choudhury, Munmun and Riedl, Mark O.},
  year = {2023},
  month = feb,
  doi = {10.1145/3579467},
  urldate = {2023-03-04},
  abstract = {Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap--divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  annotation = {\{"size": 996277, "pages": 33, "previous": "arXiv:2302.00799 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/SSSGT8ZY/Ehsan et al. - 2023 - Charting the Sociotechnical Gap in Explainable AI.pdf}
}

@misc{el-mhamdi_2022,
  title = {{{SoK}}: {{On}} the {{Impossible Security}} of {{Very Large Foundation Models}}},
  shorttitle = {{{SoK}}},
  author = {{El-Mhamdi}, El-Mahdi and Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Hoang, L{\^e}-Nguy{\^e}n and Pinot, Rafael and Stephan, John},
  year = {2022},
  month = sep,
  number = {arXiv:2209.15259},
  eprint = {2209.15259},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.15259},
  urldate = {2023-04-08},
  abstract = {Large machine learning models, or so-called foundation models, aim to serve as base-models for application-oriented machine learning. Although these models showcase impressive performance, they have been empirically found to pose serious security and privacy issues. We may however wonder if this is a limitation of the current models, or if these issues stem from a fundamental intrinsic impossibility of the foundation model learning problem itself. This paper aims to systematize our knowledge supporting the latter. More precisely, we identify several key features of today's foundation model learning problem which, given the current understanding in adversarial machine learning, suggest incompatibility of high accuracy with both security and privacy. We begin by observing that high accuracy seems to require (1) very high-dimensional models and (2) huge amounts of data that can only be procured through user-generated datasets. Moreover, such data is fundamentally heterogeneous, as users generally have very specific (easily identifiable) data-generating habits. More importantly, users' data is filled with highly sensitive information, and maybe heavily polluted by fake users. We then survey lower bounds on accuracy in privacy-preserving and Byzantine-resilient heterogeneous learning that, we argue, constitute a compelling case against the possibility of designing a secure and privacy-preserving high-accuracy foundation model. We further stress that our analysis also applies to other high-stake machine learning applications, including content recommendation. We conclude by calling for measures to prioritize security and privacy, and to slow down the race for ever larger models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/S5MNGT6Y/El-Mhamdi et al. - 2022 - SoK On the Impossible Security of Very Large Foun.pdf}
}

@misc{elazar_2023,
  title = {What's {{In My Big Data}}?},
  author = {Elazar, Yanai and Bhagia, Akshita and Magnusson, Ian and Ravichander, Abhilasha and Schwenk, Dustin and Suhr, Alane and Walsh, Pete and Groeneveld, Dirk and Soldaini, Luca and Singh, Sameer and Hajishirzi, Hanna and Smith, Noah A. and Dodge, Jesse},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20707},
  eprint = {2310.20707},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.20707},
  urldate = {2023-11-02},
  abstract = {Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50\% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them: github.com/allenai/wimbd.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Elazar et_al/2023/Elazar_et_al_(2023)_What's_In_My_Big_Data.pdf}
}

@misc{erscoi_2023,
  title = {Pygmalion {{Displacement}}: {{When Humanising AI Dehumanises Women}}},
  shorttitle = {Pygmalion {{Displacement}}},
  author = {Erscoi, Lelia and Kleinherenbrink, Annelies and Guest, Olivia},
  year = {2023},
  month = feb,
  publisher = {SocArXiv},
  doi = {10.31235/osf.io/jqxb6},
  urldate = {2023-02-19},
  abstract = {We use the myth of Pygmalion as a lens to investigate and frame the relationship between women and artificial intelligence (AI). Pygmalion was a legendary ancient king of Cyprus and sculptor. Having been repulsed by women, he used his skills to create a statue, which was imbued with life by the goddess Aphrodite. This can be seen as one of the primordial AI-like myths, wherein humanity creates intelligent life-like self-images to reproduce or replace ourselves. In addition, the myth prefigures historical and present gendered dynamics within the field of AI and between AI and society at large. Throughout history, the theme of women being replaced by inanimate objects (e.g. automata, algorithms) has been repeated, and continues to repeat in contemporary AI technologies. However, this socially detrimental pattern in technology --- what we dub Pygmalion displacement --- is often overlooked, whether due to naive excitement about new developments, or due to an unacknowledged sexist history of the field itself. As we demonstrate herein, Pygmalion displacement prefigures heavily, but in an unacknowledged way, in the original Turing test, the imitation game: a central thought experiment, foundational to AI. With women, and the feminine generally, being both dislocated and erased from and by technology, AI is and has been (presented as) created mainly by privileged men, subserving capitalist patriarchal ends. This poses serious dangers to women and other marginalised people. By tracing the historical and ongoing entwinement of femininity (from a patriarchal perspective) and AI, we aim to understand, make visible, and start a dialogue on the ways in which AI harms women.},
  langid = {american},
  keywords = {and Sexuality Studies,Arts and Humanities,Feminist,Gender,Psychology,Science and Technology Studies,Social and Behavioral Sciences,Theory and Philosophy,Women's Studies},
  file = {/Users/chenghao/Zotero/storage/GTB4XUC3/Erscoi_et_al_(2023)_Pygmalion_Displacement.pdf}
}

@article{fedus_2021,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03961 [cs]},
  eprint = {2101.03961},
  primaryclass = {cs},
  urldate = {2021-09-10},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,moe,nosource,transformer},
  annotation = {277 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/CNTIHHGV/Fedus_et_al_(2021)_Switch_Transformers.pdf}
}

@misc{fernandes_2023,
  title = {Bridging the {{Gap}}: {{A Survey}} on {{Integrating}} ({{Human}}) {{Feedback}} for {{Natural Language Generation}}},
  shorttitle = {Bridging the {{Gap}}},
  author = {Fernandes, Patrick and Madaan, Aman and Liu, Emmy and Farinhas, Ant{\'o}nio and Martins, Pedro Henrique and Bertsch, Amanda and {de Souza}, Jos{\'e} G. C. and Zhou, Shuyan and Wu, Tongshuang and Neubig, Graham and Martins, Andr{\'e} F. T.},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-05-10},
  abstract = {Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.},
  howpublished = {https://arxiv.org/abs/2305.00955v1},
  langid = {english},
  annotation = {\{"size": 394060, "pages": 24, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/UHZ3F26Q/Fernandes et al. - 2023 - Bridging the Gap A Survey on Integrating (Human) .pdf}
}

@misc{fried_2022,
  title = {{{InCoder}}: {{A Generative Model}} for {{Code Infilling}} and {{Synthesis}}},
  shorttitle = {{{InCoder}}},
  author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05999},
  eprint = {2204.05999},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.05999},
  urldate = {2023-03-18},
  abstract = {Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  annotation = {69 citations (Semantic Scholar/arXiv) [2023-03-18] 69 citations (Semantic Scholar/DOI) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/NW4DX7KA/Fried_et_al_(2022)_InCoder.pdf}
}

@inproceedings{frobe_2021,
  title = {{{CopyCat}}: {{Near-Duplicates Within}} and {{Between}} the {{ClueWeb}} and the {{Common Crawl}}},
  shorttitle = {{{CopyCat}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Fr{\"o}be, Maik and Bevendorff, Janek and Gienapp, Lukas and V{\"o}lske, Michael and Stein, Benno and Potthast, Martin and Hagen, Matthias},
  year = {2021},
  month = jul,
  pages = {2398--2404},
  publisher = {ACM},
  address = {Virtual Event Canada},
  doi = {10.1145/3404835.3463246},
  urldate = {2022-11-24},
  abstract = {The amount of near-duplicates in web crawls like the ClueWeb or Common Crawl demands from their users either to develop a preprocessing pipeline for deduplication, which is costly both computationally and in person hours, or accepting the undesired effects that near-duplicates have on reliability and validity of experiments. We introduce ChatNoir-CopyCat-21, which simplifies deduplication significantly. It comes in two parts: (1) A compilation of near-duplicate documents within the ClueWeb09, the ClueWeb12, and two Common Crawl snapshots, as well as between selections of these crawls, and (2) a software library that implements the deduplication of arbitrary document sets. Our analysis shows that 14--52 \% of the documents within a crawl and around 0.7--2.5 \% between the crawls are near-duplicates. Two showcases demonstrate the application and usefulness of our resource.},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  keywords = {/unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2022-11-24]},
  file = {/Users/chenghao/Zotero/storage/FJDK5F2H/Fröbe_et_al_(2021)_CopyCat.pdf}
}

@misc{gandikota_2023,
  title = {Erasing {{Concepts}} from {{Diffusion Models}}},
  author = {Gandikota, Rohit and Materzynska, Joanna and {Fiotto-Kaufman}, Jaden and Bau, David},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07345},
  eprint = {2303.07345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.07345},
  urldate = {2023-03-14},
  abstract = {Motivated by recent advancements in text-to-image diffusion, we study erasure of specific concepts from the model's weights. While Stable Diffusion has shown promise in producing explicit or realistic artwork, it has raised concerns regarding its potential for misuse. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at https://erasing.baulab.info/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-03-14]},
  file = {/Users/chenghao/Zotero/storage/CX87I6ZL/Gandikota_et_al_(2023)_Erasing_Concepts_from_Diffusion_Models.pdf}
}

@article{gao_2020,
  title = {The {{Pile}}: {{An 800GB Dataset}} of {{Diverse Text}} for {{Language Modeling}}},
  shorttitle = {The {{Pile}}},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = {2020},
  month = dec,
  journal = {arXiv:2101.00027 [cs]},
  eprint = {2101.00027},
  primaryclass = {cs},
  urldate = {2021-09-11},
  abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,data,nlp},
  annotation = {78 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2101.00027 [cs]/2020/gao_2020_the_pile_-_an_800gb_dataset_of_diverse_text_for_language_modeling.pdf}
}

@article{gao_2021,
  title = {An {{Empirical Exploration}} in {{Quality Filtering}} of {{Text Data}}},
  author = {Gao, Leo},
  year = {2021},
  month = oct,
  journal = {arXiv:2109.00698 [cs]},
  eprint = {2109.00698},
  primaryclass = {cs},
  urldate = {2021-11-28},
  abstract = {While conventional wisdom suggests that more aggressively filtering data from low-quality sources like Common Crawl always monotonically improves the quality of training data, we find that aggressive filtering can in fact lead to a decrease in model quality on a wide array of downstream tasks for a GPT-like language model. We speculate that this is because optimizing sufficiently strongly for a proxy metric harms performance on the true objective, suggesting a need for more robust filtering objectives when attempting to filter more aggressively. We hope this work leads to detailed analysis of the effects of dataset filtering design choices on downstream model performance in future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,data,filtering},
  annotation = {2 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2109.00698 [cs]/2021/gao_2021_an_empirical_exploration_in_quality_filtering_of_text_data.pdf}
}

@misc{gao_2024,
  title = {Efficient {{Tool Use}} with {{Chain-of-Abstraction Reasoning}}},
  author = {Gao, Silin and {Dwivedi-Yu}, Jane and Yu, Ping and Tan, Xiaoqing Ellen and Pasunuru, Ramakanth and Golovneva, Olga and Sinha, Koustuv and Celikyilmaz, Asli and Bosselut, Antoine and Wang, Tianlu},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-02-01},
  abstract = {To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average {\textasciitilde}6\% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average {\textasciitilde}1.4x faster than baseline tool-augmented LLMs.},
  howpublished = {https://arxiv.org/abs/2401.17464v1},
  langid = {english},
  annotation = {\{"size": 937097, "pages": 17, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/5PHDIAN6/Gao_et_al_(2024)_Efficient_Tool_Use_with_Chain-of-Abstraction_Reasoning.pdf}
}

@article{garncarek_2021,
  title = {{{LAMBERT}}: {{Layout-Aware Language Modeling}} for {{Information Extraction}}},
  shorttitle = {{{LAMBERT}}},
  author = {Garncarek, Lukasz and Powalski, Rafal and Stanis{\l}awek, Tomasz and Topolski, Bartosz and Halama, Piotr and Turski, M. and Grali'nski, Filip},
  year = {2021},
  journal = {undefined},
  urldate = {2022-10-28},
  abstract = {This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). This paper investigates the Relation Extraction task in documents by benchmarking two different neural network models: a multimodal language model (LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). For this benchmark, we use the XFUND dataset, released along with LayoutXLM. While both models reach similar results, they both exhibit very different charac-teristics. This raises the question on how to integrate various modalities in a neural network: by merging all modalities thanks to additional pretraining (LayoutXLM), or in a cascaded way (ECN). We conclude by discussing some methodological issues that must be considered for new datasets and task definition in the do-main of Information Extraction with complex documents.},
  langid = {english},
  annotation = {GSCC: 0000031},
  file = {/Users/chenghao/Zotero/storage/9GPAHSAZ/Garncarek_et_al_(2021)_LAMBERT.pdf;/Users/chenghao/Zotero/storage/L2WQW7V9/f344647eef878684ad231f804bca20da068d9736.html}
}

@misc{geiping_2022,
  title = {Cramming: {{Training}} a {{Language Model}} on a {{Single GPU}} in {{One Day}}},
  shorttitle = {Cramming},
  author = {Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  month = dec,
  number = {arXiv:2212.14034},
  eprint = {2212.14034},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-07},
  abstract = {Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-01-06]},
  file = {/Users/chenghao/Zotero/storage/CY3R4SLI/Geiping_Goldstein_(2022)_Cramming.pdf}
}

@misc{gesmundo_2022,
  title = {An {{Evolutionary Approach}} to {{Dynamic Introduction}} of {{Tasks}} in {{Large-scale Multitask Learning Systems}}},
  author = {Gesmundo, Andrea and Dean, Jeff},
  year = {2022},
  month = may,
  number = {arXiv:2205.12755},
  eprint = {2205.12755},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.12755},
  urldate = {2022-05-27},
  abstract = {Multitask learning assumes that models capable of learning from multiple tasks can achieve better quality and efficiency via knowledge transfer, a key feature of human learning. Though, state of the art ML models rely on high customization for each task and leverage size and data scale rather than scaling the number of tasks. Also, continual learning, that adds the temporal aspect to multitask, is often focused to the study of common pitfalls such as catastrophic forgetting instead of being studied at a large scale as a critical component to build the next generation artificial intelligence. We propose an evolutionary method that can generate a large scale multitask model, and can support the dynamic and continuous addition of new tasks. The generated multitask model is sparsely activated and integrates a task-based routing that guarantees bounded compute cost and fewer added parameters per task as the model expands. The proposed method relies on a knowledge compartmentalization technique to achieve immunity against catastrophic forgetting and other common pitfalls such as gradient interference and negative transfer. We empirically show that the proposed method can jointly solve and achieve competitive results on 69image classification tasks, for example achieving the best test accuracy reported fora model trained only on public data for competitive tasks such as cifar10: 99.43\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-05-27]},
  file = {/Users/chenghao/Zotero/storage/EQS4NI8L/Gesmundo_Dean_(2022)_An_Evolutionary_Approach_to_Dynamic_Introduction_of_Tasks_in_Large-scale.pdf}
}

@misc{gilardi_2023,
  title = {{{ChatGPT Outperforms Crowd-Workers}} for {{Text-Annotation Tasks}}},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  year = {2023},
  month = mar,
  number = {arXiv:2303.15056},
  eprint = {2303.15056},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.15056},
  urldate = {2023-03-28},
  abstract = {Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/chenghao/Zotero/storage/FIGARLHI/Gilardi_et_al_(2023)_ChatGPT_Outperforms_Crowd-Workers_for_Text-Annotation_Tasks.pdf}
}

@misc{groeneveld_2023,
  title = {Catwalk: {{A Unified Language Model Evaluation Framework}} for {{Many Datasets}}},
  shorttitle = {Catwalk},
  author = {Groeneveld, Dirk and Awadalla, Anas and Beltagy, Iz and Bhagia, Akshita and Magnusson, Ian and Peng, Hao and Tafjord, Oyvind and Walsh, Pete and Richardson, Kyle and Dodge, Jesse},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10253},
  eprint = {2312.10253},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10253},
  urldate = {2023-12-19},
  abstract = {The success of large language models has shifted the evaluation paradigms in natural language processing (NLP). The community's interest has drifted towards comparing NLP models across many tasks, domains, and datasets, often at an extreme scale. This imposes new engineering challenges: efforts in constructing datasets and models have been fragmented, and their formats and interfaces are incompatible. As a result, it often takes extensive (re)implementation efforts to make fair and controlled comparisons at scale. Catwalk aims to address these issues. Catwalk provides a unified interface to a broad range of existing NLP datasets and models, ranging from both canonical supervised training and fine-tuning, to more modern paradigms like in-context learning. Its carefully-designed abstractions allow for easy extensions to many others. Catwalk substantially lowers the barriers to conducting controlled experiments at scale. For example, we finetuned and evaluated over 64 models on over 86 datasets with a single command, without writing any code. Maintained by the AllenNLP team at the Allen Institute for Artificial Intelligence (AI2), Catwalk is an ongoing open-source effort: https://github.com/allenai/catwalk.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {\{"size": 648839, "pages": 16, "previous": "arXiv:2312.10253 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/CHVKPIZX/Groeneveld_et_al_(2023)_Catwalk.pdf}
}

@misc{groeneveld_2024,
  title = {{{OLMo}}: {{Accelerating}} the {{Science}} of {{Language Models}}},
  shorttitle = {{{OLMo}}},
  author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-02},
  abstract = {Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.},
  howpublished = {https://arxiv.org/abs/2402.00838v1},
  langid = {english},
  annotation = {\{"size": 518849, "pages": 21, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/4AEWMPLR/Groeneveld et al. - 2024 - OLMo Accelerating the Science of Language Models.pdf}
}

@misc{grosse_2023,
  title = {Studying {{Large Language Model Generalization}} with {{Influence Functions}}},
  author = {Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage, Nelson and Tamkin, Alex and Tajdini, Amirhossein and Steiner, Benoit and Li, Dustin and Durmus, Esin and Perez, Ethan and Hubinger, Evan and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared and Bowman, Samuel R.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03296},
  eprint = {2308.03296},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.03296},
  urldate = {2023-08-11},
  abstract = {When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {\{"size": 3772019, "pages": 119, "previous": "arXiv:2308.03296 [cs, stat]"\}},
  file = {/Users/chenghao/Zotero/storage/WMWTLVGR/Grosse et al. - 2023 - Studying Large Language Model Generalization with .pdf}
}

@misc{gu_2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  year = {2023},
  month = dec,
  number = {arXiv:2312.00752},
  eprint = {2312.00752},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.00752},
  urldate = {2023-12-04},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {\{"size": 1293868, "pages": 37, "previous": "arXiv:2312.00752 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/5WW2TPFN/Gu and Dao - 2023 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf}
}

@misc{gudibande_2023,
  title = {The {{False Promise}} of {{Imitating Proprietary LLMs}}},
  author = {Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  year = {2023},
  month = may,
  number = {arXiv:2305.15717},
  eprint = {2305.15717},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15717},
  urldate = {2023-05-26},
  abstract = {An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/4C83PGN7/Gudibande et al. - 2023 - The False Promise of Imitating Proprietary LLMs.pdf}
}

@misc{guo_2024,
  title = {Direct {{Language Model Alignment}} from {{Online AI Feedback}}},
  author = {Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and Ferret, Johan and Blondel, Mathieu},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-08},
  abstract = {Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.},
  howpublished = {https://arxiv.org/abs/2402.04792v1},
  langid = {english},
  annotation = {\{"size": 987022, "pages": 18, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/PHHW447R/Guo et al. - 2024 - Direct Language Model Alignment from Online AI Feedback.pdf}
}

@misc{gururangan_2023,
  title = {Scaling {{Expert Language Models}} with {{Unsupervised Domain Discovery}}},
  author = {Gururangan, Suchin and Li, Margaret and Lewis, Mike and Shi, Weijia and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2023},
  month = mar,
  number = {arXiv:2303.14177},
  eprint = {2303.14177},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.14177},
  urldate = {2023-03-28},
  abstract = {Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-03-27]},
  file = {/Users/chenghao/Zotero/storage/PSJ29H5F/Gururangan_et_al_(2023)_Scaling_Expert_Language_Models_with_Unsupervised_Domain_Discovery.pdf}
}

@misc{gutierrez-fandino_2021,
  title = {Spanish {{Legalese Language Model}} and {{Corpora}}},
  author = {{Guti{\'e}rrez-Fandi{\~n}o}, Asier and {Armengol-Estap{\'e}}, Jordi and {Gonzalez-Agirre}, Aitor and Villegas, Marta},
  year = {2021},
  month = oct,
  number = {arXiv:2110.12201},
  eprint = {2110.12201},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.12201},
  urldate = {2023-03-18},
  abstract = {There are many Language Models for the English language according to its worldwide relevance. However, for the Spanish language, even if it is a widely spoken language, there are very few Spanish Language Models which result to be small and too general. Legal slang could be think of a Spanish variant on its own as it is very complicated in vocabulary, semantics and phrase understanding. For this work we gathered legal-domain corpora from different sources, generated a model and evaluated against Spanish general domain tasks. The model provides reasonable results in those tasks.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/L5A5INR4/Gutiérrez-Fandiño et al. - 2021 - Spanish Legalese Language Model and Corpora.pdf}
}

@article{gutierrez-fandino_2022,
  title = {{{MarIA}}: {{Spanish Language Models}}},
  shorttitle = {{{MarIA}}},
  author = {{Guti{\'e}rrez-Fandi{\~n}o}, Asier and {Armengol-Estap{\'e}}, Jordi and P{\`a}mies, Marc and {Llop-Palao}, Joan and {Silveira-Ocampo}, Joaqu{\'i}n and Carrino, Casimiro Pio and {Gonzalez-Agirre}, Aitor and {Armentano-Oller}, Carme and {Rodriguez-Penagos}, Carlos and Villegas, Marta},
  year = {2022},
  journal = {Procesamiento del Lenguaje Natural},
  eprint = {2107.07253},
  primaryclass = {cs},
  pages = {39--60},
  issn = {1989-7553},
  doi = {10.26342/2022-68-3},
  urldate = {2023-03-21},
  abstract = {This work presents MarIA, a family of Spanish language models and associated resources made available to the industry and the research community. Currently, MarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish language models, which can arguably be presented as the largest and most proficient language models in Spanish. The models were pretrained using a massive corpus of 570GB of clean and deduplicated texts with 135 billion words extracted from the Spanish Web Archive crawled by the National Library of Spain between 2009 and 2019. We assessed the performance of the models with nine existing evaluation datasets and with a novel extractive Question Answering dataset created ex novo. Overall, MarIA models outperform the existing Spanish models across a variety of NLU tasks and training settings.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {18 citations (Semantic Scholar/arXiv) [2023-03-20] 18 citations (Semantic Scholar/DOI) [2023-03-20]},
  file = {/Users/chenghao/Zotero/storage/X7LJS5L7/Gutiérrez-Fandiño et al. - 2022 - MarIA Spanish Language Models.pdf}
}

@misc{gutmann_2022,
  title = {Pen and {{Paper Exercises}} in {{Machine Learning}}},
  author = {Gutmann, Michael U.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.13446},
  eprint = {2206.13446},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.13446},
  urldate = {2022-09-10},
  abstract = {This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/6U3GWTYS/Gutmann_(2022)_Pen_and_Paper_Exercises_in_Machine_Learning.pdf}
}

@article{hartman_2021,
  title = {Scaling {{TensorFlow}} to 300 Million Predictions per Second},
  author = {Hartman, Jan and Kopi{\v c}, Davorin},
  year = {2021},
  month = sep,
  journal = {Fifteenth ACM Conference on Recommender Systems},
  eprint = {2109.09541},
  pages = {595--597},
  doi = {10.1145/3460231.3474605},
  urldate = {2021-09-25},
  abstract = {We present the process of transitioning machine learning models to the TensorFlow framework at a large scale in an online advertising ecosystem. In this talk we address the key challenges we faced and describe how we successfully tackled them; notably, implementing the models in TF and serving them efficiently with low latency using various optimization techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Performance,nlp,scaling,transformer},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-05] 0 citations (Semantic Scholar/DOI) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/Fifteenth ACM Conference on Recommender Systems/2021/hartman_kopič_2021_scaling_tensorflow_to_300_million_predictions_per_second.pdf}
}

@misc{he_2023,
  title = {Simplifying {{Transformer Blocks}}},
  author = {He, Bobby and Hofmann, Thomas},
  year = {2023},
  month = nov,
  number = {arXiv:2311.01906},
  eprint = {2311.01906},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01906},
  urldate = {2023-12-09},
  abstract = {A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections \& normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable. In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15\% faster training throughput, and using 15\% fewer parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {\{"size": 5303539, "pages": 27, "previous": "arXiv:2311.01906 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/UUXBH243/He_Hofmann_(2023)_Simplifying_Transformer_Blocks.pdf}
}

@misc{heffernan_2022,
  title = {Bitext {{Mining Using Distilled Sentence Representations}} for {{Low-Resource Languages}}},
  author = {Heffernan, Kevin and {\c C}elebi, Onur and Schwenk, Holger},
  year = {2022},
  month = may,
  number = {arXiv:2205.12654},
  eprint = {2205.12654},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.12654},
  urldate = {2022-07-09},
  abstract = {Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. A promising approach has been to train one-for-all multilingual models capable of cross-lingual transfer, but these models often suffer from insufficient capacity and interference between unrelated languages. Instead, we move away from this approach and focus on training multiple language (family) specific representations, but most prominently enable all languages to still be encoded in the same representational space. To achieve this, we focus on teacher-student training, allowing all encoders to be mutually compatible for bitext mining, and enabling fast learning of new languages. We introduce a new teacher-student training scheme which combines supervised and self-supervised training, allowing encoders to take advantage of monolingual training data, which is valuable in the low-resource setting. Our approach significantly outperforms the original LASER encoder. We study very low-resource languages and handle 50 African languages, many of which are not covered by any other model. For these languages, we train sentence encoders, mine bitexts, and validate the bitexts by training NMT systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,dataset,multilingual,natural language processing},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-09] 0 citations (Semantic Scholar/DOI) [2022-07-09]},
  file = {/Users/chenghao/Zotero/storage/9NBFQ29I/Heffernan_et_al_(2022)_Bitext_Mining_Using_Distilled_Sentence_Representations_for_Low-Resource.pdf}
}

@misc{henderson_2022,
  title = {Pile of {{Law}}: {{Learning Responsible Data Filtering}} from the {{Law}} and a {{256GB Open-Source Legal Dataset}}},
  shorttitle = {Pile of {{Law}}},
  author = {Henderson, Peter and Krass, Mark S. and Zheng, Lucia and Guha, Neel and Manning, Christopher D. and Jurafsky, Dan and Ho, Daniel E.},
  year = {2022},
  month = jul,
  number = {arXiv:2207.00220},
  eprint = {2207.00220},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.00220},
  urldate = {2022-07-09},
  abstract = {One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take into account context. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may potentially help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,data filtering,dataset,law,natural language processing},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-09] 0 citations (Semantic Scholar/DOI) [2022-07-09]},
  file = {/Users/chenghao/Zotero/storage/S6ZYUG63/Henderson_et_al_(2022)_Pile_of_Law.pdf}
}

@misc{hendrycks_2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2021},
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-23},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {50 citations (Semantic Scholar/arXiv) [2022-06-23]},
  file = {/Users/chenghao/Zotero/storage/ZEDGJKED/Hendrycks_et_al_(2021)_Measuring_Massive_Multitask_Language_Understanding.pdf}
}

@misc{hermann_2023,
  title = {For {{Human-Like Models}}, {{Train}} on {{Human-Like Tasks}}},
  author = {Hermann, Katherine and Nayebi, Aran and van Steenkiste, Sjoerd and Jones, Matthew},
  year = {2023},
  month = mar,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/a35mt},
  urldate = {2023-03-04},
  abstract = {Bowers et al. (2022) express skepticism about deep neural networks (DNNs) as models of human vision due to DNNs' failures to account for results from psychological research. We argue that to fairly assess DNNs, we must first train them on more human-like tasks which we hypothesize will induce more human-like behaviors and representations.},
  langid = {american},
  keywords = {Computational Neuroscience,Deep Neural Networks,Human--Machine Comparison,Neuroscience,Perception},
  file = {/Users/chenghao/Zotero/storage/XLHL6IS7/Hermann et al. - 2023 - For Human-Like Models, Train on Human-Like Tasks.pdf}
}

@misc{hernandez_2022,
  title = {Scaling {{Laws}} and {{Interpretability}} of {{Learning}} from {{Repeated Data}}},
  author = {Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Henighan, Tom and Hume, Tristan and Johnston, Scott and Mann, Ben and Olah, Chris and Olsson, Catherine and Amodei, Dario and Joseph, Nicholas and Kaplan, Jared and McCandlish, Sam},
  year = {2022},
  month = may,
  number = {arXiv:2205.10487},
  eprint = {2205.10487},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.10487},
  urldate = {2022-08-21},
  abstract = {Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1\% of the data 100 times, despite the other 90\% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/Q4VIZQMZ/Hernandez_et_al_(2022)_Scaling_Laws_and_Interpretability_of_Learning_from_Repeated_Data.pdf}
}

@misc{hewitt_2023,
  title = {Backpack {{Language Models}}},
  author = {Hewitt, John and Thickstun, John and Manning, Christopher D. and Liang, Percy},
  year = {2023},
  month = may,
  number = {arXiv:2305.16765},
  eprint = {2305.16765},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16765},
  urldate = {2023-12-05},
  abstract = {We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/N4PKWVAQ/Hewitt et al. - 2023 - Backpack Language Models.pdf}
}

@article{hong_2020,
  title = {{{BROS}}: {{A Pre-trained Language Model}} for {{Understanding Texts}} in {{Document}}},
  shorttitle = {{{BROS}}},
  author = {Hong, Teakgyu and Kim, DongHyun and Ji, Mingi and Hwang, Wonseok and Nam, Daehyun and Park, Sungrae},
  year = {2020},
  month = sep,
  urldate = {2021-08-11},
  abstract = {Understanding document from their visual snapshots is an emerging and challenging problem that requires both advanced computer vision and NLP methods. Although the recent advance in OCR enables the...},
  langid = {english},
  keywords = {document,nlp},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/undefined/2020/hong_2020_bros_-_a_pre-trained_language_model_for_understanding_texts_in_document.pdf}
}

@misc{hong_2022,
  title = {{{BROS}}: {{A Pre-trained Language Model Focusing}} on {{Text}} and {{Layout}} for {{Better Key Information Extraction}} from {{Documents}}},
  shorttitle = {{{BROS}}},
  author = {Hong, Teakgyu and Kim, Donghyun and Ji, Mingi and Hwang, Wonseok and Nam, Daehyun and Park, Sungrae},
  year = {2022},
  month = apr,
  number = {arXiv:2108.04539},
  eprint = {2108.04539},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.04539},
  urldate = {2022-10-31},
  abstract = {Key information extraction (KIE) from document images requires understanding the contextual and spatial semantics of texts in two-dimensional (2D) space. Many recent studies try to solve the task by developing pre-trained language models focusing on combining visual features from document images with texts and their layout. On the other hand, this paper tackles the problem by going back to the basic: effective combination of text and layout. Specifically, we propose a pre-trained language model, named BROS (BERT Relying On Spatiality), that encodes relative positions of texts in 2D space and learns from unlabeled documents with area-masking strategy. With this optimized training scheme for understanding texts in 2D space, BROS shows comparable or better performance compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and SciTSR) without relying on visual features. This paper also reveals two real-world challenges in KIE tasks-(1) minimizing the error from incorrect text ordering and (2) efficient learning from fewer downstream examples-and demonstrates the superiority of BROS over previous methods. Code is available at https://github.com/clovaai/bros.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/L2HDGR8S/Hong_et_al_(2022)_BROS.pdf;/Users/chenghao/Zotero/storage/Z3R5QME2/2108.html}
}

@article{hou_2022,
  title = {Token {{Dropping}} for {{Efficient BERT Pretraining}}},
  author = {Hou, Le and Pang, Richard Yuanzhe and Zhou, Tianyi and Wu, Yuexin and Song, Xinying and Song, Xiaodan and Zhou, Denny},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.13240 [cs]},
  eprint = {2203.13240},
  primaryclass = {cs},
  urldate = {2022-04-02},
  abstract = {Transformer-based models generally allocate the same amount of computation for each token in a given sequence. We develop a simple but effective "token dropping" method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks. In short, we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens; the dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences. We leverage the already built-in masked language modeling (MLM) loss to identify unimportant tokens with practically no computational overhead. In our experiments, this simple approach reduces the pretraining cost of BERT by 25\% while achieving similar overall fine-tuning performance on standard downstream tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,nlp,pre-training,token},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2203.13240 [cs]/2022/Hou et al_2022_Token Dropping for Efficient BERT Pretraining.pdf}
}

@article{huang_2022,
  title = {{{LayoutLMv3}}: {{Pre-training}} for {{Document AI}} with {{Unified Text}} and {{Image Masking}}},
  shorttitle = {{{LayoutLMv3}}},
  author = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.08387 [cs]},
  eprint = {2204.08387},
  primaryclass = {cs},
  urldate = {2022-04-19},
  abstract = {Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,layout,layoutlm,nlp,plm},
  annotation = {GSCC: 0000012},
  file = {/Users/chenghao/Zotero/storage/6KT7KPRJ/Huang_et_al_(2022)_LayoutLMv3.pdf;/Users/chenghao/Zotero/storage/SYSEH2JQ/Huang_et_al_(2022)_LayoutLMv3.pdf}
}

@misc{huang_2023,
  title = {Language {{Is Not All You Need}}: {{Aligning Perception}} with {{Language Models}}},
  shorttitle = {Language {{Is Not All You Need}}},
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  year = {2023},
  month = feb,
  number = {arXiv:2302.14045},
  eprint = {2302.14045},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.14045},
  urldate = {2023-02-28},
  abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 3743140, "pages": 26, "previous": "arXiv:2302.14045 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/FX4IH3JF/Huang_et_al_(2023)_Language_Is_Not_All_You_Need.pdf}
}

@misc{hubinger_2024,
  title = {Sleeper {{Agents}}: {{Training Deceptive LLMs}} That {{Persist Through Safety Training}}},
  shorttitle = {Sleeper {{Agents}}},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-15},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  howpublished = {https://arxiv.org/abs/2401.05566v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/TVHG2NGZ/Hubinger et al. - 2024 - Sleeper Agents Training Deceptive LLMs that Persist Through Safety Training.pdf}
}

@article{jaegle_2021,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  shorttitle = {Perceiver {{IO}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and H{\'e}naff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Jo{\~a}o},
  year = {2021},
  month = aug,
  journal = {arXiv:2107.14795 [cs, eess]},
  eprint = {2107.14795},
  primaryclass = {cs, eess},
  urldate = {2021-08-03},
  abstract = {The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.},
  archiveprefix = {arxiv},
  keywords = {bert,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,input,nosource},
  annotation = {34 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/E83UCN2U/Jaegle_et_al_(2021)_Perceiver_IO.pdf}
}

@inproceedings{jain_2020,
  title = {{{SciREX}}: {{A Challenge Dataset}} for {{Document-Level Information Extraction}}},
  shorttitle = {{{SciREX}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jain, Sarthak and {van Zuylen}, Madeleine and Hajishirzi, Hannaneh and Beltagy, Iz},
  year = {2020},
  month = jul,
  pages = {7506--7516},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.670},
  urldate = {2022-10-07},
  abstract = {Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .},
  annotation = {GSCC: 0000071  57 citations (Semantic Scholar/DOI) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/QQCPH6NS/Jain_et_al_(2020)_SciREX.pdf}
}

@article{jiang_2021,
  title = {Delphi: {{Towards Machine Ethics}} and {{Norms}}},
  shorttitle = {Delphi},
  author = {Jiang, Liwei and Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Forbes, Maxwell and Borchardt, Jon and Liang, Jenny and Etzioni, Oren and Sap, Maarten and Choi, Yejin},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07574 [cs]},
  eprint = {2110.07574},
  primaryclass = {cs},
  urldate = {2021-10-16},
  abstract = {What would it take to teach a machine to behave ethically? While broad ethical rules may seem straightforward to state ("thou shalt not kill"), applying such rules to real-world situations is far more complex. For example, while "helping a friend" is generally a good thing to do, "helping a friend spread fake news" is not. We identify four underlying challenges towards machine ethics and norms: (1) an understanding of moral precepts and social norms; (2) the ability to perceive real-world situations visually or by reading natural language descriptions; (3) commonsense reasoning to anticipate the outcome of alternative actions in different contexts; (4) most importantly, the ability to make ethical judgments given the interplay between competing values and their grounding in different contexts (e.g., the right to freedom of expression vs. preventing the spread of fake news). Our paper begins to address these questions within the deep learning paradigm. Our prototype model, Delphi, demonstrates strong promise of language-based commonsense moral reasoning, with up to 92.1\% accuracy vetted by humans. This is in stark contrast to the zero-shot performance of GPT-3 of 52.3\%, which suggests that massive scale alone does not endow pre-trained neural language models with human values. Thus, we present Commonsense Norm Bank, a moral textbook customized for machines, which compiles 1.7M examples of people's ethical judgments on a broad spectrum of everyday situations. In addition to the new resources and baseline performances for future research, our study provides new insights that lead to several important open research questions: differentiating between universal human values and personal values, modeling different moral frameworks, and explainable, consistent approaches to machine ethics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,nlp,social norm},
  annotation = {19 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2110.07574 [cs]/2021/jiang_2021_delphi_-_towards_machine_ethics_and_norms.pdf}
}

@misc{jiang_2022,
  title = {Retrieval as {{Attention}}: {{End-to-end Learning}} of {{Retrieval}} and {{Reading}} within a {{Single Transformer}}},
  shorttitle = {Retrieval as {{Attention}}},
  author = {Jiang, Zhengbao and Gao, Luyu and Araki, Jun and Ding, Haibo and Wang, Zhiruo and Callan, Jamie and Neubig, Graham},
  year = {2022},
  month = dec,
  number = {arXiv:2212.02027},
  eprint = {2212.02027},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.02027},
  urldate = {2022-12-10},
  abstract = {Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents to generate answers. Retrievers and readers are usually modeled separately, which necessitates a cumbersome implementation and is hard to train and adapt in an end-to-end fashion. In this paper, we revisit this design and eschew the separate architecture and training in favor of a single Transformer that performs Retrieval as Attention (ReAtt), and end-to-end training solely based on supervision from the end QA task. We demonstrate for the first time that a single model trained end-to-end can achieve both competitive retrieval and QA performance, matching or slightly outperforming state-of-the-art separately trained retrievers and readers. Moreover, end-to-end adaptation significantly boosts its performance on out-of-domain datasets in both supervised and unsupervised settings, making our model a simple and adaptable solution for knowledge-intensive tasks. Code and models are available at https://github.com/jzbjyb/ReAtt.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-09] 0 citations (Semantic Scholar/DOI) [2022-12-09]},
  file = {/Users/chenghao/Zotero/storage/7EJ7BXX8/Jiang_et_al_(2022)_Retrieval_as_Attention.pdf}
}

@misc{jiang_2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  howpublished = {https://arxiv.org/abs/2310.06825v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/PAA3EDR8/Jiang et al. - 2023 - Mistral 7B.pdf}
}

@misc{jiang_2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-09},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  howpublished = {https://arxiv.org/abs/2401.04088v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/AFY4K6W6/Jiang et al. - 2024 - Mixtral of Experts.pdf}
}

@misc{jin_2024,
  title = {{{LLM Maybe LongLM}}: {{Self-Extend LLM Context Window Without Tuning}}},
  shorttitle = {{{LLM Maybe LongLM}}},
  author = {Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-12},
  abstract = {This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.},
  howpublished = {https://arxiv.org/abs/2401.01325v1},
  langid = {english},
  annotation = {\{"size": -1, "pages": -1, "previous": ""\}},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Jin et_al/2024/Jin_et_al_(2024)_LLM_Maybe_LongLM.pdf}
}

@misc{jones_2023,
  title = {Does {{GPT-4 Pass}} the {{Turing Test}}?},
  author = {Jones, Cameron and Bergen, Benjamin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20216},
  eprint = {2310.20216},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.20216},
  urldate = {2023-12-02},
  abstract = {We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41\% of games, outperforming baselines set by ELIZA (27\%) and GPT-3.5 (14\%), but falling short of chance and the baseline set by human participants (63\%). Participants' decisions were based mainly on linguistic style (35\%) and socio-emotional traits (27\%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/S8DLVSU3/Jones and Bergen - 2023 - Does GPT-4 Pass the Turing Test.pdf}
}

@misc{jurgenschmidhuber_2022,
  title = {Annotated {{History}} of {{Modern AI}} and {{Deep Learning}}},
  author = {{J{\"u}rgen Schmidhuber}},
  year = {2022},
  month = dec,
  keywords = {nosource},
  annotation = {\{"size": 6899575, "pages": 86, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/VI5H6374/Jürgen_Schmidhuber_(2022)_Annotated_History_of_Modern_AI_and_Deep_Learning.pdf}
}

@misc{kadavath_2022,
  title = {Language {{Models}} ({{Mostly}}) {{Know What They Know}}},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and {Tran-Johnson}, Eli and Johnston, Scott and {El-Showk}, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  year = {2022},
  month = jul,
  number = {arXiv:2207.05221},
  eprint = {2207.05221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.05221},
  urldate = {2022-07-23},
  abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-07-23] 1 citations (Semantic Scholar/DOI) [2022-07-23]},
  file = {/Users/chenghao/Zotero/storage/8MUJ4U2J/Kadavath_et_al_(2022)_Language_Models_(Mostly)_Know_What_They_Know.pdf}
}

@misc{kalluri_2023,
  title = {The {{Surveillance AI Pipeline}}},
  author = {Kalluri, Pratyusha Ria and Agnew, William and Cheng, Myra and Owens, Kentrell and Soldaini, Luca and Birhane, Abeba},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-01-03},
  abstract = {A rapidly growing number of voices argue that AI research, and computer vision in particular, is powering mass surveillance. Yet the direct path from computer vision research to surveillance has remained obscured and difficult to assess. Here, we reveal the Surveillance AI pipeline by analyzing three decades of computer vision research papers and downstream patents, more than 40,000 documents. We find the large majority of annotated computer vision papers and patents self-report their technology enables extracting data about humans. Moreover, the majority of these technologies specifically enable extracting data about human bodies and body parts. We present both quantitative and rich qualitative analysis illuminating these practices of human data extraction. Studying the roots of this pipeline, we find that institutions that prolifically produce computer vision research, namely elite universities and "big tech" corporations, are subsequently cited in thousands of surveillance patents. Further, we find consistent evidence against the narrative that only these few rogue entities are contributing to surveillance. Rather, we expose the fieldwide norm that when an institution, nation, or subfield authors computer vision papers with downstream patents, the majority of these papers are used in surveillance patents. In total, we find the number of papers with downstream surveillance patents increased more than five-fold between the 1990s and the 2010s, with computer vision research now having been used in more than 11,000 surveillance patents. Finally, in addition to the high levels of surveillance we find documented in computer vision papers and patents, we unearth pervasive patterns of documents using language that obfuscates the extent of surveillance. Our analysis reveals the pipeline by which computer vision research has powered the ongoing expansion of surveillance.},
  howpublished = {https://arxiv.org/abs/2309.15084v2},
  langid = {english},
  annotation = {\{"size": 3273321, "pages": 23, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/ZB7U2SHI/Kalluri et al. - 2023 - The Surveillance AI Pipeline.pdf}
}

@misc{kandpal_2022,
  title = {Deduplicating {{Training Data Mitigates Privacy Risks}} in {{Language Models}}},
  author = {Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  year = {2022},
  month = feb,
  number = {arXiv:2202.06539},
  eprint = {2202.06539},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.06539},
  urldate = {2022-08-21},
  abstract = {Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated {\textasciitilde}1000 times more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  annotation = {16 citations (Semantic Scholar/arXiv) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/YTDEHYLJ/Kandpal_et_al_(2022)_Deduplicating_Training_Data_Mitigates_Privacy_Risks_in_Language_Models.pdf}
}

@article{kaplan_2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08361 [cs, stat]},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  urldate = {2021-09-11},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,lm,nlp,scale,Statistics - Machine Learning},
  annotation = {383 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2001.08361 [cs, stat]/2020/kaplan_2020_scaling_laws_for_neural_language_models.pdf}
}

@article{keren_2022,
  title = {Breaking {{Character}}: {{Are Subwords Good Enough}} for {{MRLs After All}}?},
  shorttitle = {Breaking {{Character}}},
  author = {Keren, Omri and Avinari, Tal and Tsarfaty, Reut and Levy, Omer},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.04748 [cs]},
  eprint = {2204.04748},
  primaryclass = {cs},
  urldate = {2022-04-30},
  abstract = {Large pretrained language models (PLMs) typically tokenize the input string into contiguous subwords before any pretraining or inference. However, previous studies have claimed that this form of subword tokenization is inadequate for processing morphologically-rich languages (MRLs). We revisit this hypothesis by pretraining a BERT-style masked language model over character sequences instead of word-pieces. We compare the resulting model, dubbed TavBERT, against contemporary PLMs based on subwords for three highly complex and ambiguous MRLs (Hebrew, Turkish, and Arabic), testing them on both morphological and semantic tasks. Our results show, for all tested languages, that while TavBERT obtains mild improvements on surface-level tasks {\textbackslash}`a la POS tagging and full morphological disambiguation, subword-based PLMs achieve significantly higher performance on semantic tasks, such as named entity recognition and extractive question answering. These results showcase and (re)confirm the potential of subword tokenization as a reasonable modeling assumption for many languages, including MRLs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,morphological rich language,nlp,subword,tokenization},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-04-29]},
  file = {/Users/chenghao/Zotero/storage/BR4CSAXG/Keren_et_al_(2022)_Breaking_Character.pdf}
}

@misc{khashabi_2020,
  title = {{{UnifiedQA}}: {{Crossing Format Boundaries With}} a {{Single QA System}}},
  shorttitle = {{{UnifiedQA}}},
  author = {Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  year = {2020},
  month = oct,
  number = {arXiv:2005.00700},
  eprint = {2005.00700},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.00700},
  urldate = {2022-10-31},
  abstract = {Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/XET925BI/Khashabi_et_al_(2020)_UnifiedQA.pdf;/Users/chenghao/Zotero/storage/RHM7IZLN/2005.html}
}

@article{kim_2021,
  title = {Learned {{Token Pruning}} for {{Transformers}}},
  author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Hassoun, Joseph and Keutzer, Kurt},
  year = {2021},
  journal = {arXiv},
  abstract = {A major challenge in deploying transformer models is their prohibitive inference cost, which quadratically scales with the input sequence length. This makes it especially difficult to use transformers for processing long sequences. To address this, we present a novel Learned Token Pruning (LTP) method that reduces redundant tokens as the data passes through the different layers of the transformer. In particular, LTP prunes tokens with an attention score below a threshold value, which is learned during training. Importantly, our threshold based method avoids algorithmically expensive operations such as top-k token selection which are used in prior token pruning methods, and also leads to structured pruning. We extensively test the performance of our approach on multiple GLUE tasks and show that our learned threshold based method consistently outperforms the prior state-of-the-art top-k token based method by up to {\textbackslash}textbackslashtextasciitilde2\% higher accuracy with the same amount of FLOPs. Furthermore, our preliminary results show up to 1.4x and 1.9x throughput improvement on Tesla T4 GPU and Intel Haswell CPU, respectively, with less than 1\% of accuracy drop (and up to 2.1x FLOPs reduction). Our code has been developed in PyTorch and has been open-sourced.},
  keywords = {nlp,nosource,token pruning,transformer}
}

@misc{kim_2022,
  title = {{{OCR-free Document Understanding Transformer}}},
  author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, Jeongyeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  year = {2022},
  month = oct,
  number = {arXiv:2111.15664},
  eprint = {2111.15664},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.15664},
  urldate = {2022-10-28},
  abstract = {Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-10-28]},
  file = {/Users/chenghao/Zotero/storage/75CYXN2T/Kim_et_al_(2022)_OCR-free_Document_Understanding_Transformer.pdf;/Users/chenghao/Zotero/storage/LDKGNYP6/Kim_et_al_(2022)_OCR-free_Document_Understanding_Transformer.pdf;/Users/chenghao/Zotero/storage/GC3QUA4P/2111.html}
}

@misc{kim_2023,
  title = {Cream: {{Visually-Situated Natural Language Understanding}} with {{Contrastive Reading Model}} and {{Frozen Large Language Models}}},
  shorttitle = {Cream},
  author = {Kim, Geewook and Lee, Hodong and Kim, Daehee and Jung, Haeji and Park, Sanghee and Kim, Yoonsik and Yun, Sangdoo and Kil, Taeho and Lee, Bado and Park, Seunghyun},
  year = {2023},
  month = may,
  number = {arXiv:2305.15080},
  eprint = {2305.15080},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15080},
  urldate = {2023-05-25},
  abstract = {Advances in Large Language Models (LLMs) have inspired a surge of research exploring their expansion into the visual domain. While recent models exhibit promise in generating abstract captions for images and conducting natural conversations, their performance on text-rich images leaves room for improvement. In this paper, we propose the Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details typically overlooked by existing methods. Cream integrates vision and auxiliary encoders, complemented by a contrastive feature alignment technique, resulting in a more effective understanding of textual information within document images. Our approach, thus, seeks to bridge the gap between vision and language understanding, paving the way for more sophisticated Document Intelligence Assistants. Rigorous evaluations across diverse tasks, such as visual question answering on document images, demonstrate the efficacy of Cream as a state-of-the-art model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {\{"size": 6931297, "pages": 23, "previous": "arXiv:2305.15080 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/J7I8F5EV/Kim et al. - 2023 - Cream Visually-Situated Natural Language Understa.pdf}
}

@inproceedings{kiveris_2014,
  title = {Connected {{Components}} in {{MapReduce}} and {{Beyond}}},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Kiveris, Raimondas and Lattanzi, Silvio and Mirrokni, Vahab and Rastogi, Vibhor and Vassilvitskii, Sergei},
  year = {2014},
  month = nov,
  series = {{{SOCC}} '14},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2670979.2670997},
  urldate = {2023-03-15},
  abstract = {Computing connected components of a graph lies at the core of many data mining algorithms, and is a fundamental subroutine in graph clustering. This problem is well studied, yet many of the algorithms with good theoretical guarantees perform poorly in practice, especially when faced with graphs with hundreds of billions of edges. In this paper, we design improved algorithms based on traditional MapReduce architecture for large scale data analysis. We also explore the effect of augmenting MapReduce with a distributed hash table (DHT) service. We show that these algorithms have provable theoretical guarantees, and easily outperform previously studied algorithms, sometimes by more than an order of magnitude. In particular, our iterative MapReduce algorithms run 3 to 15 times faster than the best previously studied algorithms, and the MapReduce implementation using a DHT is 10 to 30 times faster than the best previously studied algorithms. These are the fastest algorithms that easily scale to graphs with hundreds of billions of edges.},
  isbn = {978-1-4503-3252-1},
  keywords = {/unread,Connected Components,MapReduce Algorithms},
  annotation = {64 citations (Semantic Scholar/DOI) [2023-03-15]},
  file = {/Users/chenghao/Zotero/storage/DICTWYC5/Kiveris_et_al_(2014)_Connected_Components_in_MapReduce_and_Beyond.pdf}
}

@misc{klaiman_2021,
  title = {{{DocReader}}: {{Bounding-Box Free Training}} of a {{Document Information Extraction Model}}},
  shorttitle = {{{DocReader}}},
  author = {Klaiman, Shachar and Lehne, Marius},
  year = {2021},
  month = may,
  number = {arXiv:2105.04313},
  eprint = {2105.04313},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.04313},
  urldate = {2022-10-31},
  abstract = {Information extraction from documents is a ubiquitous first step in many business applications. During this step, the entries of various fields must first be read from the images of scanned documents before being further processed and inserted into the corresponding databases. While many different methods have been developed over the past years in order to automate the above extraction step, they all share the requirement of bounding-box or text segment annotations of their training documents. In this work we present DocReader, an end-to-end neural-network-based information extraction solution which can be trained using solely the images and the target values that need to be read. The DocReader can thus leverage existing historical extraction data, completely eliminating the need for any additional annotations beyond what is naturally available in existing human-operated service centres. We demonstrate that the DocReader can reach and surpass other methods which require bounding-boxes for training, as well as provide a clear path for continual learning during its deployment in production.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/2XZC5UL8/Klaiman_Lehne_(2021)_DocReader.pdf;/Users/chenghao/Zotero/storage/LJKC8CWT/2105.html}
}

@misc{kleinberg_2022,
  title = {Textwash -- Automated Open-Source Text Anonymisation},
  author = {Kleinberg, Bennett and Davies, Toby and Mozes, Maximilian},
  year = {2022},
  month = aug,
  number = {arXiv:2208.13081},
  eprint = {2208.13081},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-05},
  abstract = {The increased use of text data in social science research has benefited from easy-to-access data (e.g., Twitter). That trend comes at the cost of research requiring sensitive but hard-to-share data (e.g., interview data, police reports, electronic health records). We introduce a solution to that stalemate with the open-source text anonymisation software\_Textwash\_. This paper presents the empirical evaluation of the tool using the TILD criteria: a technical evaluation (how accurate is the tool?), an information loss evaluation (how much information is lost in the anonymisation process?) and a de-anonymisation test (can humans identify individuals from anonymised text data?). The findings suggest that Textwash performs similar to state-of-the-art entity recognition models and introduces a negligible information loss of 0.84\%. For the de-anonymisation test, we tasked humans to identify individuals by name from a dataset of crowdsourced person descriptions of very famous, semi-famous and non-existing individuals. The de-anonymisation rate ranged from 1.01-2.01\% for the realistic use cases of the tool. We replicated the findings in a second study and concluded that Textwash succeeds in removing potentially sensitive information that renders detailed person descriptions practically anonymous.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Computers and Society},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-07-06]},
  file = {/Users/chenghao/Zotero/storage/HR5DWPCG/Kleinberg et al. - 2022 - Textwash -- automated open-source text anonymisati.pdf;/Users/chenghao/Zotero/storage/GWG3XRLV/2208.html}
}

@misc{kocetkov_2022,
  title = {The {{Stack}}: 3 {{TB}} of Permissively Licensed Source Code},
  shorttitle = {The {{Stack}}},
  author = {Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Ferrandis, Carlos Mu{\~n}oz and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and {von Werra}, Leandro and {de Vries}, Harm},
  year = {2022},
  month = nov,
  number = {arXiv:2211.15533},
  eprint = {2211.15533},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.15533},
  urldate = {2022-12-20},
  abstract = {Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,nosource},
  file = {/Users/chenghao/Zotero/storage/LA6EBPZS/Kocetkov et al. - 2022 - The Stack 3 TB of permissively licensed source code.pdf}
}

@misc{kojima_2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  month = may,
  number = {arXiv:2205.11916},
  eprint = {2205.11916},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.11916},
  urldate = {2022-05-27},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with an off-the-shelf 175B parameter model. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted through simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,few-shot,PLM,zero-shot},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-05-26]},
  file = {/Users/chenghao/Zotero/storage/QF5YF25E/Kojima_et_al_(2022)_Large_Language_Models_are_Zero-Shot_Reasoners.pdf}
}

@article{koreeda_2021,
  title = {{{ContractNLI}}: {{A Dataset}} for {{Document-level Natural Language Inference}} for {{Contracts}}},
  shorttitle = {{{ContractNLI}}},
  author = {Koreeda, Yuta and Manning, Christopher D.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.01799 [cs]},
  eprint = {2110.01799},
  primaryclass = {cs},
  urldate = {2021-10-06},
  abstract = {Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose "document-level natural language inference (NLI) for contracts", a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as "Some obligations of Agreement may survive termination.") and a contract, and it is asked to classify whether each hypothesis is "entailed by", "contradicting to" or "not mentioned by" (neutral to) the contract as well as identifying "evidence" for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (1) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (2) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,contracts,dataset,document,nlp},
  annotation = {3 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2110.01799 [cs]/2021/koreeda_manning_2021_contractnli_-_a_dataset_for_document-level_natural_language_inference_for.pdf}
}

@misc{krishna_2023,
  title = {{{USB}}: {{A Unified Summarization Benchmark Across Tasks}} and {{Domains}}},
  shorttitle = {{{USB}}},
  author = {Krishna, Kundan and Gupta, Prakhar and Ramprasad, Sanjana and Wallace, Byron C. and Bigham, Jeffrey P. and Lipton, Zachary C.},
  year = {2023},
  month = may,
  number = {arXiv:2305.14296},
  eprint = {2305.14296},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14296},
  urldate = {2023-05-24},
  abstract = {An abundance of datasets exist for training and evaluating models on the task of summary generation.However, these datasets are often derived heuristically, and lack sufficient annotations to support research into all aspects of summarization, such as evidence extraction and controllable summarization. We introduce a benchmark comprising 8 tasks that require multi-dimensional understanding of summarization, e.g., surfacing evidence for a summary, assessing its correctness, and gauging its relevance to different topics. We compare various methods on this benchmark and discover that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models. For factuality related tasks, we also evaluate existing heuristics to create training data and find that training on them performs worse than training on \$20{\textbackslash}times\$ less human-labeled data. Our benchmark consists of data from 6 different domains, allowing us to study cross-domain performance of trained models. We find that for some tasks, the amount of training data matters more than the domain where it comes from, while for other tasks training specifically on data from the target domain, even if limited, is more beneficial. Our work fulfills the need for a well-annotated summarization benchmark with diverse tasks, and provides useful insights about the impact of the quality, size and domain of training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 1756720, "pages": 18, "previous": "arXiv:2305.14296 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/HC492MGS/Krishna et al. - 2023 - USB A Unified Summarization Benchmark Across Task.pdf}
}

@article{krzywinski_2013,
  title = {Error Bars},
  author = {Krzywinski, Martin and Altman, Naomi},
  year = {2013},
  month = oct,
  journal = {Nature Methods},
  volume = {10},
  number = {10},
  pages = {921--922},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2659},
  urldate = {2023-05-08},
  abstract = {The meaning of error bars is often misinterpreted, as is the statistical significance of their overlap.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Publishing,Research data,Statistical methods},
  file = {/Users/chenghao/Zotero/storage/PUE4BBCE/Krzywinski and Altman - 2013 - Error bars.pdf}
}

@misc{kudugunta_2023,
  title = {{{MADLAD-400}}: {{A Multilingual And Document-Level Large Audited Dataset}}},
  shorttitle = {{{MADLAD-400}}},
  author = {Kudugunta, Sneha and Caswell, Isaac and Zhang, Biao and Garcia, Xavier and {Choquette-Choo}, Christopher A. and Lee, Katherine and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan},
  year = {2023},
  month = sep,
  number = {arXiv:2309.04662},
  eprint = {2309.04662},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.04662},
  urldate = {2023-09-13},
  abstract = {We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/MLTNEEKK/Kudugunta et al. - 2023 - MADLAD-400 A Multilingual And Document-Level Large Audited Dataset.pdf}
}

@article{kusupati_2022,
  title = {Matryoshka {{Representation Learning}}},
  author = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and {Howard-Snyder}, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {30233--30249},
  urldate = {2024-01-27},
  langid = {english},
  annotation = {\{"size": 6759513, "pages": 17, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/66D5DK63/Kusupati_et_al_(2022)_Matryoshka_Representation_Learning.pdf}
}

@article{kuznetsov_2021,
  title = {Spelling {{Correction}} with {{Denoising Transformer}}},
  author = {Kuznetsov, Alex and Urdiales, Hector},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05977 [cs]},
  eprint = {2105.05977},
  primaryclass = {cs},
  urldate = {2022-03-23},
  abstract = {We present a novel method of performing spelling correction on short input strings, such as search queries or individual words. At its core lies a procedure for generating artificial typos which closely follow the error patterns manifested by humans. This procedure is used to train the production spelling correction model based on a transformer architecture. This model is currently served in the HubSpot product search. We show that our approach to typo generation is superior to the widespread practice of adding noise, which ignores human patterns. We also demonstrate how our approach may be extended to resource-scarce settings and train spelling correction models for Arabic, Greek, Russian, and Setswana languages, without using any labeled data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,nlp,spelling correction,transformer},
  annotation = {2 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/LMCB3YA5/Kuznetsov_Urdiales_2021_Spelling Correction with Denoising Transformer_annotated.pdf}
}

@misc{kwon_2023,
  title = {Efficient {{Memory Management}} for {{Large Language Model Serving}} with {{PagedAttention}}},
  author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06180},
  eprint = {2309.06180},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.06180},
  urldate = {2023-12-21},
  abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  annotation = {\{"size": 1459631, "pages": 16, "previous": "arXiv:2309.06180 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/JUIDZQDG/Kwon_et_al_(2023)_Efficient_Memory_Management_for_Large_Language_Model_Serving_with_PagedAttention.pdf}
}

@misc{lacki_2018,
  title = {Connected {{Components}} at {{Scale}} via {{Local Contractions}}},
  author = {{\L}{\k a}cki, Jakub and Mirrokni, Vahab and W{\l}odarczyk, Micha{\l}},
  year = {2018},
  month = jul,
  number = {arXiv:1807.10727},
  eprint = {1807.10727},
  publisher = {arXiv},
  urldate = {2023-09-17},
  abstract = {As a fundamental tool in hierarchical graph clustering, computing connected components has been a central problem in large-scale data mining. While many known algorithms have been developed for this problem, they are either not scalable in practice or lack strong theoretical guarantees on the parallel running time, that is, the number of communication rounds. So far, the best proven guarantee is \${\textbackslash}Oh({\textbackslash}log n)\$, which matches the running time in the PRAM model. In this paper, we aim to design a distributed algorithm for this problem that works well in theory and practice. In particular, we present a simple algorithm based on contractions and provide a scalable implementation of it in MapReduce. On the theoretical side, in addition to showing \${\textbackslash}Oh({\textbackslash}log n)\$ convergence for all graphs, we prove an \${\textbackslash}Oh({\textbackslash}log {\textbackslash}log n)\$ parallel running time with high probability for a certain class of random graphs. We work in the MPC model that captures popular parallel computing frameworks, such as MapReduce, Hadoop or Spark. On the practical side, we show that our algorithm outperforms the state-of-the-art MapReduce algorithms. To confirm its scalability, we report empirical results on graphs with several trillions of edges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Distributed Parallel and Cluster Computing},
  annotation = {\{"size": 344578, "pages": 12, "previous": "arXiv:1807.10727 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/GXVNVFT6/Łącki et al. - 2018 - Connected Components at Scale via Local Contractions.pdf}
}

@misc{landeghem_2023,
  title = {Document {{Understanding Dataset}} and {{Evaluation}} ({{DUDE}})},
  author = {Landeghem, Jordy and Tito, Rub{\'e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and J{\'o}ziak, Pawe{\l} and Powalski, Rafa{\l} and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Ackaert, Bertrand and Valveny, Ernest and Blaschko, Matthew and Moens, Sien and Stanis{\l}awek, Tomasz},
  year = {2023},
  month = may,
  number = {arXiv:2305.08455},
  eprint = {2305.08455},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.08455},
  urldate = {2023-05-16},
  abstract = {We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance of finding more efficient ways to model language, images, and layout in DocAI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 2472696, "pages": 22, "previous": "arXiv:2305.08455 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/PJHTRVXH/Landeghem et al. - 2023 - Document Understanding Dataset and Evaluation (DUD.pdf}
}

@inproceedings{laurencon_2022,
  title = {The {{BigScience ROOTS Corpus}}: {{A}} 1.{{6TB Composite Multilingual Dataset}}},
  shorttitle = {The {{BigScience ROOTS Corpus}}},
  booktitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Lauren{\c c}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Scao, Teven Le and Werra, Leandro Von and Mou, Chenghao and Ponferrada, Eduardo Gonz{\'a}lez and Nguyen, Huu and Frohberg, J{\"o}rg and {\v S}a{\v s}ko, Mario and Lhoest, Quentin and {McMillan-Major}, Angelina and Dupont, G{\'e}rard and Biderman, Stella and Rogers, Anna and Allal, Loubna Ben and Toni, Francesco De and Pistilli, Giada and Nguyen, Olivier and Nikpoor, Somaieh and Masoud, Maraim and Colombo, Pierre and de la Rosa, Javier and Villegas, Paulo and Thrush, Tristan and Longpre, Shayne and Nagel, Sebastian and Weber, Leon and Mu{\~n}oz, Manuel Romero and Zhu, Jian and Strien, Daniel Van and Alyafeai, Zaid and Almubarak, Khalid and Chien, Vu Minh and {Gonzalez-Dios}, Itziar and Soroa, Aitor and Lo, Kyle and Dey, Manan and Suarez, Pedro Ortiz and Gokaslan, Aaron and Bose, Shamik and Adelani, David Ifeoluwa and Phan, Long and Tran, Hieu and Yu, Ian and Pai, Suhas and Chim, Jenny and Lepercq, Violette and Ilic, Suzana and Mitchell, Margaret and Luccioni, Sasha and Jernite, Yacine},
  year = {2022},
  month = oct,
  urldate = {2022-12-20},
  abstract = {As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {nosource},
  file = {/Users/chenghao/Zotero/storage/KK7EIDKL/Laurençon et al. - 2022 - The BigScience ROOTS Corpus A 1.6TB Composite Multilingual Dataset.pdf}
}

@misc{lee_2022,
  title = {Do {{Language Models Plagiarize}}?},
  author = {Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
  year = {2022},
  month = mar,
  number = {arXiv:2203.07618},
  eprint = {2203.07618},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.07618},
  urldate = {2023-01-06},
  abstract = {Past literature has illustrated that language models do not fully understand the context and sensitivity of text and can sometimes memorize phrases or sentences present in their training sets. In this paper, we investigate whether they not only memorize but also plagiarize training samples when generating artificial texts. Our findings support that they, especially GPT-2, reuse particular pieces of texts from the training corpus with or without obfuscation. We have four main results: 1) language models with more capacity plagiarize more; 2) fine-tuned language models demonstrate differing patterns of plagiarism based on characteristics of auxiliary data; 3) sampling from truncated language modeling distributions tends to heighten the degree of plagiarism as opposed to temperature sampling, and 4) plagiarism in language models can have serious privacy consequences. Overall, our work implies that future research on neural language models should take precautions to avoid models plagiarizing their training datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {1 citations (Semantic Scholar/arXiv) [2023-01-05] 1 citations (Semantic Scholar/DOI) [2023-01-05]},
  file = {/Users/chenghao/Zotero/storage/RKF678WJ/Lee_et_al_(2022)_Do_Language_Models_Plagiarize.pdf}
}

@misc{lee_2022a,
  title = {Deduplicating {{Training Data Makes Language Models Better}}},
  author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and {Callison-Burch}, Chris and Carlini, Nicholas},
  year = {2022},
  month = mar,
  number = {arXiv:2107.06499},
  eprint = {2107.06499},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.06499},
  urldate = {2023-03-12},
  abstract = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1\% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4\% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {101 citations (Semantic Scholar/arXiv) [2023-03-12]},
  file = {/Users/chenghao/Zotero/storage/SVZ4YK4G/Lee et al. - 2022 - Deduplicating Training Data Makes Language Models .pdf}
}

@misc{lenat_2023,
  title = {Getting from {{Generative AI}} to {{Trustworthy AI}}: {{What LLMs}} Might Learn from {{Cyc}}},
  shorttitle = {Getting from {{Generative AI}} to {{Trustworthy AI}}},
  author = {Lenat, Doug and Marcus, Gary},
  year = {2023},
  month = jul,
  number = {arXiv:2308.04445},
  eprint = {2308.04445},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.04445},
  urldate = {2023-08-10},
  abstract = {Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable. We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however a catch: if the logical language is expressive enough to fully represent the meaning of anything we can say in English, then the inference engine runs much too slowly. That's why symbolic AI systems typically settle for some fast but much less expressive logic, such as knowledge graphs. We describe how one AI system, Cyc, has developed ways to overcome that tradeoff and is able to reason in higher order logic in real time. We suggest that any trustworthy general AI will need to hybridize the approaches, the LLM approach and more formal approach, and lay out a path to realizing that dream.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0},
  annotation = {\{"size": 415258, "pages": 21, "previous": "arXiv:2308.04445 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/FQXV2AM6/Lenat and Marcus - 2023 - Getting from Generative AI to Trustworthy AI What.pdf}
}

@misc{leong_2019,
  type = {{{SSRN Scholarly Paper}}},
  title = {Robot {{Eyes Wide Shut}}: {{Understanding Dishonest Anthropomorphism}}},
  shorttitle = {Robot {{Eyes Wide Shut}}},
  author = {Leong, Brenda and Selinger, Evan},
  year = {2019},
  month = jan,
  number = {3762223},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.3762223},
  urldate = {2023-05-08},
  abstract = {The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, ``Averting Robot Eyes.'' Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.},
  langid = {english},
  keywords = {anthropomorphism,boundary management theory,deception,ethics,privacy,robotics},
  annotation = {\{"size": 452529, "pages": 16, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/HY4TFYQH/Leong and Selinger - 2019 - Robot Eyes Wide Shut Understanding Dishonest Anth.pdf}
}

@misc{leviathan_2023,
  title = {Fast {{Inference}} from {{Transformers}} via {{Speculative Decoding}}},
  author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  year = {2023},
  month = may,
  number = {arXiv:2211.17192},
  eprint = {2211.17192},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.17192},
  urldate = {2023-09-02},
  abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/SU5Z4XYC/Leviathan et al. - 2023 - Fast Inference from Transformers via Speculative Decoding.pdf}
}

@misc{lewistunstall_,
  title = {Natural {{Language Processing}} with {{Transformers}}, {{Revised Edition}}},
  author = {{Lewis Tunstall} and {Leandro von Werra} and {Thomas Wolf}},
  urldate = {2023-03-21},
  abstract = {Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or {\dots} - Selection from Natural Language Processing with Transformers, Revised Edition [Book]},
  howpublished = {https://www.oreilly.com/library/view/natural-language-processing/9781098136789/},
  isbn = {9781098136796},
  langid = {english},
  keywords = {/unread}
}

@article{lhoest_2021,
  title = {Datasets: {{A Community Library}} for {{Natural Language Processing}}},
  shorttitle = {Datasets},
  author = {Lhoest, Quentin and {del Moral}, Albert Villanova and Jernite, Yacine and Thakur, Abhishek and {von Platen}, Patrick and Patil, Suraj and Chaumond, Julien and Drame, Mariama and Plu, Julien and Tunstall, Lewis and Davison, Joe and {\v S}a{\v s}ko, Mario and Chhablani, Gunjan and Malik, Bhavitvya and Brandeis, Simon and Scao, Teven Le and Sanh, Victor and Xu, Canwen and Patry, Nicolas and {McMillan-Major}, Angelina and Schmid, Philipp and Gugger, Sylvain and Delangue, Cl{\'e}ment and Matussi{\`e}re, Th{\'e}o and Debut, Lysandre and Bekman, Stas and Cistac, Pierric and Goehringer, Thibault and Mustar, Victor and Lagunas, Fran{\c c}ois and Rush, Alexander M. and Wolf, Thomas},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.02846 [cs]},
  eprint = {2109.02846},
  primaryclass = {cs},
  urldate = {2021-09-08},
  abstract = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,datasets,huggingface,nlp},
  annotation = {31 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2109.02846 [cs]/2021/lhoest_2021_datasets_-_a_community_library_for_natural_language_processing2.pdf}
}

@article{li_2020,
  title = {{{DocBank}}: {{A Benchmark Dataset}} for {{Document Layout Analysis}}},
  shorttitle = {{{DocBank}}},
  author = {Li, Minghao and Xu, Yiheng and Cui, Lei and Huang, Shaohan and Wei, Furu and Li, Zhoujun and Zhou, Ming},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.01038 [cs]},
  eprint = {2006.01038},
  primaryclass = {cs},
  urldate = {2022-02-07},
  abstract = {Document layout analysis usually relies on computer vision models to understand documents while ignoring textual information that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present {\textbackslash}textbf\{DocBank\}, a benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the {\textbackslash}LaTeX\{\} documents available on the arXiv.com. With DocBank, models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of document layout analysis. We build several strong baselines and manually split train/dev/test sets for evaluation. Experiment results show that models trained on DocBank accurately recognize the layout information for a variety of documents. The DocBank dataset is publicly available at {\textbackslash}url\{https://github.com/doc-analysis/DocBank\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,dataset,document,nlp},
  annotation = {42 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/CAA8EFJD/Li_et_al_(2020)_DocBank.pdf}
}

@article{li_2021,
  title = {{{TrOCR}}: {{Transformer-based Optical Character Recognition}} with {{Pre-trained Models}}},
  shorttitle = {{{TrOCR}}},
  author = {Li, Minghao and Lv, Tengchao and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
  year = {2021},
  month = sep,
  urldate = {2021-09-23},
  abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches for text recognition are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on both printed and handwritten text recognition tasks. The code and models will be publicly available at https://aka.ms/TrOCR.},
  langid = {english},
  keywords = {nlp,ocr,plm,transformer},
  annotation = {5 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/undefined/2021/li_2021_trocr_-_transformer-based_optical_character_recognition_with_pre-trained_models.pdf}
}

@inproceedings{li_2022,
  title = {{{MarkupLM}}: {{Pre-training}} of {{Text}} and {{Markup Language}} for {{Visually Rich Document Understanding}}},
  shorttitle = {{{MarkupLM}}},
  booktitle = {{{ACL}}},
  author = {Li, Junlong and Xu, Yiheng and Cui, Lei and Wei, Furu},
  year = {2022},
  doi = {10.18653/v1/2022.acl-long.420},
  abstract = {This paper proposes MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Multimodal pre-training with text, layout, and image has made significant progress for Visually Rich Document Understanding (VRDU), especially the fixed-layout documents such as scanned document images. While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks. The pre-trained model and code will be publicly available at https://aka.ms/markuplm.},
  annotation = {10 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000006},
  file = {/Users/chenghao/Zotero/storage/7QKFCIPS/Li_et_al_(2022)_MarkupLM.pdf}
}

@article{li_2022a,
  title = {{{DiT}}: {{Self-supervised Pre-training}} for {{Document Image Transformer}}},
  shorttitle = {{{DiT}}},
  author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.02378 [cs]},
  eprint = {2203.02378},
  primaryclass = {cs},
  urldate = {2022-03-10},
  abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, as well as table detection. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \${\textbackslash}rightarrow\$ 92.69), document layout analysis (91.0 \${\textbackslash}rightarrow\$ 94.9) and table detection (94.23 \${\textbackslash}rightarrow\$ 96.55). The code and pre-trained models are publicly available at {\textbackslash}url\{https://aka.ms/msdit\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,document,nlp,transformer},
  annotation = {GSCC: 0000008  0 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/6LJPAXWE/Li_et_al_(2022)_DiT.pdf;/Users/chenghao/Zotero/storage/PKP4VARX/Li_et_al_(2022)_DiT.pdf}
}

@misc{li_2022b,
  title = {Branch-{{Train-Merge}}: {{Embarrassingly Parallel Training}} of {{Expert Language Models}}},
  shorttitle = {Branch-{{Train-Merge}}},
  author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03306},
  eprint = {2208.03306},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.03306},
  urldate = {2022-08-20},
  abstract = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-08-20] 1 citations (Semantic Scholar/DOI) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/C6SCND92/Li_et_al_(2022)_Branch-Train-Merge.pdf}
}

@article{li_2022c,
  title = {Competition-{{Level Code Generation}} with {{AlphaCode}}},
  author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and {d'Autume}, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and {de Freitas}, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  year = {2022},
  month = dec,
  journal = {Science},
  volume = {378},
  number = {6624},
  eprint = {2203.07814},
  primaryclass = {cs},
  pages = {1092--1097},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abq1158},
  urldate = {2023-03-18},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  annotation = {173 citations (Semantic Scholar/arXiv) [2023-03-18] 173 citations (Semantic Scholar/DOI) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/U4J685K8/Li_et_al_(2022)_Competition-Level_Code_Generation_with_AlphaCode.pdf}
}

@misc{li_2023,
  title = {Internet {{Explorer}}: {{Targeted Representation Learning}} on the {{Open Web}}},
  shorttitle = {Internet {{Explorer}}},
  author = {Li, Alexander C. and Brown, Ellis and Efros, Alexei A. and Pathak, Deepak},
  year = {2023},
  month = feb,
  number = {arXiv:2302.14051},
  eprint = {2302.14051},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.14051},
  urldate = {2023-02-28},
  abstract = {Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30--40 hours. Results, visualizations, and videos at https://internet-explorer-ssl.github.io/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/Users/chenghao/Zotero/storage/72MANP75/Li_et_al_(2023)_Internet_Explorer.pdf}
}

@misc{li_2023a,
  title = {Task {{Contamination}}: {{Language Models May Not Be Few-Shot Anymore}}},
  shorttitle = {Task {{Contamination}}},
  author = {Li, Changmao and Flanigan, Jeffrey},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-01-01},
  abstract = {Large language models (LLMs) offer impressive performance in various zero-shot and few-shot tasks. However, their success in zero-shot and few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined. This paper investigates how zero-shot and few-shot performance of LLMs has changed chronologically over time. Utilizing GPT-3 series models and several other recent open-sourced LLMs, and controlling for dataset difficulty, we find that on datasets released before the LLM training data creation date, LLMs perform surprisingly better than on datasets released after. This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets released prior to the LLMs' training data creation date. Additionally, we utilize training data inspection, task example extraction, and a membership inference attack, which reveal further evidence of task contamination. Importantly, we find that for classification tasks with no possibility of task contamination, LLMs rarely demonstrate statistically significant improvements over simple majority baselines, in both zero and few-shot settings.},
  howpublished = {https://arxiv.org/abs/2312.16337v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/JG62PR2J/Li and Flanigan - 2023 - Task Contamination Language Models May Not Be Few-Shot Anymore.pdf}
}

@misc{lialin_2023,
  title = {Scaling {{Down}} to {{Scale Up}}: {{A Guide}} to {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {Scaling {{Down}} to {{Scale Up}}},
  author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  year = {2023},
  month = mar,
  number = {arXiv:2303.15647},
  eprint = {2303.15647},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.15647},
  urldate = {2023-03-29},
  abstract = {This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {\{"size": 860410, "pages": 21, "previous": "0 citations (Semantic Scholar/arXiv) [2023-03-29]{\textbackslash}narXiv:2303.15647 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/B4XJ674F/Lialin_et_al_(2023)_Scaling_Down_to_Scale_Up.pdf}
}

@misc{liang_2023,
  title = {Unleashing {{Infinite-Length Input Capacity}} for {{Large-scale Language Models}} with {{Self-Controlled Memory System}}},
  author = {Liang, Xinnian and Wang, Bing and Huang, Hui and Wu, Shuangzhi and Wu, Peihao and Lu, Lu and Ma, Zejun and Li, Zhoujun},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13343},
  eprint = {2304.13343},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13343},
  urldate = {2023-04-27},
  abstract = {Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not optimized for multi-turn dialogue, to achieve multi-turn dialogue capabilities that are comparable to ChatGPT, and to outperform ChatGPT in scenarios involving ultra-long document summarization or long-term conversations. Additionally, we will supply a test set, which covers common long-text input scenarios, for evaluating the abilities of LLMs in processing long documents.{\textasciitilde}{\textbackslash}footnote\{Working in progress.\}{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/wbbeyourself/SCM4LLMs\}\}},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/EZPS3CTL/Liang et al. - 2023 - Unleashing Infinite-Length Input Capacity for Larg.pdf}
}

@misc{liang_2024,
  title = {Monitoring {{AI-Modified Content}} at {{Scale}}: {{A Case Study}} on the {{Impact}} of {{ChatGPT}} on {{AI Conference Peer Reviews}}},
  shorttitle = {Monitoring {{AI-Modified Content}} at {{Scale}}},
  author = {Liang, Weixin and Izzo, Zachary and Zhang, Yaohui and Lepp, Haley and Cao, Hancheng and Zhao, Xuandong and Chen, Lingjiao and Ye, Haotian and Liu, Sheng and Huang, Zhi and McFarland, Daniel A. and Zou, James Y.},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-17},
  abstract = {We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5\% and 16.9\% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.},
  howpublished = {https://arxiv.org/abs/2403.07183v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/WQXFWXF2/Liang_et_al_(2024)_Monitoring_AI-Modified_Content_at_Scale.pdf}
}

@misc{liao_2024,
  title = {Adding {{NVMe SSDs}} to {{Enable}} and {{Accelerate 100B Model Fine-tuning}} on a {{Single GPU}}},
  author = {Liao, Changyue and Sun, Mo and Yang, Zihan and Chen, Kaiqi and Yuan, Binhang and Wu, Fei and Wang, Zeke},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-12},
  abstract = {Recent advances in large language models have brought immense value to the world, with their superior capabilities stemming from the massive number of parameters they utilize. However, even the GPUs with the highest memory capacities, currently peaking at 80GB, are far from sufficient to accommodate these vast parameters and their associated optimizer states when conducting stochastic gradient descent-based optimization. One approach to hosting such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most academic researchers, who always have a limited budget for many high-end GPU servers. In this paper, we focus on huge model fine-tuning on a single, even low-end, GPU in a commodity server, which is accessible to most AI researchers. In such a scenario, the state-of-the-art work ZeRO-Infinity suffers from two severe issues when running in a commodity server: 1) low GPU utilization due to inefficient swapping, and 2) limited trainable model size due to CPU memory capacity. The underlying reason is that ZeRO-Infinity is optimized for running on high-end GPU servers. To this end, we present Fuyou, a low-cost training framework that enables efficient 100B huge model fine-tuning on a low-end server with a low-end GPU and limited CPU memory capacity. The key idea is to add the SSD-CPU communication as an optimization dimension and thus carefully co-optimize computation and data swapping from a systematic approach to maximize GPU utilization. The experimental results show that 1) Fuyou is able to fine-tune 175B GPT-3 on a consumer GPU RTX 4090 with high GPU utilization, while ZeRO-Infinity fails to fine-tune; and 2) when training a small GPT-3 13B model, Fuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves 45 TFLOPS.},
  howpublished = {https://arxiv.org/abs/2403.06504v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/9BUYR8DF/Liao et al. - 2024 - Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU.pdf}
}

@misc{liesenfeld_2023,
  title = {Opening up {{ChatGPT}}: {{Tracking}} Openness, Transparency, and Accountability in Instruction-Tuned Text Generators},
  shorttitle = {Opening up {{ChatGPT}}},
  author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
  year = {2023},
  month = jul,
  eprint = {2307.05532},
  primaryclass = {cs},
  doi = {10.1145/3571884.3604316},
  urldate = {2023-07-17},
  abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI's ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-07-17] 0 citations (Semantic Scholar/DOI) [2023-07-17]},
  file = {/Users/chenghao/Zotero/storage/DB656MRP/Liesenfeld et al. - 2023 - Opening up ChatGPT Tracking openness, transparenc.pdf}
}

@article{lin_2021,
  title = {{{ViBERTgrid}}: {{A Jointly Trained Multi-Modal 2D Document Representation}} for {{Key Information Extraction}} from {{Documents}}},
  shorttitle = {{{ViBERTgrid}}},
  author = {Lin, Weihong and Gao, Qifang and Sun, Lei and Zhong, Zhuoyao and Hu, Kaiqin and Ren, Qin and Huo, Qiang},
  year = {2021},
  journal = {ICDAR},
  doi = {10.1007/978-3-030-86549-8_35},
  abstract = {This paper proposes a new multi-modal backbone network by concatenating a BERTgrid to an intermediate layer of a CNN model, to generate a more powerful grid-based document representation, named ViBERTgrid, which has achieved state-of-the-art performance on real-world datasets. Recent grid-based document representations like BERTgrid allow the simultaneous encoding of the textual and layout information of a document in a 2D feature map so that state-of-the-art image segmentation and/or object detection models can be straightforwardly leveraged to extract key information from documents. However, such methods have not achieved comparable performance to state-of-the-art sequenceand graph-based methods such as LayoutLM and PICK yet. In this paper, we propose a new multi-modal backbone network by concatenating a BERTgrid to an intermediate layer of a CNN model, where the input of CNN is a document image and the BERTgrid is a grid of word embeddings, to generate a more powerful grid-based document representation, named ViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal backbone network are trained jointly. Our experimental results demonstrate that this joint training strategy improves significantly the representation ability of ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction approach has achieved state-of-the-art performance on real-world datasets.},
  annotation = {7 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000011},
  file = {/Users/chenghao/Zotero/storage/N62XZ3KQ/Lin_et_al_(2021)_ViBERTgrid.pdf}
}

@article{lin_2021a,
  title = {Few-Shot {{Learning}} with {{Multilingual Language Models}}},
  author = {Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and Pasunuru, Ramakanth and Shleifer, Sam and Koura, Punit Singh and Chaudhary, Vishrav and O'Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Kozareva, Zornitsa and Diab, Mona and Stoyanov, Veselin and Li, Xian},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.10668 [cs]},
  eprint = {2112.10668},
  primaryclass = {cs},
  urldate = {2021-12-24},
  abstract = {Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4\% absolute accuracy improvement in 0-shot settings and +9.4\% in 4-shot settings) and natural language inference (+5.4\% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model succeeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,data,few-shot learning,lm,multilingual,nlp},
  annotation = {7 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2112.10668 [cs]/2021/Lin et al_2021_Few-shot Learning with Multilingual Language Models.pdf}
}

@misc{lin_2024,
  title = {{{MaLA-500}}: {{Massive Language Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{MaLA-500}}},
  author = {Lin, Peiqin and Ji, Shaoxiong and Tiedemann, J{\"o}rg and Martins, Andr{\'e} F. T. and Sch{\"u}tze, Hinrich},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Large language models have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves state-of-the-art in-context learning results. We release MaLA-500 at https://huggingface.co/MaLA-LM},
  howpublished = {https://arxiv.org/abs/2401.13303v1},
  langid = {english},
  annotation = {\{"size": 412975, "pages": 14, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/8GRFS38Q/Lin et al. - 2024 - MaLA-500 Massive Language Adaptation of Large Language Models.pdf}
}

@inproceedings{lison_2021,
  title = {Anonymisation {{Models}} for {{Text Data}}: {{State}} of the Art, {{Challenges}} and {{Future Directions}}},
  shorttitle = {Anonymisation {{Models}} for {{Text Data}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lison, Pierre and Pil{\'a}n, Ildik{\'o} and Sanchez, David and Batet, Montserrat and {\O}vrelid, Lilja},
  year = {2021},
  month = aug,
  pages = {4188--4203},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.323},
  urldate = {2023-07-10},
  abstract = {This position paper investigates the problem of automated text anonymisation, which is a prerequisite for secure sharing of documents containing sensitive information about individuals. We summarise the key concepts behind text anonymisation and provide a review of current approaches. Anonymisation methods have so far been developed in two fields with little mutual interaction, namely natural language processing and privacy-preserving data publishing. Based on a case study, we outline the benefits and limitations of these approaches and discuss a number of open challenges, such as (1) how to account for multiple types of semantic inferences, (2) how to strike a balance between disclosure risk and data utility and (3) how to evaluate the quality of the resulting anonymisation. We lay out a case for moving beyond sequence labelling models and incorporate explicit measures of disclosure risk into the text anonymisation process.},
  keywords = {/unread},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/chenghao/Zotero/storage/NDVLSZSI/Lison et al. - 2021 - Anonymisation Models for Text Data State of the a.pdf}
}

@article{liu_,
  title = {Towards {{Efficient NLP}}: {{A Standard Evaluation}} and {{A Strong Baseline}}},
  author = {Liu, Xiangyang and Sun, Tianxiang and He, Junliang and Wu, Lingling and Zhang, Xinyu and Jiang, Hao and Cao, Zhao and Huang, Xuanjing and Qiu, Xipeng},
  pages = {15},
  abstract = {Supersized pre-trained language models have pushed the accuracy of various NLP tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, most works are pursuing improvement on other dimensions such as efficiency, leading to "Pareto SOTA". Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and public leaderboard for efficient NLP models. ELUE is dedicated to depict the Pareto Front for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also pre-train and release a strong baseline, ElasticBERT, whose elasticity is both static and dynamic. ElasticBERT is static in that it allows reducing model layers on demand. ElasticBERT is dynamic in that it selectively executes parts of model layers conditioned on the input. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. The ELUE benchmark is publicly available at http://eluebenchmark.fastnlp.top/1.},
  langid = {english},
  keywords = {benchmark,efficiency,nlp,nosource}
}

@article{liu_2021,
  title = {P-{{Tuning}} v2: {{Prompt Tuning Can Be Comparable}} to {{Fine-tuning Universally Across Scales}} and {{Tasks}}},
  shorttitle = {P-{{Tuning}} V2},
  author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07602 [cs]},
  eprint = {2110.07602},
  primaryclass = {cs},
  urldate = {2021-10-16},
  abstract = {Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work and our results reveal that existing methods of prompt tuning do not perform well for normal-sized pre-trained models and for hard sequence tasks, indicating lack of universality. We present a novel empirical finding that properly-optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks, where it matches the performance of fine-tuning while having only 0.1{\textbackslash}\%-3{\textbackslash}\% tuned parameters. Our method P-Tuning v2 is not a new method but a version of prefix-tuning {\textbackslash}cite\{li2021prefix\} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative for fine-tuning and a strong baseline for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,nlp,prompt tuning},
  annotation = {19 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2110.07602 [cs]/2021/liu_2021_p-tuning_v2_-_prompt_tuning_can_be_comparable_to_fine-tuning_universally_across.pdf}
}

@misc{liu_2023,
  title = {Evaluating {{Verifiability}} in {{Generative Search Engines}}},
  author = {Liu, Nelson F. and Zhang, Tianyi and Liang, Percy},
  year = {2023},
  month = apr,
  number = {arXiv:2304.09848},
  eprint = {2304.09848},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.09848},
  urldate = {2023-04-20},
  abstract = {Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5\% of generated sentences are fully supported by citations and only 74.5\% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {\{"size": 1320879, "pages": 25, "previous": "0 citations (Semantic Scholar/arXiv) [2023-04-20]{\textbackslash}narXiv:2304.09848 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/BL2WVD4C/Liu_et_al_(2023)_Evaluating_Verifiability_in_Generative_Search_Engines.pdf}
}

@misc{liu_2023a,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03172},
  doi = {10.48550/arXiv.2307.03172},
  urldate = {2023-07-17},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {\{"size": 804284, "pages": 15, "previous": "1 citations (Semantic Scholar/arXiv) [2023-07-17]{\textbackslash}n1 citations (Semantic Scholar/DOI) [2023-07-17]{\textbackslash}narXiv:2307.03172 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/ZD2SJ3FS/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long Contexts.pdf}
}

@misc{liu_2023b,
  title = {Blockwise {{Parallel Transformer}} for {{Long Context Large Models}}},
  author = {Liu, Hao and Abbeel, Pieter},
  year = {2023},
  month = may,
  number = {arXiv:2305.19370},
  eprint = {2305.19370},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.19370},
  urldate = {2023-06-05},
  abstract = {Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/388UWMPN/Liu and Abbeel - 2023 - Blockwise Parallel Transformer for Long Context La.pdf}
}

@misc{liu_2023c,
  title = {{{LLM360}}: {{Towards Fully Transparent Open-Source LLMs}}},
  shorttitle = {{{LLM360}}},
  author = {Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and Fan, Richard and Gu, Yi and Miller, Victor and Zhuang, Yonghao and He, Guowei and Li, Haonan and Koto, Fajri and Tang, Liping and Ranjan, Nikhil and Shen, Zhiqiang and Ren, Xuguang and Iriondo, Roberto and Mu, Cun and Hu, Zhiting and Schulze, Mark and Nakov, Preslav and Baldwin, Tim and Xing, Eric P.},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-01-10},
  abstract = {The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.},
  howpublished = {https://arxiv.org/abs/2312.06550v1},
  langid = {english},
  annotation = {\{"size": 8826637, "pages": 15, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/HSQGT2A3/Liu et al. - 2023 - LLM360 Towards Fully Transparent Open-Source LLMs.pdf}
}

@misc{lourie_2021,
  title = {{{UNICORN}} on {{RAINBOW}}: {{A Universal Commonsense Reasoning Model}} on a {{New Multitask Benchmark}}},
  shorttitle = {{{UNICORN}} on {{RAINBOW}}},
  author = {Lourie, Nicholas and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  year = {2021},
  month = mar,
  number = {arXiv:2103.13009},
  eprint = {2103.13009},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2103.13009},
  urldate = {2022-05-24},
  abstract = {Commonsense AI has long been seen as a near impossible goal -- until recently. Now, research interest has sharply increased with an influx of new benchmarks and models. We propose two new ways to evaluate commonsense models, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote research on commonsense models that generalize well over multiple tasks and datasets. Second, we propose a novel evaluation, the cost equivalent curve, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency. We perform extensive experiments -- over 200 experiments encompassing 4800 models -- and report multiple valuable and sometimes surprising findings, e.g., that transfer almost always leads to better or equivalent performance if following a particular recipe, that QA-based commonsense datasets transfer well with each other, while commonsense knowledge graphs do not, and that perhaps counter-intuitively, larger models benefit more from transfer than smaller ones. Last but not least, we introduce a new universal commonsense reasoning model, UNICORN, that establishes new state-of-the-art performance across 8 popular commonsense benchmarks, aNLI (87.3\%), CosmosQA (91.8\%), HellaSWAG (93.9\%), PIQA (90.1\%), SocialIQa (83.2\%), WinoGrande (86.6\%), CycIC (94.0\%) and CommonsenseQA (79.3\%).},
  archiveprefix = {arxiv},
  keywords = {benchmark,commonsense,Computer Science - Computation and Language,nlp},
  annotation = {38 citations (Semantic Scholar/arXiv) [2022-05-23]},
  file = {/Users/chenghao/Zotero/storage/FDDDW3S9/Lourie_et_al_(2021)_UNICORN_on_RAINBOW.pdf}
}

@misc{lu_2023,
  title = {Unified-{{IO}} 2: {{Scaling Autoregressive Multimodal Models}} with {{Vision}}, {{Language}}, {{Audio}}, and {{Action}}},
  shorttitle = {Unified-{{IO}} 2},
  author = {Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha},
  year = {2023},
  month = dec,
  number = {arXiv:2312.17172},
  eprint = {2312.17172},
  publisher = {arXiv},
  urldate = {2023-12-29},
  abstract = {We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 14617196, "pages": 38, "previous": "arXiv:2312.17172 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/48AILZ6P/Lu et al. - 2023 - Unified-IO 2 Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action.pdf}
}

@article{lukas_2023,
  title = {Analyzing {{Leakage}} of {{Personally Identifiable Information}} in {{Language Models}}},
  author = {Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and {Zanella-B{\'e}guelin}, Santiago},
  year = {2023},
  doi = {10.48550/ARXIV.2302.00539},
  urldate = {2023-07-05},
  abstract = {Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10\${\textbackslash}times\$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3\% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing\_pii\_leakage.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {/unread,FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {\{"size": 1224364, "pages": 18, "previous": "5 citations (Semantic Scholar/arXiv) [2023-07-06]{\textbackslash}n5 citations (Semantic Scholar/DOI) [2023-07-06]{\textbackslash}nPublisher: arXiv{\textbackslash}nVersion Number: 4"\}},
  file = {/Users/chenghao/Zotero/storage/R5NSNBR6/Lukas et al. - 2023 - Analyzing Leakage of Personally Identifiable Infor.pdf}
}

@misc{ma_2022,
  title = {Mega: {{Moving Average Equipped Gated Attention}}},
  shorttitle = {Mega},
  author = {Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  year = {2022},
  month = sep,
  number = {arXiv:2209.10655},
  eprint = {2209.10655},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.10655},
  urldate = {2022-09-24},
  abstract = {The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-09-24]},
  file = {/Users/chenghao/Zotero/storage/6CACZM4C/Ma_et_al_(2022)_Mega.pdf}
}

@misc{ma_2024,
  title = {The {{Era}} of 1-Bit {{LLMs}}: {{All Large Language Models}} Are in 1.58 {{Bits}}},
  shorttitle = {The {{Era}} of 1-Bit {{LLMs}}},
  author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-28},
  abstract = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \{-1, 0, 1\}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  howpublished = {https://arxiv.org/abs/2402.17764v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/8S6LDR9V/Ma et al. - 2024 - The Era of 1-bit LLMs All Large Language Models are in 1.58 Bits.pdf}
}

@misc{madaan_2023,
  title = {Self-{{Refine}}: {{Iterative Refinement}} with {{Self-Feedback}}},
  shorttitle = {Self-{{Refine}}},
  author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Welleck, Sean and Majumder, Bodhisattwa Prasad and Gupta, Shashank and Yazdanbakhsh, Amir and Clark, Peter},
  year = {2023},
  month = mar,
  number = {arXiv:2303.17651},
  eprint = {2303.17651},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17651},
  urldate = {2023-04-08},
  abstract = {Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving on average by absolute 20\% across tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/VIVK8Z8T/Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf}
}

@misc{magnusson_2023,
  title = {Paloma: {{A Benchmark}} for {{Evaluating Language Model Fit}}},
  shorttitle = {Paloma},
  author = {Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and Schwenk, Dustin and Walsh, Evan Pete and Elazar, Yanai and Lo, Kyle and Groeneveld, Dirk and Beltagy, Iz and Hajishirzi, Hannaneh and Smith, Noah A. and Richardson, Kyle and Dodge, Jesse},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10523},
  eprint = {2312.10523},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10523},
  urldate = {2023-12-19},
  abstract = {Language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains\${\textbackslash}unicode\{x2013\}\$varying distributions of language. Rather than assuming perplexity on one distribution extrapolates to others, Perplexity Analysis for Language Model Assessment (Paloma), measures LM fit to 585 text domains, ranging from nytimes.com to r/depression on Reddit. We invite submissions to our benchmark and organize results by comparability based on compliance with guidelines such as removal of benchmark contamination from pretraining. Submissions can also record parameter and training token count to make comparisons of Pareto efficiency for performance as a function of these measures of cost. We populate our benchmark with results from 6 baselines pretrained on popular corpora. In case studies, we demonstrate analyses that are possible with Paloma, such as finding that pretraining without data beyond Common Crawl leads to inconsistent fit to many domains.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {\{"size": 4782118, "pages": 36, "previous": "arXiv:2312.10523 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/QC7IGSAL/Magnusson_et_al_(2023)_Paloma2.pdf}
}

@misc{malladi_2023,
  title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},
  author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},
  year = {2023},
  month = may,
  number = {arXiv:2305.17333},
  eprint = {2305.17333},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17333},
  urldate = {2023-06-07},
  abstract = {Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-06-06]},
  file = {/Users/chenghao/Zotero/storage/DLMCW8S6/[2305.17333] Fine-Tuning Language Models with Just.pdf}
}

@inproceedings{manku_2007,
  title = {Detecting Near-Duplicates for Web Crawling},
  booktitle = {Proceedings of the 16th International Conference on {{World Wide Web}}  - {{WWW}} '07},
  author = {Manku, Gurmeet Singh and Jain, Arvind and Das Sarma, Anish},
  year = {2007},
  pages = {141},
  publisher = {ACM Press},
  address = {Banff, Alberta, Canada},
  doi = {10.1145/1242572.1242592},
  urldate = {2022-02-20},
  abstract = {Near-duplicate web documents are abundant. Two such documents differ from each other in a very small portion that displays advertisements, for example. Such differences are irrelevant for web search. So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not. In the course of developing a near-duplicate detection system for a multi-billion page repository, we make two research contributions. First, we demonstrate that Charikar's fingerprinting technique is appropriate for this goal. Second, we present an algorithmic technique for identifying existing f bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k. Our technique is useful for both online queries (single fingerprints) and batch queries (multiple fingerprints). Experimental evaluation over real data confirms the practicality of our design.},
  isbn = {978-1-59593-654-7},
  langid = {english},
  keywords = {data,deduplication},
  annotation = {593 citations (Semantic Scholar/DOI) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/JNKC9YBP/Manku_et_al_(2007)_Detecting_near-duplicates_for_web_crawling.pdf}
}

@article{manning_2022,
  title = {Human {{Language Understanding}} \& {{Reasoning}}},
  author = {Manning, Christopher D.},
  year = {2022},
  month = may,
  journal = {Daedalus},
  volume = {151},
  number = {2},
  pages = {127--138},
  issn = {0011-5266, 1548-6192},
  doi = {10.1162/daed_a_01905},
  urldate = {2022-05-15},
  abstract = {Abstract             The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.},
  langid = {english},
  keywords = {language understanding},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-05-14]},
  file = {/Users/chenghao/Zotero/storage/8MTVTTJN/Manning_(2022)_Human_Language_Understanding_&_Reasoning.pdf}
}

@misc{marcus_2022,
  title = {A Very Preliminary Analysis of {{DALL-E}} 2},
  author = {Marcus, Gary and Davis, Ernest and Aaronson, Scott},
  year = {2022},
  month = may,
  number = {arXiv:2204.13807},
  eprint = {2204.13807},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-04},
  abstract = {The DALL-E 2 system generates original synthetic images corresponding to an input text as caption. We report here on the outcome of fourteen tests of this system designed to assess its common sense, reasoning and ability to understand complex texts. All of our prompts were intentionally much more challenging than the typical ones that have been showcased in recent weeks. Nevertheless, for 5 out of the 14 prompts, at least one of the ten images fully satisfied our requests. On the other hand, on no prompt did all of the ten images satisfy our requests.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {3 citations (Semantic Scholar/arXiv) [2022-06-04]},
  file = {/Users/chenghao/Zotero/storage/X4VTAIE7/Marcus_et_al_(2022)_A_very_preliminary_analysis_of_DALL-E_2.pdf}
}

@article{mathew_2020,
  title = {Document {{Visual Question Answering Challenge}} 2020},
  author = {Mathew, Minesh and Tito, Rub{\`e}n P{\'e}rez and Karatzas, Dimosthenis and Manmatha, R. and Jawahar, C. V.},
  year = {2020},
  journal = {ArXiv},
  abstract = {This paper presents results of Document Visual Question Answering Challenge organized as part of "Text and Documents in the Deep Learning Era" workshop, in CVPR 2020, with results of two tasks concerned with asking questions on a single document image. This paper presents results of Document Visual Question Answering Challenge organized as part of "Text and Documents in the Deep Learning Era" workshop, in CVPR 2020. The challenge introduces a new problem - Visual Question Answering on document images. The challenge comprised two tasks. The first task concerns with asking questions on a single document image. On the other hand, the second task is set as a retrieval task where the question is posed over a collection of images. For the task 1 a new dataset is introduced comprising 50,000 questions-answer(s) pairs defined over 12,767 document images. For task 2 another dataset has been created comprising 20 questions over 14,362 document images which share the same document template.},
  annotation = {GSCC: 0000023},
  file = {/Users/chenghao/Zotero/storage/AEEJ27EA/Mathew_et_al_(2020)_Document_Visual_Question_Answering_Challenge_2020.pdf}
}

@misc{mathew_2021,
  title = {{{DocVQA}}: {{A Dataset}} for {{VQA}} on {{Document Images}}},
  shorttitle = {{{DocVQA}}},
  author = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, C. V.},
  year = {2021},
  month = jan,
  number = {arXiv:2007.00398},
  eprint = {2007.00398},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.00398},
  urldate = {2022-10-31},
  abstract = {We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36\% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {/Users/chenghao/Zotero/storage/NIMCPAHB/Mathew_et_al_(2021)_DocVQA.pdf;/Users/chenghao/Zotero/storage/7YRDRMHG/2007.html}
}

@article{mcdonald_2022,
  title = {Detect, {{Retrieve}}, {{Comprehend}}: {{A Flexible Framework}} for {{Zero-Shot Document-Level Question Answering}}},
  shorttitle = {Detect, {{Retrieve}}, {{Comprehend}}},
  author = {McDonald, T. and Tsan, Brian and Saini, Amar and Ordo{\~n}ez, Juanita and Gutierrez, Luis and Nguyen, Phan-Anh-Huy and Mason, Blake and Ng, Brenda},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2210.01959},
  abstract = {This work presents a three-stage document QA approach: text extraction from PDF; evidence retrieval from extracted texts to form well-posed contexts; and QA to extract knowledge from contexts to return high-quality an- swers -- extractive, abstractive, or Boolean. Businesses generate thousands of documents that communi- cate their strategic vision and provide details of key prod-ucts, services, entities, and processes. Knowledge workers then face the laborious task of reading these documents to identify, extract, and synthesize information relevant to their organizational goals. To automate information gather-ing, question answering (QA) offers a flexible framework where human-posed questions can be adapted to extract diverse knowledge. Finetuning QA systems requires access to labeled data (tuples of context, question and answer). How-ever, data curation for document QA is uniquely challeng- ing because the context (i.e. answer evidence passage) needs to be retrieved from potentially long, ill-formatted docu- ments. Existing QA datasets sidestep this challenge by providing short, well-defined contexts that are unrealistic in real- world applications. We present a three-stage document QA approach: (1) text extraction from PDF; (2) evidence retrieval from extracted texts to form well-posed contexts; (3) QA to extract knowledge from contexts to return high-quality an- swers -- extractive, abstractive, or Boolean. Using Q ASPER for evaluation, our detect-retrieve-comprehend (DRC) sys- tem achieves a +6.25 improvement in Answer- F 1 over existing baselines while delivering superior context selection. Our results demonstrate that DRC holds tremendous promise as a flexible framework for practical document QA.},
  annotation = {GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/ZAPHE3H8/McDonald_et_al_(2022)_Detect,_Retrieve,_Comprehend.pdf}
}

@misc{mckinzie_2024,
  title = {{{MM1}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Multimodal LLM Pre-training}}},
  shorttitle = {{{MM1}}},
  author = {McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and Belyi, Anton and Zhang, Haotian and Singh, Karanjeet and Kang, Doug and H{\`e}, Hongyu and Schwarzer, Max and Gunter, Tom and Kong, Xiang and Zhang, Aonan and Wang, Jianyu and Wang, Chong and Du, Nan and Lei, Tao and Wiseman, Sam and Lee, Mark and Wang, Zirui and Pang, Ruoming and Grasch, Peter and Toshev, Alexander and Yang, Yinfei},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-15},
  abstract = {In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.},
  howpublished = {https://arxiv.org/abs/2403.09611v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-03-15]},
  file = {/Users/chenghao/Zotero/storage/BGIMCSJI/McKinzie et al. - 2024 - MM1 Methods, Analysis & Insights from Multimodal LLM Pre-training.pdf;/Users/chenghao/Zotero/storage/E5AZHVLU/McKinzie et al. - 2024 - MM1 Methods, Analysis & Insights from Multimodal LLM Pre-training.pdf}
}

@misc{meng_2022,
  title = {Locating and {{Editing Factual Associations}} in {{GPT}}},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2022},
  month = oct,
  number = {arXiv:2202.05262},
  eprint = {2202.05262},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.05262},
  urldate = {2022-11-13},
  abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  annotation = {5 citations (Semantic Scholar/arXiv) [2022-11-12]},
  file = {/Users/chenghao/Zotero/storage/A5ZJ74VU/Meng_et_al_(2022)_Locating_and_Editing_Factual_Associations_in_GPT.pdf}
}

@misc{milliere_2023,
  title = {The {{Alignment Problem}} in {{Context}}},
  author = {Milli{\`e}re, Rapha{\"e}l},
  year = {2023},
  month = nov,
  number = {arXiv:2311.02147},
  eprint = {2311.02147},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.02147},
  urldate = {2023-11-08},
  abstract = {A core challenge in the development of increasingly capable AI systems is to make them safe and reliable by ensuring their behaviour is consistent with human values. This challenge, known as the alignment problem, does not merely apply to hypothetical future AI systems that may pose catastrophic risks; it already applies to current systems, such as large language models, whose potential for harm is rapidly increasing. In this paper, I assess whether we are on track to solve the alignment problem for large language models, and what that means for the safety of future AI systems. I argue that existing strategies for alignment are insufficient, because large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I offer an explanation of this lingering vulnerability on which it is not simply a contingent limitation of current language models, but has deep technical ties to a crucial aspect of what makes these models useful and versatile in the first place -- namely, their remarkable aptitude to learn "in context" directly from user instructions. It follows that the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities. Furthermore, this assessment raises concerns about the prospect of ensuring the safety of future and more capable AI systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {\{"size": 473766, "pages": 25, "previous": "arXiv:2311.02147 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/FXD3AZPK/Millière_(2023)_The_Alignment_Problem_in_Context.pdf}
}

@misc{milliere_2024,
  title = {A {{Philosophical Introduction}} to {{Language Models}} -- {{Part I}}: {{Continuity With Classic Debates}}},
  shorttitle = {A {{Philosophical Introduction}} to {{Language Models}} -- {{Part I}}},
  author = {Milli{\`e}re, Rapha{\"e}l and Buckner, Cameron},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-09},
  abstract = {Large language models like GPT-4 have achieved remarkable proficiency in a broad spectrum of language-based tasks, some of which are traditionally associated with hallmarks of human intelligence. This has prompted ongoing disagreements about the extent to which we can meaningfully ascribe any kind of linguistic or cognitive competence to language models. Such questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. This article -- the first part of two companion papers -- serves both as a primer on language models for philosophers, and as an opinionated survey of their significance in relation to classic debates in the philosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as compositionality, language acquisition, semantic competence, grounding, world models, and the transmission of cultural knowledge. We argue that the success of language models challenges several long-held assumptions about artificial neural networks. However, we also highlight the need for further empirical investigation to better understand their internal mechanisms. This sets the stage for the companion paper (Part II), which turns to novel empirical methods for probing the inner workings of language models, and new philosophical questions prompted by their latest developments.},
  howpublished = {https://arxiv.org/abs/2401.03910v1},
  langid = {english},
  annotation = {\{"size": 1066034, "pages": 30, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/SGUKZSEN/Millière and Buckner - 2024 - A Philosophical Introduction to Language Models -- Part I Continuity With Classic Debates.pdf}
}

@article{min_,
  title = {{{FACTSCORE}}: {{Fine-grained Atomic Evaluation}} of {{Factual Precision}} in {{Long Form Text Generation}}},
  author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/FKE3IDEU/Min et al. - FACTSCORE Fine-grained Atomic Evaluation of Factu.pdf}
}

@misc{mitra_2023,
  title = {Orca 2: {{Teaching Small Language Models How}} to {{Reason}}},
  shorttitle = {Orca 2},
  author = {Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and Palangi, Hamid and Zheng, Guoqing and Rosset, Corby and Khanpour, Hamed and Awadallah, Ahmed},
  year = {2023},
  month = nov,
  number = {arXiv:2311.11045},
  eprint = {2311.11045},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.11045},
  urldate = {2023-12-09},
  abstract = {Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {\{"size": 1127197, "pages": 53, "previous": "arXiv:2311.11045 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/EM7X8KUB/Mitra_et_al_(2023)_Orca_2.pdf}
}

@misc{muennighoff_2022,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"i}c and Reimers, Nils},
  year = {2022},
  month = oct,
  number = {arXiv:2210.07316},
  eprint = {2210.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.07316},
  urldate = {2022-10-22},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 56 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://huggingface.co/spaces/mteb/leaderboard.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-10-22] 0 citations (Semantic Scholar/DOI) [2022-10-22]},
  file = {/Users/chenghao/Zotero/storage/TSLI8J3Z/Muennighoff_et_al_(2022)_MTEB.pdf}
}

@misc{muennighoff_2023,
  title = {Scaling {{Data-Constrained Language Models}}},
  author = {Muennighoff, Niklas and Rush, Alexander M. and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  year = {2023},
  month = may,
  number = {arXiv:2305.16264},
  eprint = {2305.16264},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16264},
  urldate = {2023-05-26},
  abstract = {The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are publicly available at https://github.com/huggingface/datablations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/HMI42UTG/Muennighoff et al. - 2023 - Scaling Data-Constrained Language Models.pdf}
}

@misc{nasr_2023,
  title = {Scalable {{Extraction}} of {{Training Data}} from ({{Production}}) {{Language Models}}},
  author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and {Choquette-Choo}, Christopher A. and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  year = {2023},
  month = nov,
  number = {arXiv:2311.17035},
  eprint = {2311.17035},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.17035},
  urldate = {2023-11-29},
  abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  annotation = {\{"size": 1433005, "pages": 64, "previous": "arXiv:2311.17035 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/2AMQJPVX/Nasr et al. - 2023 - Scalable Extraction of Training Data from (Production) Language Models.pdf}
}

@inproceedings{nguyen_2021,
  title = {Skim-{{Attention}}: {{Learning}} to {{Focus}} via {{Document Layout}}},
  shorttitle = {Skim-{{Attention}}},
  booktitle = {{{EMNLP}}},
  author = {Nguyen, Laura and Scialom, Thomas and Staiano, Jacopo and Piwowarski, Benjamin},
  year = {2021},
  doi = {10.18653/v1/2021.findings-emnlp.207},
  abstract = {Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout, and can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2dimensional position of the words in a document. Our experiments show that SkimAttention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention.},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000001},
  file = {/Users/chenghao/Zotero/storage/BFFJHAXU/Nguyen_et_al_(2021)_Skim-Attention.pdf}
}

@inproceedings{nguyen_2021a,
  title = {A {{Span Extraction Approach}} for {{Information Extraction}} on {{Visually-Rich Documents}}},
  booktitle = {{{ICDAR Workshops}}},
  author = {Nguyen, Tuan-Anh Dang and Vu, Hieu M. and Son, Nguyen Hong and Nguyen, Minh-Tien},
  year = {2021},
  doi = {10.1007/978-3-030-86159-9_25},
  abstract = {A new query-based IE model that employs span extraction instead of using the common sequence labeling approach is introduced and a new training task focusing on modelling the relationships among semantic entities within a document is proposed. Information extraction (IE) from visually-rich documents (VRDs) has achieved SOTA performance recently thanks to the adaptation of Transformer-based language models, which demonstrates great potential of pre-training methods. In this paper, we present a new approach to improve the capability of language model pre-training on VRDs. Firstly, we introduce a new IE model that is query-based and employs the span extraction formulation instead of the commonly used sequence labelling approach. Secondly, to further extend the span extraction formulation, we propose a new training task which focuses on modelling the relationships between semantic entities within a document. This task enables the spans to be extracted recursively and can be used as both a pre-training objective as well as an IE downstream task. Evaluation on various datasets of popular business documents (invoices, receipts) shows that our proposed method can improve the performance of existing models significantly, while providing a mechanism to accumulate model knowledge from multiple downstream IE tasks.},
  annotation = {GSCC: 0000003},
  file = {/Users/chenghao/Zotero/storage/TK7T5JTG/Nguyen_et_al_(2021)_A_Span_Extraction_Approach_for_Information_Extraction_on_Visually-Rich_Documents.pdf}
}

@misc{nijkamp_2023,
  title = {{{CodeGen}}: {{An Open Large Language Model}} for {{Code}} with {{Multi-Turn Program Synthesis}}},
  shorttitle = {{{CodeGen}}},
  author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  year = {2023},
  month = feb,
  number = {arXiv:2203.13474},
  eprint = {2203.13474},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.13474},
  urldate = {2023-03-18},
  abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages},
  annotation = {22 citations (Semantic Scholar/arXiv) [2023-03-18] 66 citations (Semantic Scholar/DOI) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/MF6JVVJ4/Nijkamp_et_al_(2023)_CodeGen.pdf}
}

@misc{niven_2019,
  title = {Probing {{Neural Network Comprehension}} of {{Natural Language Arguments}}},
  author = {Niven, Timothy and Kao, Hung-Yu},
  year = {2019},
  month = sep,
  number = {arXiv:1907.07355},
  eprint = {1907.07355},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.07355},
  urldate = {2023-03-30},
  abstract = {We are surprised to find that BERT's peak performance of 77\% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {332 citations (Semantic Scholar/arXiv) [2023-03-29]},
  file = {/Users/chenghao/Zotero/storage/AJ8RUM55/Niven_Kao_(2019)_Probing_Neural_Network_Comprehension_of_Natural_Language_Arguments.pdf}
}

@misc{nunes_2023,
  title = {{{DotHash}}: {{Estimating Set Similarity Metrics}} for {{Link Prediction}} and {{Document Deduplication}}},
  shorttitle = {{{DotHash}}},
  author = {Nunes, Igor and Heddes, Mike and Verg{\'e}s, Pere and Abraham, Danny and Veidenbaum, Alexander and Nicolau, Alexandru and Givargis, Tony},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  doi = {10.1145/3580305.3599314},
  urldate = {2024-02-24},
  abstract = {Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.},
  howpublished = {https://arxiv.org/abs/2305.17310v1},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/arXiv) [2024-02-24]},
  file = {/Users/chenghao/Zotero/storage/EY4HT2VW/Nunes et al. - 2023 - DotHash Estimating Set Similarity Metrics for Link Prediction and Document Deduplication.pdf}
}

@misc{nussbaum_2024,
  title = {Nomic {{Embed}}: {{Training}} a {{Reproducible Long Context Text Embedder}}},
  shorttitle = {Nomic {{Embed}}},
  author = {Nussbaum, Zach and Morris, John X. and Duderstadt, Brandon and Mulyar, Andriy},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-05},
  abstract = {This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors},
  howpublished = {https://arxiv.org/abs/2402.01613v1},
  langid = {english},
  annotation = {\{"size": 356769, "pages": 12, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/7PE46NUF/Nussbaum et al. - 2024 - Nomic Embed Training a Reproducible Long Context Text Embedder.pdf}
}

@misc{nylund_2023,
  title = {Time Is {{Encoded}} in the {{Weights}} of {{Finetuned Language Models}}},
  author = {Nylund, Kai and Gururangan, Suchin and Smith, Noah A.},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13401},
  eprint = {2312.13401},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.13401},
  urldate = {2023-12-22},
  abstract = {We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/4NCTAMGU/Nylund et al. - 2023 - Time is Encoded in the Weights of Finetuned Language Models.pdf}
}

@article{ogorman_1993,
  title = {The Document Spectrum for Page Layout Analysis},
  author = {O'Gorman, L.},
  year = {Nov./1993},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {15},
  number = {11},
  pages = {1162--1173},
  issn = {01628828},
  doi = {10.1109/34.244677},
  urldate = {2023-05-13},
  abstract = {Page layout analysis is a document processing technique used to determine the format of a page. This paper describes the document spectrum, or docstrum, which is a method for structural page layout analysis based on bottom-up, nearestneighbor clustering of page components. The method yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks. It is advantageous over many other methods in three main ways: independencefrom skew angle, independence from different text spacings, and the ability to process local regions of different text orientations within the same image. Results of the method shown for several different page formats and for randomly oriented subpages on the same image illustrate the versatility of the method. We also discuss the differences, advantages, and disadvantages of the docstrum with respect to other lay-out methods.},
  langid = {english},
  annotation = {748 citations (Semantic Scholar/DOI) [2023-05-13]},
  file = {/Users/chenghao/Zotero/storage/CX8ERB6L/O'Gorman - 1993 - The document spectrum for page layout analysis.pdf}
}

@misc{opitz_2022,
  title = {{{SBERT}} Studies {{Meaning Representations}}: {{Decomposing Sentence Embeddings}} into {{Explainable Semantic Features}}},
  shorttitle = {{{SBERT}} Studies {{Meaning Representations}}},
  author = {Opitz, Juri and Frank, Anette},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07023},
  eprint = {2206.07023},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.07023},
  urldate = {2022-12-19},
  abstract = {Models based on large-pretrained language models, such as S(entence)BERT, provide effective and efficient sentence embeddings that show high correlation to human similarity ratings, but lack interpretability. On the other hand, graph metrics for graph-based meaning representations (e.g., Abstract Meaning Representation, AMR) can make explicit the semantic aspects in which two sentences are similar. However, such metrics tend to be slow, rely on parsers, and do not reach state-of-the-art performance when rating sentence similarity. In this work, we aim at the best of both worlds, by learning to induce \$S\$emantically \$S\$tructured \$S\$entence BERT embeddings (S\$\^{}3\$BERT). Our S\$\^{}3\$BERT embeddings are composed of explainable sub-embeddings that emphasize various semantic sentence features (e.g., semantic roles, negation, or quantification). We show how to i) learn a decomposition of the sentence embeddings into semantic features, through approximation of a suite of interpretable AMR graph metrics, and how to ii) preserve the overall power of the neural embeddings by controlling the decomposition learning process with a second objective that enforces consistency with the similarity ratings of an SBERT teacher model. In our experimental studies, we show that our approach offers interpretability -- while fully preserving the effectiveness and efficiency of the neural sentence embeddings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/3ZZ5EDHY/Opitz_Frank_(2022)_SBERT_studies_Meaning_Representations.pdf}
}

@misc{overwijk_2022,
  title = {{{ClueWeb22}}: 10 {{Billion Web Documents}} with {{Rich Information}}},
  shorttitle = {{{ClueWeb22}}},
  author = {Overwijk, Arnold and Xiong, Chenyan and Liu, Xiao and VandenBerg, Cameron and Callan, Jamie},
  year = {2022},
  month = nov,
  number = {arXiv:2211.15848},
  eprint = {2211.15848},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.15848},
  urldate = {2022-11-30},
  abstract = {ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10 billion web pages affiliated with rich information. Its design was influenced by the need for a high quality, large scale web corpus to support a range of academic and industry research, for example, in information systems, retrieval-augmented AI systems, and model pretraining. Compared with earlier ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of higher-quality, and aligned with the document distributions in commercial web search. Besides raw HTML, ClueWeb22 includes rich information about the web pages provided by industry-standard document understanding systems, including the visual representation of pages rendered by a web browser, parsed HTML structure information from a neural network parser, and pre-processed cleaned document text to lower the barrier to entry. Many of these signals have been widely used in industry but are available to the research community for the first time at this scale.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-11-30]},
  file = {/Users/chenghao/Zotero/storage/BARSZZUK/Overwijk_et_al_(2022)_ClueWeb22.pdf}
}

@inproceedings{papadopoulou_2022,
  title = {Neural {{Text Sanitization}} with {{Explicit Measures}} of {{Privacy Risk}}},
  booktitle = {Proceedings of the 2nd {{Conference}} of the {{Asia-Pacific Chapter}} of the {{Association}} for {{Computational Linguistics}} and the 12th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Papadopoulou, Anthi and Yu, Yunhao and Lison, Pierre and {\O}vrelid, Lilja},
  year = {2022},
  month = nov,
  pages = {217--229},
  publisher = {Association for Computational Linguistics},
  address = {Online only},
  urldate = {2023-07-05},
  abstract = {We present a novel approach for text sanitization, which is the task of editing a document to mask all (direct and indirect) personal identifiers and thereby conceal the identity of the individuals(s) mentioned in the text. In contrast to previous work, the approach relies on explicit measures of privacy risk, making it possible to explicitly control the trade-off between privacy protection and data utility. The approach proceeds in three steps. A neural, privacy-enhanced entity recognizer is first employed to detect and classify potential personal identifiers. We then determine which entities, or combination of entities, are likely to pose a re-identification risk through a range of privacy risk assessment measures. We present three such measures of privacy risk, respectively based on (1) span probabilities derived from a BERT language model, (2) web search queries and (3) a classifier trained on labelled data. Finally, a linear optimization solver decides which entities to mask to minimize the semantic loss while simultaneously ensuring that the estimated privacy risk remains under a given threshold. We evaluate the approach both in the absence and presence of manually annotated data. Our results highlight the potential of the approach, as well as issues specific types of personal data can introduce to the process.},
  keywords = {/unread},
  file = {/Users/chenghao/Zotero/storage/6MRP82FU/Papadopoulou et al. - 2022 - Neural Text Sanitization with Explicit Measures of.pdf}
}

@article{penedo_,
  title = {The {{RefinedWeb Dataset}} for {{Falcon LLM}}: {{Outperforming Curated Corpora}} with {{Web Data}}, and {{Web Data Only}}},
  author = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  abstract = {Large language models are commonly trained on a mixture of filtered web data and curated ``high-quality'' corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our REFINEDWEB dataset, and 1.3/7.5B parameters language models trained on it*.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/6UXEE77Z/Penedo et al. - The RefinedWeb Dataset for Falcon LLM Outperformi.pdf}
}

@misc{peng_2022,
  title = {{{ERNIE-Layout}}: {{Layout Knowledge Enhanced Pre-training}} for {{Visually-rich Document Understanding}}},
  shorttitle = {{{ERNIE-Layout}}},
  author = {Peng, Qiming and Pan, Yinxu and Wang, Wenjin and Luo, Bin and Zhang, Zhenyu and Huang, Zhengjie and Hu, Teng and Yin, Weichong and Chen, Yongfeng and Zhang, Yin and Feng, Shikun and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06155},
  eprint = {2210.06155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.06155},
  urldate = {2022-10-13},
  abstract = {Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge enhancement in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model\_zoo/ernie-layout.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/4ECVFZ28/Peng_et_al_(2022)_ERNIE-Layout.pdf}
}

@misc{perry_2022,
  title = {Do {{Users Write More Insecure Code}} with {{AI Assistants}}?},
  author = {Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
  year = {2022},
  month = dec,
  number = {arXiv:2211.03622},
  eprint = {2211.03622},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.03622},
  urldate = {2022-12-26},
  abstract = {We conduct the first large-scale user study examining how users interact with an AI Code assistant to solve a variety of security related tasks across different programming languages. Overall, we find that participants who had access to an AI assistant based on OpenAI's codex-davinci-002 model wrote significantly less secure code than those without access. Additionally, participants with access to an AI assistant were more likely to believe they wrote secure code than those without access to the AI assistant. Furthermore, we find that participants who trusted the AI less and engaged more with the language and format of their prompts (e.g. re-phrasing, adjusting temperature) provided code with fewer security vulnerabilities. Finally, in order to better inform the design of future AI-based Code assistants, we provide an in-depth analysis of participants' language and interaction behavior, as well as release our user interface as an instrument to conduct similar studies in the future.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-12-26] 1 citations (Semantic Scholar/DOI) [2022-12-26]},
  file = {/Users/chenghao/Zotero/storage/VBN3HP3Z/Perry_et_al_(2022)_Do_Users_Write_More_Insecure_Code_with_AI_Assistants.pdf}
}

@misc{piantadosi_2022,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantadosi, Steven T. and Hill, Felix},
  year = {2022},
  month = aug,
  number = {arXiv:2208.02957},
  eprint = {2208.02957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.02957},
  urldate = {2022-08-15},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-08-14] 0 citations (Semantic Scholar/DOI) [2022-08-14]},
  file = {/Users/chenghao/Zotero/storage/2ZHRWW93/Piantadosi_Hill_(2022)_Meaning_without_reference_in_large_language_models.pdf}
}

@article{pilan_2022,
  title = {The {{Text Anonymization Benchmark}} ({{TAB}}): {{A Dedicated Corpus}} and {{Evaluation Framework}} for {{Text Anonymization}}},
  shorttitle = {The {{Text Anonymization Benchmark}} ({{TAB}})},
  author = {Pil{\'a}n, Ildik{\'o} and Lison, Pierre and {\O}vrelid, Lilja and Papadopoulou, Anthi and S{\'a}nchez, David and Batet, Montserrat},
  year = {2022},
  month = dec,
  journal = {Computational Linguistics},
  volume = {48},
  number = {4},
  pages = {1053--1101},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00458},
  urldate = {2023-07-05},
  abstract = {Abstract             We present a novel benchmark and associated evaluation metrics for assessing the performance of text anonymization methods. Text anonymization, defined as the task of editing a text document to prevent the disclosure of personal information, currently suffers from a shortage of privacy-oriented annotated text resources, making it difficult to properly evaluate the level of privacy protection offered by various anonymization methods. This paper presents TAB (Text Anonymization Benchmark), a new, open-source annotated corpus developed to address this shortage. The corpus comprises 1,268 English-language court cases from the European Court of Human Rights (ECHR) enriched with comprehensive annotations about the personal information appearing in each document, including their semantic category, identifier type, confidential attributes, and co-reference relations. Compared with previous work, the TAB corpus is designed to go beyond traditional de-identification (which is limited to the detection of predefined semantic categories), and explicitly marks which text spans ought to be masked in order to conceal the identity of the person to be protected.             Along with presenting the corpus and its annotation layers, we also propose a set of evaluation metrics that are specifically tailored toward measuring the performance of text anonymization, both in terms of privacy protection and utility preservation. We illustrate the use of the benchmark and the proposed metrics by assessing the empirical performance of several baseline text anonymization models. The full corpus along with its privacy-oriented annotation guidelines, evaluation scripts, and baseline models are available on: https://github.com/NorskRegnesentral/text-anonymization-benchmark.},
  langid = {english},
  keywords = {/unread},
  annotation = {\{"size": 1249147, "pages": 49, "previous": "17 citations (Semantic Scholar/DOI) [2023-07-06]"\}},
  file = {/Users/chenghao/Zotero/storage/5GUC9694/Pilán_et_al_(2022)_The_Text_Anonymization_Benchmark_(TAB).pdf}
}

@article{pinter_2021,
  title = {Learning to {{Look Inside}}: {{Augmenting Token-Based Encoders}} with {{Character-Level Information}}},
  shorttitle = {Learning to {{Look Inside}}},
  author = {Pinter, Yuval and Stent, Amanda and Dredze, Mark and Eisenstein, Jacob},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.00391 [cs]},
  eprint = {2108.00391},
  primaryclass = {cs},
  urldate = {2021-08-03},
  abstract = {Commonly-used transformer language models depend on a tokenization schema which sets an unchangeable subword vocabulary prior to pre-training, destined to be applied to all downstream tasks regardless of domain shift, novel word formations, or other sources of vocabulary mismatch. Recent work has shown that "token-free" models can be trained directly on characters or bytes, but training these models from scratch requires substantial computational resources, and this implies discarding the many domain-specific models that were trained on tokens. In this paper, we present XRayEmb, a method for retrofitting existing token-based models with character-level information. XRayEmb is composed of a character-level "encoder" that computes vector representations of character sequences, and a generative component that decodes from the internal representation to a character sequence. We show that incorporating XRayEmb's learned vectors into sequences of pre-trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on non-standard English text.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,nosource,token},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/JLUTX6YN/Pinter_et_al_(2021)_Learning_to_Look_Inside.pdf}
}

@misc{pioro_2024,
  title = {{{MoE-Mamba}}: {{Efficient Selective State Space Models}} with {{Mixture}} of {{Experts}}},
  shorttitle = {{{MoE-Mamba}}},
  author = {Pi{\'o}ro, Maciej and Ciebiera, Kamil and Kr{\'o}l, Krystian and Ludziejewski, Jan and Jaszczur, Sebastian},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-09},
  abstract = {State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based LLMs, including recent state-of-the-art open-source models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable, Transformer-like performance. Our model, MoE-Mamba, outperforms both Mamba and Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in 2.2x less training steps while preserving the inference performance gains of Mamba against the Transformer.},
  howpublished = {https://arxiv.org/abs/2401.04081v1},
  langid = {english},
  annotation = {\{"size": 721554, "pages": 9, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/P57GTS4B/Pióro et al. - 2024 - MoE-Mamba Efficient Selective State Space Models with Mixture of Experts.pdf}
}

@misc{powalski_2021,
  title = {Going {{Full-TILT Boogie}} on {{Document Understanding}} with {{Text-Image-Layout Transformer}}},
  author = {Powalski, Rafa{\l} and Borchmann, {\L}ukasz and Jurkiewicz, Dawid and Dwojak, Tomasz and Pietruszka, Micha{\l} and Pa{\l}ka, Gabriela},
  year = {2021},
  month = jul,
  number = {arXiv:2102.09550},
  eprint = {2102.09550},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09550},
  urldate = {2022-10-28},
  abstract = {We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of unifying a variety of problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. Our novel approach achieves state-of-the-art results in extracting information from documents and answering questions which demand layout understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process by employing an end-to-end model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {38 citations (Semantic Scholar/arXiv) [2022-10-28]},
  file = {/Users/chenghao/Zotero/storage/MALB6JGZ/Powalski_et_al_(2021)_Going_Full-TILT_Boogie_on_Document_Understanding_with_Text-Image-Layout.pdf;/Users/chenghao/Zotero/storage/LDAR9JXA/2102.html}
}

@misc{pramanik_2022,
  title = {Towards a {{Multi-modal}}, {{Multi-task Learning}} Based {{Pre-training Framework}} for {{Document Representation Learning}}},
  author = {Pramanik, Subhojeet and Mujumdar, Shashank and Patel, Hima},
  year = {2022},
  month = jan,
  number = {arXiv:2009.14457},
  eprint = {2009.14457},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.14457},
  urldate = {2022-11-02},
  abstract = {Recent approaches in literature have exploited the multi-modal information in documents (text, layout, image) to serve specific downstream document tasks. However, they are limited by their - (i) inability to learn cross-modal representations across text, layout and image dimensions for documents and (ii) inability to process multi-page documents. Pre-training techniques have been shown in Natural Language Processing (NLP) domain to learn generic textual representations from large unlabelled datasets, applicable to various downstream NLP tasks. In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation applicable to various downstream document tasks. Specifically, we introduce Document Topic Modelling and Document Shuffle Prediction as novel pre-training tasks to learn rich image representations along with the text and layout representations for documents. We utilize the Longformer network architecture as the backbone to encode the multi-modal information from multi-page documents in an end-to-end fashion. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We evaluate our framework on different standard document datasets and conduct exhaustive experiments to compare performance against various ablations of our framework and state-of-the-art baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/HNSNJFM7/Pramanik_et_al_(2022)_Towards_a_Multi-modal,_Multi-task_Learning_based_Pre-training_Framework_for.pdf;/Users/chenghao/Zotero/storage/MPB9TDLX/2009.html}
}

@misc{qin_2023,
  title = {Scaling {{TransNormer}} to 175 {{Billion Parameters}}},
  author = {Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and Qiao, Yu and Zhong, Yiran},
  year = {2023},
  month = jul,
  number = {arXiv:2307.14995},
  eprint = {2307.14995},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.14995},
  urldate = {2024-01-13},
  abstract = {We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20\%. Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. Scalability is at the heart of our model's design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics. Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure data quality and relevance, we implement a new self-cleaning strategy to filter our collected data. Our pre-trained models will be released to foster community advancements in efficient LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Qin et_al/2023/Qin_et_al_(2023)_Scaling_TransNormer_to_175_Billion_Parameters.pdf}
}

@misc{qin_2024,
  title = {Lightning {{Attention-2}}: {{A Free Lunch}} for {{Handling Unlimited Sequence Lengths}} in {{Large Language Models}}},
  shorttitle = {Lightning {{Attention-2}}},
  author = {Qin, Zhen and Sun, Weigao and Li, Dong and Shen, Xuyang and Sun, Weixuan and Zhong, Yiran},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-10},
  abstract = {Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.},
  howpublished = {https://arxiv.org/abs/2401.04658v1},
  langid = {english},
  annotation = {\{"size": 709518, "pages": 11, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/T9ZTJGCV/Qin et al. - 2024 - Lightning Attention-2 A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models.pdf}
}

@article{rae_2021,
  title = {Scaling {{Language Models}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Training Gopher}}},
  shorttitle = {Scaling {{Language Models}}},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and {d'Autume}, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.11446 [cs]},
  eprint = {2112.11446},
  primaryclass = {cs},
  urldate = {2021-12-24},
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,data,lm,nlp,scaling},
  annotation = {38 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2112.11446 [cs]/2021/Rae et al_2021_Scaling Language Models.pdf}
}

@misc{rafailov_2023,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  howpublished = {https://arxiv.org/abs/2305.18290v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/J697QUWN/Rafailov et al. - 2023 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf}
}

@inproceedings{raji_2021,
  title = {You {{Can}}'t {{Sit With Us}}: {{Exclusionary Pedagogy}} in {{AI Ethics Education}}},
  shorttitle = {You {{Can}}'t {{Sit With Us}}},
  author = {Raji, Inioluwa Deborah and Scheuerman, Morgan Klaus and Amironesei, Razvan},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {515--525},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445914},
  urldate = {2022-12-27},
  abstract = {Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)---and its implications for AI---has led to the current "ethics crisis". However, we claim that the current AI ethics education space relies on a form of "exclusionary pedagogy," where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as "ethical unicorns" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.},
  isbn = {978-1-4503-8309-7},
  annotation = {27 citations (Semantic Scholar/DOI) [2022-12-27]},
  file = {/Users/chenghao/Zotero/storage/4N7ZXGNU/Raji_et_al_(2021)_You_Can't_Sit_With_Us.pdf}
}

@misc{raschka_2020,
  title = {Model {{Evaluation}}, {{Model Selection}}, and {{Algorithm Selection}} in {{Machine Learning}}},
  author = {Raschka, Sebastian},
  year = {2020},
  month = nov,
  number = {arXiv:1811.12808},
  eprint = {1811.12808},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.12808},
  urldate = {2022-11-22},
  abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,nosource,Statistics - Machine Learning},
  annotation = {\{"size": 1955226, "pages": 49, "previous": "390 citations (Semantic Scholar/arXiv) [2022-11-21]{\textbackslash}narXiv:1811.12808 [cs, stat]{\textbackslash}nversion: 2"\}},
  file = {/Users/chenghao/Zotero/storage/5647EYZI/Raschka - 2020 - Model Evaluation, Model Selection, and Algorithm S.pdf}
}

@article{rasenberg_2023,
  title = {Reimagining Language: {{Towards}} a Better Understanding of Language by Including Our Interactions with Non-Humans},
  shorttitle = {Reimagining Language},
  author = {Rasenberg, Marlou and Amha, Azeb and Coler, Matt and van Koppen, Marjo and van Miltenburg, Emiel and de Rijk, Lynn and Stommel, Wyke and Dingemanse, Mark},
  year = {2023},
  month = nov,
  journal = {Linguistics in the Netherlands},
  volume = {40},
  number = {1},
  pages = {309--317},
  issn = {0929-7332, 1569-9919},
  doi = {10.1075/avt.00095.ras},
  urldate = {2023-12-25},
  abstract = {Abstract What is language and who or what can be said to have it? In this essay we consider this question in the context of interactions with non-humans, specifically: animals and computers. While perhaps an odd pairing at first glance, here we argue that these domains can offer contrasting perspectives through which we can explore and reimagine language. The interactions between humans and animals, as well as between humans and computers, reveal both the essence and the boundaries of language: from examining the role of sequence and contingency in human-animal interaction, to unravelling the challenges of natural interactions with ``smart'' speakers and language models. By bringing together disparate fields around foundational questions, we push the boundaries of linguistic inquiry and uncover new insights into what language is and how it functions in diverse non-human-exclusive contexts.},
  langid = {english},
  annotation = {\{"size": 280500, "pages": 9, "previous": "Publisher: John Benjamins"\}},
  file = {/Users/chenghao/Zotero/storage/BKEJM2EH/Rasenberg et al. - 2023 - Reimagining language Towards a better understanding of language by including our interactions with .pdf}
}

@misc{reed_2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and {Barth-Maron}, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and {de Freitas}, Nando},
  year = {2022},
  month = may,
  number = {arXiv:2205.06175},
  eprint = {2205.06175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.06175},
  urldate = {2022-08-06},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {12 citations (Semantic Scholar/arXiv) [2022-08-06] 12 citations (Semantic Scholar/DOI) [2022-08-06]},
  file = {/Users/chenghao/Zotero/storage/5JBJ8J72/Reed_et_al_(2022)_A_Generalist_Agent.pdf}
}

@misc{reid_2024,
  title = {Gemini 1.5: {{Unlocking}} Multimodal Understanding across Millions of Tokens of Context},
  shorttitle = {Gemini 1.5},
  author = {Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and Antonoglou, Ioannis and Anil, Rohan and Borgeaud, Sebastian and Dai, Andrew and Millican, Katie and Dyer, Ethan and Glaese, Mia and Sottiaux, Thibault and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Molloy, James and Chen, Jilin and Isard, Michael and Barham, Paul and Hennigan, Tom and McIlroy, Ross and Johnson, Melvin and Schalkwyk, Johan and Collins, Eli and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Meyer, Clemens and Thornton, Gregory and Yang, Zhen and Michalewski, Henryk and Abbas, Zaheer and Schucher, Nathan and Anand, Ankesh and Ives, Richard and Keeling, James and Lenc, Karel and Haykal, Salem and Shakeri, Siamak and Shyam, Pranav and Chowdhery, Aakanksha and Ring, Roman and Spencer, Stephen and Sezener, Eren and Vilnis, Luke and Chang, Oscar and Morioka, Nobuyuki and Tucker, George and Zheng, Ce and Woodman, Oliver and Attaluri, Nithya and Kocisky, Tomas and Eltyshev, Evgenii and Chen, Xi and Chung, Timothy and Selo, Vittorio and Brahma, Siddhartha and Georgiev, Petko and Slone, Ambrose and Zhu, Zhenkai and Lottes, James and Qiao, Siyuan and Caine, Ben and Riedel, Sebastian and Tomala, Alex and Chadwick, Martin and Love, Juliette and Choy, Peter and Mittal, Sid and Houlsby, Neil and Tang, Yunhao and Lamm, Matthew and Bai, Libin and Zhang, Qiao and He, Luheng and Cheng, Yong and Humphreys, Peter and Li, Yujia and Brin, Sergey and Cassirer, Albin and Miao, Yingjie and Zilka, Lukas and Tobin, Taylor and Xu, Kelvin and Proleev, Lev and Sohn, Daniel and Magni, Alberto and Hendricks, Lisa Anne and Gao, Isabel and Onta{\~n}{\'o}n, Santiago and Bunyan, Oskar and Byrd, Nathan and Sharma, Abhanshu and Zhang, Biao and Pinto, Mario and Sinha, Rishika and Mehta, Harsh and Jia, Dawei and Caelles, Sergi and Webson, Albert and Morris, Alex and Roelofs, Becca and Ding, Yifan and Strudel, Robin and Xiong, Xuehan and Ritter, Marvin and Dehghani, Mostafa and Chaabouni, Rahma and Karmarkar, Abhijit and Lai, Guangda and Mentzer, Fabian and Xu, Bibo and Li, YaGuang and Zhang, Yujing and Paine, Tom Le and Goldin, Alex and Neyshabur, Behnam and Baumli, Kate and Levskaya, Anselm and Laskin, Michael and Jia, Wenhao and Rae, Jack W. and Xiao, Kefan and He, Antoine and Giordano, Skye and Yagati, Lakshman and Lespiau, Jean-Baptiste and Natsev, Paul and Ganapathy, Sanjay and Liu, Fangyu and Martins, Danilo and Chen, Nanxin and Xu, Yunhan and Barnes, Megan and May, Rhys and Vezer, Arpi and Oh, Junhyuk and Franko, Ken and Bridgers, Sophie and Zhao, Ruizhe and Wu, Boxi and Mustafa, Basil and Sechrist, Sean and Parisotto, Emilio and Pillai, Thanumalayan Sankaranarayana and Larkin, Chris and Gu, Chenjie and Sorokin, Christina and Krikun, Maxim and Guseynov, Alexey and Landon, Jessica and Datta, Romina and Pritzel, Alexander and Thacker, Phoebe and Yang, Fan and Hui, Kevin and Hauth, Anja and Yeh, Chih-Kuan and Barker, David and {Mao-Jones}, Justin and Austin, Sophia and Sheahan, Hannah and Schuh, Parker and Svensson, James and Jain, Rohan and Ramasesh, Vinay and Briukhov, Anton and Chung, Da-Woon and {von Glehn}, Tamara and Butterfield, Christina and Jhakra, Priya and Wiethoff, Matthew and Frye, Justin and Grimstad, Jordan and Changpinyo, Beer and Lan, Charline Le and Bortsova, Anna and Wu, Yonghui and Voigtlaender, Paul and Sainath, Tara and Smith, Charlotte and Hawkins, Will and Cao, Kris and Besley, James and Srinivasan, Srivatsan and Omernick, Mark and Gaffney, Colin and Surita, Gabriela and Burnell, Ryan and Damoc, Bogdan and Ahn, Junwhan and Brock, Andrew and Pajarskas, Mantas and Petrushkina, Anastasia and Noury, Seb and Blanco, Lorenzo and Swersky, Kevin and Ahuja, Arun and Avrahami, Thi and Misra, Vedant and {de Liedekerke}, Raoul and Iinuma, Mariko and Polozov, Alex and York, Sarah and van den Driessche, George and Michel, Paul and Chiu, Justin and Blevins, Rory and Gleicher, Zach and Recasens, Adri{\`a} and Rrustemi, Alban and Gribovskaya, Elena and Roy, Aurko and Gworek, Wiktor and Arnold, S{\'e}b and Lee, Lisa and {Lee-Thorp}, James and Maggioni, Marcello and Piqueras, Enrique and Badola, Kartikeya and Vikram, Sharad and Gonzalez, Lucas and Baddepudi, Anirudh and Senter, Evan and Devlin, Jacob and Qin, James and Azzam, Michael and Trebacz, Maja and Polacek, Martin and Krishnakumar, Kashyap and Chang, Shuo-yiin and Tung, Matthew and Penchev, Ivo and Joshi, Rishabh and Olszewska, Kate and Muir, Carrie and Wirth, Mateo and Hartman, Ale Jakse and Newlan, Josh and Kashem, Sheleem and Bolina, Vijay and Dabir, Elahe and {van Amersfoort}, Joost and Ahmed, Zafarali and {Cobon-Kerr}, James and Kamath, Aishwarya and Hrafnkelsson, Arnar Mar and Hou, Le and Mackinnon, Ian and Frechette, Alexandre and Noland, Eric and Si, Xiance and Taropa, Emanuel and Li, Dong and Crone, Phil and Gulati, Anmol and Cevey, S{\'e}bastien and Adler, Jonas and Ma, Ada and Silver, David and Tokumine, Simon and Powell, Richard and Lee, Stephan and Chang, Michael and Hassan, Samer and Mincu, Diana and Yang, Antoine and Levine, Nir and Brennan, Jenny and Wang, Mingqiu and Hodkinson, Sarah and Zhao, Jeffrey and Lipschultz, Josh and Pope, Aedan and Chang, Michael B. and Li, Cheng and Shafey, Laurent El and Paganini, Michela and Douglas, Sholto and Bohnet, Bernd and Pardo, Fabio and Odoom, Seth and Rosca, Mihaela and dos Santos, Cicero Nogueira and Soparkar, Kedar and Guez, Arthur and Hudson, Tom and Hansen, Steven and Asawaroengchai, Chulayuth and Addanki, Ravi and Yu, Tianhe and Stokowiec, Wojciech and Khan, Mina and Gilmer, Justin and Lee, Jaehoon and Bostock, Carrie Grimes and Rong, Keran and Caton, Jonathan and Pejman, Pedram and Pavetic, Filip and Brown, Geoff and Sharma, Vivek and Lu{\v c}i{\'c}, Mario and Samuel, Rajkumar and Djolonga, Josip and Mandhane, Amol and Sj{\"o}sund, Lars Lowe and Buchatskaya, Elena and White, Elspeth and Clay, Natalie and Jiang, Jiepu and Lim, Hyeontaek and Hemsley, Ross and Labanowski, Jane and De Cao, Nicola and Steiner, David and Hashemi, Sayed Hadi and Austin, Jacob and Gergely, Anita and Blyth, Tim and Stanton, Joe and Shivakumar, Kaushik and Siddhant, Aditya and Andreassen, Anders and Araya, Carlos and Sethi, Nikhil and Shivanna, Rakesh and Hand, Steven and Bapna, Ankur and Khodaei, Ali and Miech, Antoine and Tanzer, Garrett and Swing, Andy and Thakoor, Shantanu and Pan, Zhufeng and Nado, Zachary and Winkler, Stephanie and Yu, Dian and Saleh, Mohammad and Maggiore, Loren and Barr, Iain and Giang, Minh and Kagohara, Thais and Danihelka, Ivo and Marathe, Amit and Feinberg, Vladimir and Elhawaty, Mohamed and Ghelani, Nimesh and Horgan, Dan and Miller, Helen and Walker, Lexi and Tanburn, Richard and Tariq, Mukarram and Shrivastava, Disha and Xia, Fei and Chiu, Chung-Cheng and Ashwood, Zoe and Baatarsukh, Khuslen and Samangooei, Sina and Alcober, Fred and Stjerngren, Axel and Komarek, Paul and Tsihlas, Katerina and Boral, Anudhyan and Comanescu, Ramona and Chen, Jeremy and Liu, Ruibo and Bloxwich, Dawn and Chen, Charlie and Sun, Yanhua and Feng, Fangxiaoyu and Mauger, Matthew and Dotiwalla, Xerxes and Hellendoorn, Vincent and Sharman, Michael and Zheng, Ivy and Haridasan, Krishna and {Barth-Maron}, Gabe and Swanson, Craig and Rogozi{\'n}ska, Dominika and Andreev, Alek and Rubenstein, Paul Kishan and Sang, Ruoxin and Hurt, Dan and Elsayed, Gamaleldin and Wang, Renshen and Lacey, Dave and Ili{\'c}, Anastasija and Zhao, Yao and Aroyo, Lora and Iwuanyanwu, Chimezie and Nikolaev, Vitaly and Lakshminarayanan, Balaji and Jazayeri, Sadegh and Kaufman, Rapha{\"e}l Lopez and Varadarajan, Mani and Tekur, Chetan and Fritz, Doug and Khalman, Misha and Reitter, David and Dasgupta, Kingshuk and Sarcar, Shourya and Ornduff, Tina and Snaider, Javier and Huot, Fantine and Jia, Johnson and Kemp, Rupert and Trdin, Nejc and Vijayakumar, Anitha and Kim, Lucy and Angermueller, Christof and Lao, Li and Liu, Tianqi and Zhang, Haibin and Engel, David and Greene, Somer and White, Ana{\"i}s and Austin, Jessica and Taylor, Lilly and Ashraf, Shereen and Liu, Dangyi and Georgaki, Maria and Cai, Irene and Kulizhskaya, Yana and Goenka, Sonam and Saeta, Brennan and Vodrahalli, Kiran and Frank, Christian and {de Cesare}, Dario and Robenek, Brona and Richardson, Harry and Alnahlawi, Mahmoud and Yew, Christopher and Ponnapalli, Priya and Tagliasacchi, Marco and Korchemniy, Alex and Kim, Yelin and Li, Dinghua and Rosgen, Bill and Ashwood, Zoe and Levin, Kyle and Wiesner, Jeremy and Banzal, Praseem and Srinivasan, Praveen and Yu, Hongkun and {\"U}nl{\"u}, {\c C}a{\u g}lar and Reid, David and Tung, Zora and Finchelstein, Daniel and Kumar, Ravin and Elisseeff, Andre and Huang, Jin and Zhang, Ming and Zhu, Rui and Aguilar, Ricardo and Gim{\'e}nez, Mai and Xia, Jiawei and Dousse, Olivier and Gierke, Willi and Yeganeh, Soheil Hassas and Yates, Damion and Jalan, Komal and Li, Lu and {Latorre-Chimoto}, Eri and Nguyen, Duc Dung and Durden, Ken and Kallakuri, Praveen and Liu, Yaxin and Johnson, Matthew and Tsai, Tomy and Talbert, Alice and Liu, Jasmine and Neitz, Alexander and Elkind, Chen and Selvi, Marco and Jasarevic, Mimi and Soares, Livio Baldini and Cui, Albert and Wang, Pidong and Wang, Alek Wenjiao and Ye, Xinyu and Kallarackal, Krystal and Loher, Lucia and Lam, Hoi and Broder, Josef and {Holtmann-Rice}, Dan and Martin, Nina and Ramadhana, Bramandia and Toyama, Daniel and Shukla, Mrinal and Basu, Sujoy and Mohan, Abhi and Fernando, Nick and Fiedel, Noah and Paterson, Kim and Li, Hui and Garg, Ankush and Park, Jane and Choi, DongHyun and Wu, Diane and Singh, Sankalp and Zhang, Zhishuai and Globerson, Amir and Yu, Lily and Carpenter, John and Quitry, F{\'e}lix de Chaumont and Radebaugh, Carey and Lin, Chu-Cheng and Tudor, Alex and Shroff, Prakash and Garmon, Drew and Du, Dayou and Vats, Neera and Lu, Han and Iqbal, Shariq and Yakubovich, Alex and Tripuraneni, Nilesh and Manyika, James and Qureshi, Haroon and Hua, Nan and Ngani, Christel and Raad, Maria Abi and Forbes, Hannah and Bulanova, Anna and Stanway, Jeff and Sundararajan, Mukund and Ungureanu, Victor and Bishop, Colton and Li, Yunjie and Venkatraman, Balaji and Li, Bo and Thornton, Chloe and Scellato, Salvatore and Gupta, Nishesh and Wang, Yicheng and Tenney, Ian and Wu, Xihui and Shenoy, Ashish and Carvajal, Gabriel and Wright, Diana Gage and Bariach, Ben and Xiao, Zhuyun and Hawkins, Peter and Dalmia, Sid and Farabet, Clement and Valenzuela, Pedro and Yuan, Quan and Welty, Chris and Agarwal, Ananth and Chen, Mia and Kim, Wooyeol and Hulse, Brice and Dukkipati, Nandita and Paszke, Adam and Bolt, Andrew and Davoodi, Elnaz and Choo, Kiam and Beattie, Jennifer and Prendki, Jennifer and Vashisht, Harsha and {Santamaria-Fernandez}, Rebeca and Cobo, Luis C. and Wilkiewicz, Jarek and Madras, David and Elqursh, Ali and Uy, Grant and Ramirez, Kevin and Harvey, Matt and Liechty, Tyler and Zen, Heiga and Seibert, Jeff and Hu, Clara Huiyi and Elhawaty, Mohamed and Khorlin, Andrey and Le, Maigo and Aharoni, Asaf and Li, Megan and Wang, Lily and Kumar, Sandeep and Lince, Alejandro and Casagrande, Norman and Hoover, Jay and Badawy, Dalia El and Soergel, David and Vnukov, Denis and Miecnikowski, Matt and Simsa, Jiri and Koop, Anna and Kumar, Praveen and Sellam, Thibault and Vlasic, Daniel and Daruki, Samira and Shabat, Nir and Zhang, John and Su, Guolong and Zhang, Jiageng and Liu, Jeremiah and Sun, Yi and Palmer, Evan and Ghaffarkhah, Alireza and Xiong, Xi and Cotruta, Victor and Fink, Michael and Dixon, Lucas and Sreevatsa, Ashwin and Goedeckemeyer, Adrian and Dimitriev, Alek and Jafari, Mohsen and Crocker, Remi and FitzGerald, Nicholas and Kumar, Aviral and Ghemawat, Sanjay and Philips, Ivan and Liu, Frederick and Liang, Yannie and Sterneck, Rachel and Repina, Alena and Wu, Marcus and Knight, Laura and Georgiev, Marin and Lee, Hyo and Askham, Harry and Chakladar, Abhishek and Louis, Annie and Crous, Carl and Cate, Hardie and Petrova, Dessie and Quinn, Michael and {Owusu-Afriyie}, Denese and Singhal, Achintya and Wei, Nan and Kim, Solomon and Vincent, Damien and Nasr, Milad and {Choquette-Choo}, Christopher A. and Tojo, Reiko and Lu, Shawn and Casas, Diego de Las and Cheng, Yuchung and Bolukbasi, Tolga and Lee, Katherine and Fatehi, Saaber and Ananthanarayanan, Rajagopal and Patel, Miteyan and Kaed, Charbel and Li, Jing and Sygnowski, Jakub and Belle, Shreyas Rammohan and Chen, Zhe and Konzelmann, Jaclyn and P{\~o}der, Siim and Garg, Roopal and Koverkathu, Vinod and Brown, Adam and Dyer, Chris and Liu, Rosanne and Nova, Azade and Xu, Jun and Petrov, Slav and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-03-11},
  abstract = {In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({$>$}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
  howpublished = {https://arxiv.org/abs/2403.05530v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/UK4FR4GA/Reid et al. - 2024 - Gemini 1.5 Unlocking multimodal understanding across millions of tokens of context.pdf}
}

@misc{riedl_2014,
  title = {The {{Lovelace}} 2.0 {{Test}} of {{Artificial Creativity}} and {{Intelligence}}},
  author = {Riedl, Mark O.},
  year = {2014},
  month = dec,
  number = {arXiv:1410.6142},
  eprint = {1410.6142},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1410.6142},
  urldate = {2022-12-22},
  abstract = {Observing that the creation of certain types of artistic artifacts necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an alternative to the Turing Test as a means of determining whether an agent is intelligent. The Lovelace 2.0 Test builds off prior tests of creativity and additionally provides a means of directly comparing the relative intelligence of different agents.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Intelligence},
  annotation = {42 citations (Semantic Scholar/arXiv) [2022-12-22]},
  file = {/Users/chenghao/Zotero/storage/ZJST9YRT/Riedl_(2014)_The_Lovelace_2.pdf}
}

@misc{rooij_2023,
  title = {Reclaiming {{AI}} as a Theoretical Tool for Cognitive Science},
  author = {van Rooij, Iris and Guest, Olivia and Adolfi, Federico G. and de Haan, Ronald and Kolokolova, Antonina and Rich, Patricia},
  year = {2023},
  month = aug,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/4cbuv},
  urldate = {2023-08-07},
  abstract = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
  langid = {american},
  keywords = {artificial intelligence (AI),Cognitive Psychology,cognitive science,computational complexity,engineering,explanation,Meta-science,Social and Behavioral Sciences,theory,Theory and Philosophy of Science},
  annotation = {\{"size": 513481, "pages": 21, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/BU8AWR2I/Rooij et al. - 2023 - Reclaiming AI as a theoretical tool for cognitive .pdf}
}

@inproceedings{ruder_2022,
  title = {Square {{One Bias}} in {{NLP}}: {{Towards}} a {{Multi-Dimensional Exploration}} of the {{Research Manifold}}},
  shorttitle = {Square {{One Bias}} in {{NLP}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Ruder, Sebastian and Vuli{\'c}, Ivan and S{\o}gaard, Anders},
  year = {2022},
  pages = {2340--2354},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.184},
  urldate = {2022-06-26},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-06-26]},
  file = {/Users/chenghao/Zotero/storage/948DPXZN/Ruder_et_al_(2022)_Square_One_Bias_in_NLP.pdf}
}

@misc{rust_2022,
  title = {Language {{Modelling}} with {{Pixels}}},
  author = {Rust, Phillip and Lotz, Jonas F. and Bugliarello, Emanuele and Salesky, Elizabeth and {de Lhoneux}, Miryam and Elliott, Desmond},
  year = {2022},
  month = jul,
  number = {arXiv:2207.06991},
  eprint = {2207.06991},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.06991},
  urldate = {2023-04-25},
  abstract = {Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches, instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 2648383, "pages": 35, "previous": "arXiv:2207.06991 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/PU2C8FBM/Rust et al. - 2022 - Language Modelling with Pixels.pdf}
}

@inproceedings{sachan_2021,
  title = {End-to-{{End Training}} of {{Multi-Document Reader}} and {{Retriever}} for {{Open-Domain Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sachan, Devendra Singh and Reddy, Siva and Hamilton, William L. and Dyer, Chris and Yogatama, Dani},
  year = {2021},
  month = oct,
  urldate = {2022-11-02},
  abstract = {We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3\% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/532JWNMA/Sachan_et_al_(2021)_End-to-End_Training_of_Multi-Document_Reader_and_Retriever_for_Open-Domain.pdf;/Users/chenghao/Zotero/storage/IRZWIX25/forum.html}
}

@article{salesky_2021,
  title = {Robust {{Open-Vocabulary Translation}} from {{Visual Text Representations}}},
  author = {Salesky, Elizabeth and Etter, David and Post, Matt},
  year = {2021},
  journal = {arXiv},
  abstract = {Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an 'open-vocabulary.' This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text. We show that models using visual text representations approach or match performance of text baselines on clean TED datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German--English task where subword models degrade to 1.9.},
  keywords = {embedding,nlp},
  annotation = {7 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv/2021/Salesky et al_2021_Robust Open-Vocabulary Translation from Visual Text Representations.pdf}
}

@misc{samragh_2023,
  title = {Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones},
  shorttitle = {Weight Subcloning},
  author = {Samragh, Mohammad and Farajtabar, Mehrdad and Mehta, Sachin and Vemulapalli, Raviteja and Faghri, Fartash and Naik, Devang and Tuzel, Oncel and Rastegari, Mohammad},
  year = {2023},
  month = dec,
  number = {arXiv:2312.09299},
  eprint = {2312.09299},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09299},
  urldate = {2023-12-22},
  abstract = {Training large transformer models from scratch for a target task requires lots of data and is computationally demanding. The usual practice of transfer learning overcomes this challenge by initializing the model with weights of a pretrained model of the same size and specification to increase the convergence and training speed. However, what if no pretrained model of the required size is available? In this paper, we introduce a simple yet effective technique to transfer the knowledge of a pretrained model to smaller variants. Our approach called weight subcloning expedites the training of scaled-down transformers by initializing their weights from larger pretrained models. Weight subcloning involves an operation on the pretrained model to obtain the equivalent initialized scaled-down model. It consists of two key steps: first, we introduce neuron importance ranking to decrease the embedding dimension per layer in the pretrained model. Then, we remove blocks from the transformer model to match the number of layers in the scaled-down network. The result is a network ready to undergo training, which gains significant improvements in training speed compared to random initialization. For instance, we achieve 4x faster training for vision transformers in image classification and language models designed for next token prediction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 1132959, "pages": 10, "previous": "arXiv:2312.09299 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/YX7VDZZ2/Samragh et al. - 2023 - Weight subcloning direct initialization of transformers using larger pretrained ones.pdf}
}

@misc{sancheti_2022,
  title = {What to {{Read}} in a {{Contract}}? {{Party-Specific Summarization}} of {{Important Obligations}}, {{Entitlements}}, and {{Prohibitions}} in {{Legal Documents}}},
  shorttitle = {What to {{Read}} in a {{Contract}}?},
  author = {Sancheti, Abhilasha and Garimella, Aparna and Srinivasan, Balaji Vasan and Rudinger, Rachel},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09825},
  eprint = {2212.09825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09825},
  urldate = {2023-06-20},
  abstract = {Legal contracts, such as employment or lease agreements, are important documents as they govern the obligations and entitlements of the various contracting parties. However, these documents are typically long and written in legalese resulting in lots of manual hours spent in understanding them. In this paper, we address the task of summarizing legal contracts for each of the contracting parties, to enable faster reviewing and improved understanding of them. Specifically, we collect a dataset consisting of pairwise importance comparison annotations by legal experts for {\textasciitilde}293K sentence pairs from lease agreements. We propose a novel extractive summarization system to automatically produce a summary consisting of the most important obligations, entitlements, and prohibitions in a contract. It consists of two modules: (1) a content categorize to identify sentences containing each of the categories (i.e., obligation, entitlement, and prohibition) for a party, and (2) an importance ranker to compare the importance among sentences of each category for a party to obtain a ranked list. The final summary is produced by selecting the most important sentences of a category for each of the parties. We demonstrate the effectiveness of our proposed system by comparing it against several text ranking baselines via automatic and human evaluation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/9Z73NXIY/Sancheti et al. - 2022 - What to Read in a Contract Party-Specific Summari.pdf;/Users/chenghao/Zotero/storage/HJRTA6DL/2212.html}
}

@misc{schaeffer_2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.15004},
  eprint = {2304.15004},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.15004},
  urldate = {2023-05-02},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers). In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-05-01]},
  file = {/Users/chenghao/Zotero/storage/YY6N3FJP/Schaeffer_et_al_(2023)_Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage.pdf}
}

@article{schick_2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  doi = {10.48550/arXiv.2302.04761},
  urldate = {2023-02-10},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/WHLJ3MX3/Schick_et_al_(2023)_Toolformer.pdf}
}

@misc{schlangen_2023,
  title = {What {{A Situated Language-Using Agent Must}} Be {{Able}} to {{Do}}: {{A Top-Down Analysis}}},
  shorttitle = {What {{A Situated Language-Using Agent Must}} Be {{Able}} to {{Do}}},
  author = {Schlangen, David},
  year = {2023},
  month = feb,
  number = {arXiv:2302.08590},
  eprint = {2302.08590},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.08590},
  urldate = {2023-04-27},
  abstract = {Even in our increasingly text-intensive times, the primary site of language use is situated, co-present interaction. It is primary ontogenetically and phylogenetically, and it is arguably also still primary in negotiating everyday social situations. Situated interaction is also the final frontier of Natural Language Processing, where, compared to the area of text processing, very little progress has been made in the past decade, and where a myriad of practical applications is waiting to be unlocked. While the usual approach in the field is to reach, bottom-up, for the ever next "adjacent possible", in this paper I attempt a top-down analysis of what the demands are that unrestricted situated interaction makes on the participating agent, and suggest ways in which this analysis can structure computational models and research on them. Specifically, I discuss representational demands (the building up and application of world model, language model, situation model, discourse model, and agent model) and what I call anchoring processes (incremental processing, incremental learning, conversational grounding, multimodal grounding) that bind the agent to the here, now, and us.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/KGHRRVEG/Schlangen - 2023 - What A Situated Language-Using Agent Must be Able .pdf}
}

@misc{schuett_2023,
  title = {Towards Best Practices in {{AGI}} Safety and Governance: {{A}} Survey of Expert Opinion},
  shorttitle = {Towards Best Practices in {{AGI}} Safety and Governance},
  author = {Schuett, Jonas and Dreksler, Noemi and Anderljung, Markus and McCaffary, David and Heim, Lennart and Bluemke, Emma and Garfinkel, Ben},
  year = {2023},
  month = may,
  number = {arXiv:2305.07153},
  eprint = {2305.07153},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07153},
  urldate = {2023-05-15},
  abstract = {A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98\% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society},
  annotation = {\{"size": 845610, "pages": 38, "previous": "arXiv:2305.07153 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/BKSWPTAH/Schuett et al. - 2023 - Towards best practices in AGI safety and governanc.pdf}
}

@misc{schuff_2022,
  title = {Human {{Interpretation}} of {{Saliency-based Explanation Over Text}}},
  author = {Schuff, Hendrik and Jacovi, Alon and Adel, Heike and Goldberg, Yoav and Vu, Ngoc Thang},
  year = {2022},
  month = jan,
  number = {arXiv:2201.11569},
  eprint = {2201.11569},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-06-19},
  abstract = {While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople's interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees' importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {1 citations (Semantic Scholar/arXiv) [2022-06-19]},
  file = {/Users/chenghao/Zotero/storage/UMGU4M7Q/Schuff_et_al_(2022)_Human_Interpretation_of_Saliency-based_Explanation_Over_Text.pdf}
}

@misc{schuster_2022,
  title = {Confident {{Adaptive Language Modeling}}},
  author = {Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q. and Tay, Yi and Metzler, Donald},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07061},
  eprint = {2207.07061},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.07061},
  urldate = {2022-07-23},
  abstract = {Recent advances in Transformer-based large language models (LLMs) have led to significant performance improvements across many tasks. These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time. In practice, however, the series of generations made by LLMs is composed of varying levels of difficulty. While certain predictions truly benefit from the models' full capacity, other continuations are more trivial and can be solved with reduced compute. In this work, we introduce Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep. Early exit decoding involves several challenges that we address here, such as: (1) what confidence measure to use; (2) connecting sequence-level constraints to local per-token exit decisions; and (3) attending back to missing hidden representations due to early exits in previous tokens. Through theoretical analysis and empirical experiments on three diverse text generation tasks, we demonstrate the efficacy of our framework in reducing compute -- potential speedup of up to \${\textbackslash}times 3\$ -- while provably maintaining high performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-23] 0 citations (Semantic Scholar/DOI) [2022-07-23]},
  file = {/Users/chenghao/Zotero/storage/HZ693FBT/Schuster_et_al_(2022)_Confident_Adaptive_Language_Modeling.pdf}
}

@misc{schwartz_2022,
  title = {On the {{Limitations}} of {{Dataset Balancing}}: {{The Lost Battle Against Spurious Correlations}}},
  shorttitle = {On the {{Limitations}} of {{Dataset Balancing}}},
  author = {Schwartz, Roy and Stanovsky, Gabriel},
  year = {2022},
  month = apr,
  number = {arXiv:2204.12708},
  eprint = {2204.12708},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2204.12708},
  urldate = {2022-06-05},
  abstract = {Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out "easy" instances (Sakaguchi et al., 2020), culminating in a recent proposal to eliminate single-word correlations altogether (Gardner et al., 2021). In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations. In parallel, a truly balanced dataset may be bound to "throw the baby out with the bathwater" and miss important signal encoding common sense and world knowledge. We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero- or few-shot setups.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/5DHSTAZA/Schwartz_Stanovsky_(2022)_On_the_Limitations_of_Dataset_Balancing.pdf}
}

@inproceedings{shah_2022,
  title = {Situating {{Search}}},
  booktitle = {{{ACM SIGIR Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Shah, Chirag and Bender, Emily M.},
  year = {2022},
  month = mar,
  pages = {221--232},
  publisher = {ACM},
  address = {Regensburg Germany},
  doi = {10.1145/3498366.3505816},
  urldate = {2022-11-17},
  abstract = {Search systems, like many other applications of machine learning, have become increasingly complex and opaque. The notions of relevance, usefulness, and trustworthiness with respect to information were already overloaded and often difficult to articulate, study, or implement. Newly surfaced proposals that aim to use large language models to generate relevant information for a user's needs pose even greater threat to transparency, provenance, and user interactions in a search system. In this perspective paper we revisit the problem of search in the larger context of information seeking and argue that removing or reducing interactions in an effort to retrieve presumably more relevant information can be detrimental to many fundamental aspects of search, including information verification, information literacy, and serendipity. In addition to providing suggestions for counteracting some of the potential problems posed by such models, we present a vision for search systems that are intelligent and effective, while also providing greater transparency and accountability.},
  isbn = {978-1-4503-9186-3},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/DOI) [2022-11-16]},
  file = {/Users/chenghao/Zotero/storage/VV96L4WX/Shah_Bender_(2022)_Situating_Search.pdf}
}

@article{shah_2024,
  title = {Envisioning {{Information Access Systems}}: {{What Makes}} for {{Good Tools}} and a {{Healthy Web}}?},
  shorttitle = {Envisioning {{Information Access Systems}}},
  author = {Shah, Chirag and Bender, Emily M.},
  year = {2024},
  month = feb,
  journal = {ACM Transactions on the Web},
  pages = {3649468},
  issn = {1559-1131, 1559-114X},
  doi = {10.1145/3649468},
  urldate = {2024-02-28},
  abstract = {We observe a recent trend towards applying large language models (LLMs) in search and positioning them as efective information access systems. While the interfaces may look appealing and the apparent breadth of applicability is exciting, we are concerned that the ield is rushing ahead with a technology without suicient study of the uses it is meant to serve, how it would be used, and what its use would mean. We argue that it is important to reassert the central research focus of the ield of information retrieval, because information access is not merely an application to be solved by the so-called `AI' techniques du jour. Rather, it is a key human activity, with impacts on both individuals and society. As information scientists, we should be asking what do people and society want and need from information access systems and how do we design and build systems to meet those needs? With that goal, in this conceptual paper we investigate fundamental questions concerning information access from user and societal viewpoints. We revisit foundational work related to information behavior, information seeking, information retrieval, information iltering, and information access to resurface what we know about these fundamental questions and what may be missing. We then provide our conceptual framing about how we could ill this gap, focusing on methods as well as experimental and evaluation frameworks. We consider the Web as an information ecosystem and explore the ways in which synthetic media, produced by LLMs and otherwise, endangers that ecosystem. The primary goal of this conceptual paper is to shed light on what we still do not know about the potential impacts of LLM-based information access systems, how to advance our understanding of user behaviors, and where the next generations of students, scholars, and developers could fruitfully invest their energies. CCS Concepts: {$\cdot$} Information systems {$\rightarrow$} Web searching and information discovery; Users and interactive retrieval; {$\cdot$} Computing methodologies {$\rightarrow$} Natural language processing.},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/LSUEXE2Z/Shah and Bender - 2024 - Envisioning Information Access Systems What Makes for Good Tools and a Healthy Web.pdf}
}

@misc{shanahan_2022,
  title = {Talking {{About Large Language Models}}},
  author = {Shanahan, Murray},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03551},
  eprint = {2212.03551},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-12-11},
  abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as ``knows'', ``believes'', and ``thinks'', when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-11]},
  file = {/Users/chenghao/Zotero/storage/LN5A4MU8/Shanahan_(2022)_Talking_About_Large_Language_Models.pdf}
}

@misc{shenoy_2024,
  title = {Lumos : {{Empowering Multimodal LLMs}} with {{Scene Text Recognition}}},
  shorttitle = {Lumos},
  author = {Shenoy, Ashish and Lu, Yichao and Jayakumar, Srihari and Chatterjee, Debojeet and Moslehpour, Mohsen and Chuang, Pierce and Harpale, Abhay and Bhardwaj, Vikas and Xu, Di and Zhao, Shicong and Zhao, Longfang and Ramchandani, Ankit and Dong, Xin Luna and Kumar, Anuj},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-14},
  abstract = {We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.},
  howpublished = {https://arxiv.org/abs/2402.08017v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/5WYTF3DY/Shenoy_et_al_(2024)_Lumos.pdf}
}

@misc{shevlane_2023,
  title = {Model Evaluation for Extreme Risks},
  author = {Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and Ho, Lewis and Siddarth, Divya and Avin, Shahar and Hawkins, Will and Kim, Been and Gabriel, Iason and Bolina, Vijay and Clark, Jack and Bengio, Yoshua and Christiano, Paul and Dafoe, Allan},
  year = {2023},
  month = may,
  number = {arXiv:2305.15324},
  eprint = {2305.15324},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15324},
  urldate = {2023-05-25},
  abstract = {Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,K.4.1},
  annotation = {\{"size": 803057, "pages": 20, "previous": "arXiv:2305.15324 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/CBXV3SUE/Shevlane et al. - 2023 - Model evaluation for extreme risks.pdf}
}

@misc{shi_2023,
  title = {Large {{Language Models Can Be Easily Distracted}} by {{Irrelevant Context}}},
  author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2302.00093},
  eprint = {2302.00093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00093},
  urldate = {2023-02-04},
  abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,nosource},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-02-04]},
  file = {/Users/chenghao/Zotero/storage/IV246Z3J/Shi_et_al_(2023)_Large_Language_Models_Can_Be_Easily_Distracted_by_Irrelevant_Context.pdf}
}

@misc{shi_2023a,
  title = {Detecting {{Pretraining Data}} from {{Large Language Models}}},
  author = {Shi, Weijia and Ajith, Anirudh and Xia, Mengzhou and Huang, Yangsibo and Liu, Daogao and Blevins, Terra and Chen, Danqi and Zettlemoyer, Luke},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16789},
  eprint = {2310.16789},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.16789},
  urldate = {2023-11-02},
  abstract = {Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K\% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K\% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K\% Prob achieves a 7.4\% improvement on WIKIMIA over these previous methods. We apply Min-K\% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/chenghao/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Shi et_al/2023/Shi_et_al_(2023)_Detecting_Pretraining_Data_from_Large_Language_Models.pdf}
}

@misc{shumailov_2023,
  title = {The {{Curse}} of {{Recursion}}: {{Training}} on {{Generated Data Makes Models Forget}}},
  shorttitle = {The {{Curse}} of {{Recursion}}},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-02-28},
  abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
  howpublished = {https://arxiv.org/abs/2305.17493v2},
  langid = {english},
  annotation = {83 citations (Semantic Scholar/arXiv) [2024-02-28]},
  file = {/Users/chenghao/Zotero/storage/3XWRCU8R/Shumailov et al. - 2023 - The Curse of Recursion Training on Generated Data Makes Models Forget.pdf}
}

@inproceedings{skalicky_2022,
  title = {Business {{Document Information Extraction}}: {{Towards Practical Benchmarks}}},
  shorttitle = {Business {{Document Information Extraction}}},
  booktitle = {{{CLEF}}},
  author = {Skalick{\'y}, Maty{\'a}s and Simsa, Step{\'a}n and U{\v r}i{\v c}{\'a}{\v r}, Michal and {\v S}ulc, Milan},
  year = {2022},
  doi = {10.48550/arXiv.2206.11229},
  abstract = {There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive, and potential sources of available documents including synthetic data are discussed. Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.},
  annotation = {GSCC: 0000132},
  file = {/Users/chenghao/Zotero/storage/CVJE2559/Skalický_et_al_(2022)_Business_Document_Information_Extraction.pdf}
}

@misc{soldaini_2024,
  title = {Dolma: An {{Open Corpus}} of {{Three Trillion Tokens}} for {{Language Model Pretraining Research}}},
  shorttitle = {Dolma},
  author = {Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and Hofmann, Valentin and Jha, Ananya Harsh and Kumar, Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan and Magnusson, Ian and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Ravichander, Abhilasha and Richardson, Kyle and Shen, Zejiang and Strubell, Emma and Subramani, Nishant and Tafjord, Oyvind and Walsh, Pete and Zettlemoyer, Luke and Smith, Noah A. and Hajishirzi, Hannaneh and Beltagy, Iz and Groeneveld, Dirk and Dodge, Jesse and Lo, Kyle},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-02-02},
  abstract = {Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.},
  howpublished = {https://arxiv.org/abs/2402.00159v1},
  langid = {english},
  annotation = {\{"size": 10087885, "pages": 83, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/ZA54BQ79/Soldaini et al. - 2024 - Dolma an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.pdf}
}

@misc{somepalli_2022,
  title = {Diffusion {{Art}} or {{Digital Forgery}}? {{Investigating Data Replication}} in {{Diffusion Models}}},
  shorttitle = {Diffusion {{Art}} or {{Digital Forgery}}?},
  author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03860},
  eprint = {2212.03860},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.03860},
  urldate = {2022-12-09},
  abstract = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they stealing content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-12-09]},
  file = {/Users/chenghao/Zotero/storage/65JD2YNJ/Somepalli_et_al_(2022)_Diffusion_Art_or_Digital_Forgery.pdf}
}

@article{song_,
  title = {{{PowerInfer}}: {{Fast Large Language Model Serving}} with a {{Consumer-grade GPU}}},
  author = {Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  abstract = {This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a powerlaw distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18\% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69{\texttimes} while retaining model accuracy.},
  langid = {english},
  annotation = {\{"size": 483962, "pages": 15, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/8Z8KK4J2/Song et al. - PowerInfer Fast Large Language Model Serving with a Consumer-grade GPU.pdf}
}

@misc{sorensen_2023,
  title = {Value {{Kaleidoscope}}: {{Engaging AI}} with {{Pluralistic Human Values}}, {{Rights}}, and {{Duties}}},
  shorttitle = {Value {{Kaleidoscope}}},
  author = {Sorensen, Taylor and Jiang, Liwei and Hwang, Jena and Levine, Sydney and Pyatkin, Valentina and West, Peter and Dziri, Nouha and Lu, Ximing and Rao, Kavel and Bhagavatula, Chandra and Sap, Maarten and Tasioulas, John and Choi, Yejin},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-01-20},
  abstract = {Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91\% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.},
  howpublished = {https://arxiv.org/abs/2309.00779v1},
  langid = {english},
  annotation = {\{"size": 10978405, "pages": 50, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/3IR6P545/Sorensen et al. - 2023 - Value Kaleidoscope Engaging AI with Pluralistic Human Values, Rights, and Duties.pdf}
}

@misc{sorscher_2022,
  title = {Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning},
  shorttitle = {Beyond Neural Scaling Laws},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14486},
  eprint = {2206.14486},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.14486},
  urldate = {2022-07-03},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this new exponential scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling performance on ResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-07-02]},
  file = {/Users/chenghao/Zotero/storage/R34G7VPG/Sorscher_et_al_(2022)_Beyond_neural_scaling_laws.pdf}
}

@misc{srivastava_2022,
  title = {Beyond the {{Imitation Game}}: {{Quantifying}} and Extrapolating the Capabilities of Language Models},
  shorttitle = {Beyond the {{Imitation Game}}},
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and {Garriga-Alonso}, Adri{\`a} and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Santilli, Andrea and Stuhlm{\"u}ller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karaka{\c s}, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bart{\l}omiej and {\"O}zyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ram{\'i}rez, C{\'e}sar Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and {Callison-Burch}, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and Gonz{\'a}lez, Daniel Mosegu{\'i} and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and {Mart{\'i}nez-Plumed}, Fernando and Happ{\'e}, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and {de Melo}, Gerard and Kruszewski, Germ{\'a}n and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and {Jaimovitch-L{\'o}pez}, Gonzalo and Betz, Gregor and {Gur-Ari}, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Sch{\"u}tze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fern{\'a}ndez and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Koco{\'n}, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and {Sohl-Dickstein}, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, J{\"o}rg and Rozen, Jos and {Hernandez-Orallo}, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and {Contreras-Ochando}, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Col{\'o}n, Luis Oliveros and Metz, Luke and {\c S}enel, L{\"u}tfi Kerem and Bosma, Maarten and Sap, Maarten and {ter Hoeve}, Maartje and Andrea, Madotto and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ram{\'i}rez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, M{\'a}ty{\'a}s and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Sw{\k e}drowski, Micha{\l} and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Mi{\l}kowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ram{\'o}n Risco and Milli{\`e}re, Rapha{\"e}l and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Th{\'e}o and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and {Telleen-Lawton}, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Xinran, Zhao and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04615},
  eprint = {2206.04615},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-06-11},
  abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/2J59UXU9/Srivastava_et_al_(2022)_Beyond_the_Imitation_Game.pdf}
}

@misc{stanislawek_2021,
  title = {Kleister: {{Key Information Extraction Datasets Involving Long Documents}} with {{Complex Layouts}}},
  shorttitle = {Kleister},
  author = {Stanis{\l}awek, Tomasz and Grali{\'n}ski, Filip and Wr{\'o}blewska, Anna and Lipi{\'n}ski, Dawid and Kaliska, Agnieszka and Rosalska, Paulina and Topolski, Bartosz and Biecek, Przemys{\l}aw},
  year = {2021},
  month = may,
  number = {arXiv:2105.05796},
  eprint = {2105.05796},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.05796},
  urldate = {2022-10-07},
  abstract = {The relevance of the Key Information Extraction (KIE) task is increasingly important in natural language processing problems. But there are still only a few well-defined problems that serve as benchmarks for solutions in this area. To bridge this gap, we introduce two new datasets (Kleister NDA and Kleister Charity). They involve a mix of scanned and born-digital long formal English-language documents. In these datasets, an NLP system is expected to find or infer various types of entities by employing both textual and structural layout features. The Kleister Charity dataset consists of 2,788 annual financial reports of charity organizations, with 61,643 unique pages and 21,612 entities to extract. The Kleister NDA dataset has 540 Non-disclosure Agreements, with 3,229 unique pages and 2,160 entities to extract. We provide several state-of-the-art baseline systems from the KIE domain (Flair, BERT, RoBERTa, LayoutLM, LAMBERT), which show that our datasets pose a strong challenge to existing models. The best model achieved an 81.77\% and an 83.57\% F1-score on respectively the Kleister NDA and the Kleister Charity datasets. We share the datasets to encourage progress on more in-depth and complex information extraction tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {GSCC: 0000007  8 citations (Semantic Scholar/arXiv) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/KIETX8JQ/Stanisławek_et_al_(2021)_Kleister.pdf}
}

@misc{subramani_2021,
  title = {A {{Survey}} of {{Deep Learning Approaches}} for {{OCR}} and {{Document Understanding}}},
  author = {Subramani, Nishant and Matton, Alexandre and Greaves, Malcolm and Lam, Adrian},
  year = {2021},
  month = feb,
  number = {arXiv:2011.13534},
  eprint = {2011.13534},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.13534},
  urldate = {2022-10-07},
  abstract = {Documents are a core part of many businesses in many fields such as law, finance, and technology among others. Automatic understanding of documents such as invoices, contracts, and resumes is lucrative, opening up many new avenues of business. The fields of natural language processing and computer vision have seen tremendous progress through the development of deep learning such that these methods have started to become infused in contemporary document understanding systems. In this survey paper, we review different techniques for document understanding for documents written in English and consolidate methodologies present in literature to act as a jumping-off point for researchers exploring this area.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {GSCC: 0000011  9 citations (Semantic Scholar/arXiv) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/PSDPX6YC/Subramani_et_al_(2021)_A_Survey_of_Deep_Learning_Approaches_for_OCR_and_Document_Understanding.pdf}
}

@misc{sucholutsky_2023,
  title = {Getting Aligned on Representational Alignment},
  author = {Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C. and Grant, Erin and Groen, Iris and Achterberg, Jascha and Tenenbaum, Joshua B. and Collins, Katherine M. and Hermann, Katherine L. and Oktar, Kerem and Greff, Klaus and Hebart, Martin N. and Jacoby, Nori and Zhang, Qiuyi and Marjieh, Raja and Geirhos, Robert and Chen, Sherol and Kornblith, Simon and Rane, Sunayana and Konkle, Talia and O'Connell, Thomas P. and Unterthiner, Thomas and Lampinen, Andrew K. and M{\"u}ller, Klaus-Robert and Toneva, Mariya and Griffiths, Thomas L.},
  year = {2023},
  month = nov,
  number = {arXiv:2310.13018},
  eprint = {2310.13018},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.13018},
  urldate = {2023-12-22},
  abstract = {Biological and artificial information processing systems form representations that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the extent to which the representations formed by these diverse systems agree? Do similarities in representations then translate into similar behavior? How can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most active research areas in cognitive science, neuroscience, and machine learning. For example, cognitive scientists measure the representational alignment of multiple individuals to identify shared cognitive priors, neuroscientists align fMRI responses from multiple individuals into a shared representational space for group-level analyses, and ML researchers distill knowledge from teacher models into student models by increasing their alignment. Unfortunately, there is limited knowledge transfer between research communities interested in representational alignment, so progress in one field often ends up being rediscovered independently in another. Thus, greater cross-field communication would be advantageous. To improve communication between these fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from all three fields and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  annotation = {\{"size": 12888323, "pages": 47, "previous": "arXiv:2310.13018 [cs, q-bio]"\}},
  file = {/Users/chenghao/Zotero/storage/CIFHXCV6/Sucholutsky_et_al_(2023)_Getting_aligned_on_representational_alignment.pdf}
}

@inproceedings{talat_2022,
  title = {On the {{Machine Learning}} of {{Ethical Judgments}} from {{Natural Language}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Talat, Zeerak and Blix, Hagen and Valvoda, Josef and Ganesh, Maya Indira and Cotterell, Ryan and Williams, Adina},
  year = {2022},
  month = jul,
  pages = {769--779},
  doi = {10.18653/v1/2022.naacl-main.56},
  urldate = {2024-01-20},
  abstract = {Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, Adina Williams. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.},
  langid = {american},
  annotation = {\{"size": 311566, "pages": 11, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/MJAH4FJ5/Talat et al. - 2022 - On the Machine Learning of Ethical Judgments from Natural Language.pdf}
}

@misc{tamkin_2023,
  title = {Evaluating and {{Mitigating Discrimination}} in {{Language Model Decisions}}},
  author = {Tamkin, Alex and Askell, Amanda and Lovitt, Liane and Durmus, Esin and Joseph, Nicholas and Kravec, Shauna and Nguyen, Karina and Kaplan, Jared and Ganguli, Deep},
  year = {2023},
  month = dec,
  number = {arXiv:2312.03689},
  eprint = {2312.03689},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.03689},
  urldate = {2023-12-12},
  abstract = {As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {\{"size": 3169819, "pages": 28, "previous": "arXiv:2312.03689 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/2DMGIYY3/Tamkin_et_al_(2023)_Evaluating_and_Mitigating_Discrimination_in_Language_Model_Decisions.pdf}
}

@misc{tang_2022,
  title = {Unifying {{Vision}}, {{Text}}, and {{Layout}} for {{Universal Document Processing}}},
  author = {Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  year = {2022},
  month = dec,
  number = {arXiv:2212.02623},
  doi = {10.48550/arXiv.2212.02623},
  urldate = {2022-12-07},
  abstract = {We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 9 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and websites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark (DUE).},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 9015523, "pages": 16, "previous": "arXiv:2212.02623 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/VD79KSPF/Tang_et_al_(2022)_Unifying_Vision,_Text,_and_Layout_for_Universal_Document_Processing.pdf}
}

@article{tao_2022,
  title = {Knowing {{Where}} and {{What}}: {{Unified Word Block Pretraining}} for {{Document Understanding}}},
  shorttitle = {Knowing {{Where}} and {{What}}},
  author = {Tao, Song and Wang, Zijian and Fan, Tiantian and Luo, Canjie and Huang, Can},
  year = {2022},
  journal = {undefined},
  urldate = {2022-10-07},
  abstract = {This paper focuses on the embedding learning of word blocks containing text and layout information, and proposes UTel, a language model with Unified TExt and Layout pre-training that achieves superior performance than previous methods on various downstream tasks, though requiring no image modality. Due to the complex layouts of documents, it is challenging to extract information for documents. Most previous studies develop multimodal pre-trained models in a self-supervised way. In this paper, we focus on the embedding learning of word blocks containing text and layout information, and propose UTel, a language model with Unified TExt and Layout pre-training. Specifically, we propose two pre-training tasks: Surrounding Word Prediction (SWP) for the layout learning, and Contrastive learning of Word Embeddings (CWE) for identifying different word blocks. Moreover, we replace the commonly used 1D position embedding with a 1D clipped relative position embedding. In this way, the joint training of Masked Layout-Language Modeling (MLLM) and two newly proposed tasks enables the interaction between semantic and spatial features in a unified way. Additionally, the proposed UTel can process arbitrary-length sequences by removing the 1D position embedding, while maintaining competitive performance. Extensive experimental results show UTel learns better joint representations and achieves superior performance than previous methods on various downstream tasks, though requiring no image modality. Code is available at https://github.com/taosong2019/UTel .},
  langid = {english},
  keywords = {nosource},
  annotation = {GSCC: 0000109}
}

@misc{tay_2022,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2022},
  month = mar,
  number = {arXiv:2009.06732},
  eprint = {2009.06732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.06732},
  urldate = {2022-09-10},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {345 citations (Semantic Scholar/arXiv) [2022-09-10]},
  file = {/Users/chenghao/Zotero/storage/WSG4Q3BT/Tay_et_al_(2022)_Efficient_Transformers.pdf}
}

@misc{taylor_2022,
  title = {Galactica: {{A Large Language Model}} for {{Science}}},
  shorttitle = {Galactica},
  author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09085},
  eprint = {2211.09085},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.09085},
  urldate = {2022-11-21},
  abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-11-20]},
  file = {/Users/chenghao/Zotero/storage/9TDKTR7G/Taylor_et_al_(2022)_Galactica.pdf}
}

@article{thoppilan_2022,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  shorttitle = {{{LaMDA}}},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and {Meier-Hellstern}, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and {Hoffman-John}, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and {Aguera-Arcas}, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.08239 [cs]},
  eprint = {2201.08239},
  primaryclass = {cs},
  urldate = {2022-01-22},
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,dialog,lm,nlp},
  annotation = {22 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2201.08239 [cs]/2022/Thoppilan et al_2022_LaMDA.pdf}
}

@inproceedings{tito_2021,
  title = {Document {{Collection Visual Question Answering}}},
  booktitle = {{{ICDAR}}},
  author = {Tito, Rub{\`e}n P{\'e}rez and Karatzas, Dimosthenis and Valveny, Ernest},
  year = {2021},
  doi = {10.1007/978-3-030-86331-9_50},
  abstract = {This work introduces Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context useful for their interpretation. To address this problem, we introduce Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.},
  annotation = {GSCC: 0000066},
  file = {/Users/chenghao/Zotero/storage/GY2FX2NF/Tito_et_al_(2021)_Document_Collection_Visual_Question_Answering.pdf}
}

@misc{tito_2022,
  title = {Hierarchical Multimodal Transformers for {{Multi-Page DocVQA}}},
  author = {Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  year = {2022},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-02-08},
  abstract = {Document Visual Question Answering (DocVQA) refers to the task of answering questions from document images. Existing work on DocVQA only considers single-page documents. However, in real scenarios documents are mostly composed of multiple pages that should be processed altogether. In this work we extend DocVQA to the multi-page scenario. For that, we first create a new dataset, MP-DocVQA, where questions are posed over multi-page documents instead of single pages. Second, we propose a new hierarchical method, Hi-VT5, based on the T5 architecture, that overcomes the limitations of current methods to process long multi-page documents. The proposed method is based on a hierarchical transformer architecture where the encoder summarizes the most relevant information of every page and then, the decoder takes this summarized information to generate the final answer. Through extensive experimentation, we demonstrate that our method is able, in a single stage, to answer the questions and provide the page that contains the relevant information to find the answer, which can be used as a kind of explainability measure.},
  howpublished = {https://arxiv.org/abs/2212.05935v2},
  langid = {english},
  annotation = {\{"size": 7354424, "pages": 14, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/JQ7DCCXM/Tito et al. - 2022 - Hierarchical multimodal transformers for Multi-Page DocVQA.pdf}
}

@misc{tonmoy_2024,
  title = {A {{Comprehensive Survey}} of {{Hallucination Mitigation Techniques}} in {{Large Language Models}}},
  author = {Tonmoy, S. M. Towhidul Islam and Zaman, S. M. Mehedi and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-05},
  abstract = {As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.},
  howpublished = {https://arxiv.org/abs/2401.01313v2},
  langid = {english},
  annotation = {\{"size": 328464, "pages": 19, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/CN253NKM/Tonmoy_et_al_(2024)_A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language2.pdf}
}

@misc{touvron_2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-04-04},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  annotation = {35 citations (Semantic Scholar/arXiv) [2023-04-03] 35 citations (Semantic Scholar/DOI) [2023-04-03]},
  file = {/Users/chenghao/Zotero/storage/3GSHT8AZ/Touvron_et_al_(2023)_LLaMA.pdf}
}

@misc{touvron_2023a,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  howpublished = {https://arxiv.org/abs/2307.09288v2},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/MTG78EZ4/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf}
}

@misc{treviso_2022,
  title = {Efficient {{Methods}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Efficient {{Methods}} for {{Natural Language Processing}}},
  author = {Treviso, Marcos and Ji, Tianchu and Lee, Ji-Ung and {van Aken}, Betty and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Martins, Pedro H. and Martins, Andr{\'e} F. T. and Milder, Peter and Raffel, Colin and Simpson, Edwin and Slonim, Noam and Balasubramanian, Niranjan and Derczynski, Leon and Schwartz, Roy},
  year = {2022},
  month = aug,
  number = {arXiv:2209.00099},
  eprint = {2209.00099},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.00099},
  urldate = {2022-09-04},
  abstract = {Getting the most out of limited resources allows advances in natural language processing (NLP) research and practice while being conservative with resources. Those resources may be data, time, storage, or energy. Recent work in NLP has yielded interesting results from scaling; however, using only scale to improve results means that resource consumption also scales. That relationship motivates research into efficient methods that require less resources to achieve similar results. This survey relates and synthesises methods and findings in those efficiencies in NLP, aiming to guide new researchers in the field and inspire the development of new methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/NQ37RYKY/Treviso_et_al_(2022)_Efficient_Methods_for_Natural_Language_Processing.pdf}
}

@misc{tunstall_2023,
  title = {Zephyr: {{Direct Distillation}} of {{LM Alignment}}},
  shorttitle = {Zephyr},
  author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and {von Werra}, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and Sarrazin, Nathan and Sanseviero, Omar and Rush, Alexander M. and Wolf, Thomas},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16944},
  eprint = {2310.16944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.16944},
  urldate = {2023-11-09},
  abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/WAV78WQD/Tunstall et al. - 2023 - Zephyr Direct Distillation of LM Alignment.pdf}
}

@misc{turski_2023,
  title = {{{CCpdf}}: {{Building}} a {{High Quality Corpus}} for {{Visually Rich Documents}} from {{Web Crawl Data}}},
  shorttitle = {{{CCpdf}}},
  author = {Turski, Micha{\l} and Stanis{\l}awek, Tomasz and Kaczmarek, Karol and Dyda, Pawe{\l} and Grali{\'n}ski, Filip},
  year = {2023},
  month = apr,
  number = {arXiv:2304.14953},
  eprint = {2304.14953},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.14953},
  urldate = {2023-05-02},
  abstract = {In recent years, the field of document understanding has progressed a lot. A significant part of this progress has been possible thanks to the use of language models pretrained on large amounts of documents. However, pretraining corpora used in the domain of document understanding are single domain, monolingual, or nonpublic. Our goal in this paper is to propose an efficient pipeline for creating a big-scale, diverse, multilingual corpus of PDF files from all over the Internet using Common Crawl, as PDF files are the most canonical types of documents as considered in document understanding. We analysed extensively all of the steps of the pipeline and proposed a solution which is a trade-off between data quality and processing time. We also share a CCpdf corpus in a form or an index of PDF files along with a script for downloading them, which produces a collection useful for language model pretraining. The dataset and tools published with this paper offer researchers the opportunity to develop even better multilingual language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-05-01]},
  file = {/Users/chenghao/Zotero/storage/PEUI6V2C/Turski_et_al_(2023)_CCpdf.pdf}
}

@article{vassei_,
  title = {{{AI Transparency}} in {{Practice}}},
  author = {Vasse'i, Ramak Molavi and McCrosky, Jesse},
  langid = {english},
  annotation = {\{"size": 4148982, "pages": 44, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/2LX5ALN5/Vasse'i and McCrosky - AI Transparency in Practice.pdf}
}

@misc{wadhawan_2024,
  title = {{{ConTextual}}: {{Evaluating Context-Sensitive Text-Rich Visual Reasoning}} in {{Large Multimodal Models}}},
  shorttitle = {{{ConTextual}}},
  author = {Wadhawan, Rohan and Bansal, Hritik and Chang, Kai-Wei and Peng, Nanyun},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8\% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In addition to human evaluations, we also employed automatic evaluation metrics using GPT-4, uncovering similar trends in performance disparities. We also perform a fine-grained evaluation across diverse visual contexts and provide qualitative analysis which provides a robust framework for future advancements in the LMM design. https://con-textual.github.io/},
  howpublished = {https://arxiv.org/abs/2401.13311v1},
  langid = {english},
  annotation = {\{"size": 32432151, "pages": 54, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/IA49T8GT/Wadhawan et al. - 2024 - ConTextual Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models.pdf}
}

@article{wang_2021,
  title = {Dodrio: {{Exploring Transformer Models}} with {{Interactive Visualization}}},
  author = {Wang, Zijie J and Turko, Robert and Chau, Duen Horng},
  year = {2021},
  journal = {arXiv},
  abstract = {Why do large pre-trained transformer-based models perform so well across a wide variety of NLP tasks? Recent research suggests the key may lie in multi-headed attention mechanism's ability to learn and represent linguistic information. Understanding how these models represent both syntactic and semantic knowledge is vital to investigate why they succeed and fail, what they have learned, and how they can improve. We present Dodrio, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge. Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text. To facilitate the visual comparison of attention weights and linguistic knowledge, Dodrio applies different graph visualization techniques to represent attention weights scalable to longer input text. Case studies highlight how Dodrio provides insights into understanding the attention mechanism in transformer-based models. Dodrio is available at https://poloclub.github.io/dodrio/.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,nlp,transformer,visualization},
  file = {/Users/chenghao/Zotero/storage/SUNNIEKU/Wang_et_al_(2021)_Dodrio.pdf}
}

@article{wang_2021a,
  title = {{{LayoutReader}}: {{Pre-training}} of {{Text}} and {{Layout}} for {{Reading Order Detection}}},
  shorttitle = {{{LayoutReader}}},
  author = {Wang, Zilong and Xu, Yiheng and Cui, Lei and Shang, Jingbo and Wei, Furu},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.11591 [cs]},
  eprint = {2108.11591},
  primaryclass = {cs},
  urldate = {2021-10-10},
  abstract = {Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that contains reading order, text, and layout information for 500,000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically, our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. We will release the dataset and model at {\textbackslash}url\{https://aka.ms/layoutreader\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,document,layout,nlp},
  annotation = {3 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv2108.11591 [cs]/2021/wang_2021_layoutreader_-_pre-training_of_text_and_layout_for_reading_order_detection.pdf}
}

@misc{wang_2022,
  title = {Pretraining {{Without Attention}}},
  author = {Wang, Junxiong and Yan, Jing Nathan and Gu, Albert and Rush, Alexander M.},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10544},
  eprint = {2212.10544},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.10544},
  urldate = {2022-12-21},
  abstract = {Transformers have been essential to pretraining success in NLP. Other architectures have been used, but require attention layers to match benchmark accuracy. This work explores pretraining without attention. We test recently developed routing layers based on state-space models (SSM) and model architectures based on multiplicative gating. Used together these modeling choices have a large impact on pretraining accuracy. Empirically the proposed Bidirectional Gated SSM (BiGS) replicates BERT pretraining results without attention and can be extended to long-form pretraining of 4096 tokens without approximation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/9EJTKXBB/Wang_et_al_(2022)_Pretraining_Without_Attention.pdf}
}

@article{wang_2022a,
  title = {{{mmLayout}}: {{Multi-grained MultiModal Transformer}} for {{Document Understanding}}},
  shorttitle = {{{mmLayout}}},
  author = {Wang, Wenjin and Huang, Zhengjie and Luo, Bin and Chen, Qianglong and Peng, Qiming and Pan, Yinxu and Yin, Weichong and Feng, Shi and Sun, Yu and Yu, Dianhai and Zhang, Yin},
  year = {2022},
  journal = {ACM Multimedia},
  doi = {10.1145/3503161.3548406},
  abstract = {Experimental results on four tasks, including information extraction and document question answering, show that the proposed method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters. Recent efforts of multimodal Transformers have improved Visually Rich Document Understanding (VrDU) tasks via incorporating visual and textual information. However, existing approaches mainly focus on fine-grained elements such as words and document image patches, making it hard for them to learn from coarse-grained elements, including natural lexical units like phrases and salient visual regions like prominent image regions. In this paper, we attach more importance to coarse-grained elements containing high-density information and consistent semantics, which are valuable for document understanding. At first, a document graph is proposed to model complex relationships among multi-grained multimodal elements, in which salient visual regions are detected by a cluster-based method. Then, a multi-grained multimodal Transformer called mmLayout is proposed to incorporate coarse-grained information into existing pre-trained fine-grained multimodal Transformers based on the graph. In mmLayout, coarse-grained information is aggregated from fine-grained, and then, after further processing, is fused back into fine-grained for final prediction. Furthermore, common sense enhancement is introduced to exploit the semantic information of natural lexical units. Experimental results on four tasks, including information extraction and document question answering, show that our method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters. Qualitative analyses show that our method can capture consistent semantics in coarse-grained elements.},
  annotation = {0 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000000},
  file = {/Users/chenghao/Zotero/storage/9NR7ITF2/Wang_et_al_(2022)_mmLayout.pdf}
}

@misc{wang_2024,
  title = {{{MambaByte}}: {{Token-free Selective State Space Model}}},
  shorttitle = {{{MambaByte}}},
  author = {Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M.},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.},
  howpublished = {https://arxiv.org/abs/2401.13660v1},
  langid = {english},
  annotation = {\{"size": 1535778, "pages": 22, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/8V4NAQSA/Wang et al. - 2024 - MambaByte Token-free Selective State Space Model.pdf}
}

@misc{wang_2024a,
  title = {Chain-of-{{Thought Reasoning Without Prompting}}},
  author = {Wang, Xuezhi and Zhou, Denny},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-02-18},
  abstract = {In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the {\textbackslash}textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' {\textbackslash}textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.},
  howpublished = {https://arxiv.org/abs/2402.10200v1},
  langid = {english}
}

@article{wasserstein_2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2023-06-19},
  annotation = {4291 citations (Semantic Scholar/DOI) [2023-06-19]},
  file = {/Users/chenghao/Zotero/storage/L67EHAC7/Wasserstein_Lazar_(2016)_The_ASA_Statement_on_p-Values.pdf}
}

@article{wei_2021,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.01652 [cs]},
  eprint = {2109.01652},
  primaryclass = {cs},
  urldate = {2021-09-06},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially boosts zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of tasks and model scale are key components to the success of instruction tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,nlp,nosource,zero-short,zero-shot},
  annotation = {66 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/84P926GC/Wei_et_al_(2021)_Finetuned_Language_Models_Are_Zero-Shot_Learners.pdf}
}

@misc{wei_2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = jun,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.07682},
  urldate = {2022-08-21},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {6 citations (Semantic Scholar/arXiv) [2022-08-20] 6 citations (Semantic Scholar/DOI) [2022-08-20]},
  file = {/Users/chenghao/Zotero/storage/22AMCFG4/Wei_et_al_(2022)_Emergent_Abilities_of_Large_Language_Models.pdf}
}

@misc{wei_2023,
  title = {Larger Language Models Do In-Context Learning Differently},
  author = {Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and Ma, Tengyu},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-01-20},
  abstract = {We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.},
  howpublished = {https://arxiv.org/abs/2303.03846v2},
  langid = {english},
  annotation = {\{"size": 964880, "pages": 51, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/2HNEC44H/Wei et al. - 2023 - Larger language models do in-context learning differently.pdf}
}

@misc{widder_2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Open ({{For Business}}): {{Big Tech}}, {{Concentrated Power}}, and the {{Political Economy}} of {{Open AI}}},
  shorttitle = {Open ({{For Business}})},
  author = {Widder, David Gray and West, Sarah and Whittaker, Meredith},
  year = {2023},
  month = aug,
  number = {4543807},
  address = {Rochester, NY},
  urldate = {2023-08-17},
  abstract = {This paper examines `open' AI in the context of recent attention to open and open source AI systems. We find that the terms `open' and `open source' are used in confusing and diverse ways, often constituting more aspiration or marketing than technical descriptor, and frequently blending concepts from both open source software and open science. This complicates an already complex landscape, in which there is currently no agreed on definition of `open' in the context of AI, and as such the term is being applied to widely divergent offerings with little reference to a stable descriptor. So, what exactly is `open' about `open' AI, and what does `open' AI enable? To better answer these questions we begin this paper by looking at the various resources required to create and deploy AI systems, alongside the components that comprise these systems. We do this with an eye to which of these can, or cannot, be made open to scrutiny, reuse, and extension. What does `open' mean in practice, and what are its limits in the context of AI? We find that while a handful of maximally open AI systems exist, which offer intentional and extensive transparency, reusability, and extensibility-- the resources needed to build AI from scratch, and to deploy large AI systems at scale, remain `closed'---available only to those with significant (almost always corporate) resources. From here, we zoom out and examine the history of open source, its cleave from free software in the mid 1990s, and the contested processes by which open source has been incorporated into, and instrumented by, large tech corporations. As a current day example of the overbroad and ill-defined use of the term by tech companies, we look at  `open' in the context of OpenAI the company. We trace its moves from a humanity-focused nonprofit to a for-profit partnered with Microsoft, and its shifting position on `open' AI. Finally, we examine the current discourse around `open' AI--looking at how the term and the (mis)understandings about what `open' enables are being deployed to shape the public's and policymakers' understanding about AI, its capabilities, and the power of the AI industry. In particular, we examine the arguments being made for and against `open' and open source AI, who's making them, and how they are being deployed in the debate over AI regulation. Taken together, we find that `open' AI can, in its more maximal instantiations, provide transparency, reusability, and extensibility that can enable third parties to deploy and build on top of powerful off-the-shelf AI models. These maximalist forms of `open' AI can also allow some forms of auditing and oversight. But even the most open of `open' AI systems do not, on their own, ensure democratic access to or meaningful competition in AI, nor does openness alone solve the problem of oversight and scrutiny. While we recognize that there is a vibrant community of earnest contributors building and contributing to `open' AI efforts in the name of expanding access and insight, we also find that marketing around openness and investment in (somewhat) open AI systems is being leveraged by powerful companies to bolster their positions in the face of growing interest in AI regulation. And that some companies have moved to embrace `open' AI as a mechanism to entrench dominance, using the rhetoric of `open' AI to expand market power while investing in `open' AI efforts in ways that allow them to set standards of development while benefiting from the free labor of open source contributors.},
  langid = {english},
  keywords = {AI,artificial intelligence,Big Tech,competition,data,open source,policy,political economy,privacy},
  file = {/Users/chenghao/Zotero/storage/EAJ8HB3A/Widder et al. - 2023 - Open (For Business) Big Tech, Concentrated Power,.pdf}
}

@misc{wolf_2023,
  title = {Fundamental {{Limitations}} of {{Alignment}} in {{Large Language Models}}},
  author = {Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11082},
  eprint = {2304.11082},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11082},
  urldate = {2023-04-25},
  abstract = {An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback increase the LLM's proneness to being prompted into the undesired behaviors. Moreover, we include the notion of personas in our BEB framework, and find that behaviors which are generally very unlikely to be exhibited by the model can be brought to the front by prompting the model to behave as specific persona. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {\{"size": 1188383, "pages": 22, "previous": "arXiv:2304.11082 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/G2RH5MVZ/Wolf et al. - 2023 - Fundamental Limitations of Alignment in Large Lang.pdf}
}

@misc{wortsman_2022,
  title = {Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  shorttitle = {Model Soups},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  year = {2022},
  month = jul,
  number = {arXiv:2203.05482},
  eprint = {2203.05482},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.05482},
  urldate = {2022-07-24},
  abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups." When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/8XGV2JEB/Wortsman_et_al_(2022)_Model_soups.pdf}
}

@misc{wu_2021,
  title = {{{LAMPRET}}: {{Layout-Aware Multimodal PreTraining}} for {{Document Understanding}}},
  shorttitle = {{{LAMPRET}}},
  author = {Wu, Te-Lin and Li, Cheng and Zhang, Mingyang and Chen, Tao and Hombaiah, Spurthi Amba and Bendersky, Michael},
  year = {2021},
  month = apr,
  number = {arXiv:2104.08405},
  eprint = {2104.08405},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08405},
  urldate = {2022-10-07},
  abstract = {Document layout comprises both structural and visual (eg. font-sizes) information that is vital but often ignored by machine learning models. The few existing models which do use layout information only consider textual contents, and overlook the existence of contents in other modalities such as images. Additionally, spatial interactions of presented contents in a layout were never really fully exploited. To bridge this gap, we parse a document into content blocks (eg. text, table, image) and propose a novel layout-aware multimodal hierarchical framework, LAMPreT, to model the blocks and the whole document. Our LAMPreT encodes each block with a multimodal transformer in the lower-level and aggregates the block-level representations and connections utilizing a specifically designed transformer at the higher-level. We design hierarchical pretraining objectives where the lower-level model is trained similarly to multimodal grounding models, and the higher-level model is trained with our proposed novel layout-aware objectives. We evaluate the proposed model on two layout-aware tasks -- text block filling and image suggestion and show the effectiveness of our proposed hierarchical architecture as well as pretraining techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  annotation = {GSCC: 0000000  9 citations (Semantic Scholar/arXiv) [2022-10-07]},
  file = {/Users/chenghao/Zotero/storage/SMU5I3G3/Wu_et_al_(2021)_LAMPRET.pdf}
}

@misc{wu_2023,
  title = {{{BloombergGPT}}: {{A Large Language Model}} for {{Finance}}},
  shorttitle = {{{BloombergGPT}}},
  author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.},
  howpublished = {https://arxiv.org/abs/2303.17564v3},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/5QWYG467/Wu et al. - 2023 - BloombergGPT A Large Language Model for Finance.pdf}
}

@misc{xie_2022,
  title = {Calibrating {{Trust}} of {{Multi-Hop Question Answering Systems}} with {{Decompositional Probes}}},
  author = {Xie, Kaige and Wiegreffe, Sarah and Riedl, Mark},
  year = {2022},
  month = oct,
  number = {arXiv:2204.07693},
  eprint = {2204.07693},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.07693},
  urldate = {2023-04-08},
  abstract = {Multi-hop Question Answering (QA) is a challenging task since it requires an accurate aggregation of information from multiple context paragraphs and a thorough understanding of the underlying reasoning chains. Recent work in multi-hop QA has shown that performance can be boosted by first decomposing the questions into simpler, single-hop questions. In this paper, we explore one additional utility of the multi-hop decomposition from the perspective of explainable NLP: to create explanation by probing a neural QA model with them. We hypothesize that in doing so, users will be better able to predict when the underlying QA system will give the correct answer. Through human participant studies, we verify that exposing the decomposition probes and answers to the probes to users can increase their ability to predict system performance on a question instance basis. We show that decomposition is an effective form of probing QA systems as well as a promising approach to explanation generation. In-depth analyses show the need for improvements in decomposition systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/XG8MMK6Q/Xie et al. - 2022 - Calibrating Trust of Multi-Hop Question Answering .pdf}
}

@misc{xie_2023,
  title = {{{DoReMi}}: {{Optimizing Data Mixtures Speeds Up Language Model Pretraining}}},
  shorttitle = {{{DoReMi}}},
  author = {Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V. and Ma, Tengyu and Yu, Adams Wei},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-05-20},
  abstract = {The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5\% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.},
  howpublished = {https://arxiv.org/abs/2305.10429v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/RMN9UIIK/Xie et al. - 2023 - DoReMi Optimizing Data Mixtures Speeds Up Languag.pdf}
}

@misc{xiong_2020,
  title = {On {{Layer Normalization}} in the {{Transformer Architecture}}},
  author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan},
  year = {2020},
  month = jun,
  number = {arXiv:2002.04745},
  eprint = {2002.04745},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.04745},
  urldate = {2022-07-31},
  abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {229 citations (Semantic Scholar/arXiv) [2022-07-31]},
  file = {/Users/chenghao/Zotero/storage/3G42ED8E/Xiong_et_al_(2020)_On_Layer_Normalization_in_the_Transformer_Architecture.pdf}
}

@misc{xiong_2022,
  title = {Simple {{Local Attentions Remain Competitive}} for {{Long-Context Tasks}}},
  author = {Xiong, Wenhan and O{\u g}uz, Barlas and Gupta, Anchit and Chen, Xilun and Liskovich, Diana and Levy, Omer and Yih, Wen-tau and Mehdad, Yashar},
  year = {2022},
  month = may,
  number = {arXiv:2112.07210},
  eprint = {2112.07210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.07210},
  urldate = {2022-07-31},
  abstract = {Many NLP tasks require processing long contexts beyond the length limit of pretrained models. In order to scale these models to longer text sequences, many efficient long-range attention variants have been proposed. Despite the abundance of research along this direction, it is still difficult to gauge the relative effectiveness of these models in practical use cases, e.g., if we apply these models following the pretrain-and-finetune paradigm. In this work, we aim to conduct a thorough analysis of these emerging models with large-scale and controlled experiments. For each attention variant, we pretrain large-size models using the same long-doc corpus and then finetune these models for real-world long-context tasks. Our findings reveal pitfalls of an existing widely-used long-range benchmark and show none of the tested efficient attentions can beat a simple local window attention under standard pretraining paradigms. Further analysis on local attention variants suggests that even the commonly used attention-window overlap is not necessary to achieve good downstream results -- using disjoint local attentions, we are able to build a simpler and more efficient long-doc QA model that matches the performance of Longformer{\textasciitilde}{\textbackslash}citep\{longformer\} with half of its pretraining compute. The code to replicate our experiments can be found at https://github.com/pytorch/fairseq/tree/main/examples/xformers},
  archiveprefix = {arxiv},
  keywords = {architecture,attention,Computer Science - Computation and Language,efficacy,transformers},
  file = {/Users/chenghao/Zotero/storage/XNJKJQ29/Xiong_et_al_(2022)_Simple_Local_Attentions_Remain_Competitive_for_Long-Context_Tasks.pdf}
}

@misc{xu_2021,
  title = {{{LayoutXLM}}: {{Multimodal Pre-training}} for {{Multilingual Visually-rich Document Understanding}}},
  shorttitle = {{{LayoutXLM}}},
  author = {Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Wei, Furu},
  year = {2021},
  month = sep,
  number = {arXiv:2104.08836},
  eprint = {2104.08836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08836},
  urldate = {2022-10-31},
  abstract = {Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also introduce a multilingual form understanding benchmark dataset named XFUND, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled for each language. Experiment results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset. The pre-trained LayoutXLM model and the XFUND dataset are publicly available at https://aka.ms/layoutxlm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/chenghao/Zotero/storage/H5VEIFM8/Xu_et_al_(2021)_LayoutXLM.pdf;/Users/chenghao/Zotero/storage/WA7CNF5Z/Xu_et_al_(2021)_LayoutXLM.pdf;/Users/chenghao/Zotero/storage/WMR73VUP/2104.html}
}

@article{xu_2022,
  title = {{{LayoutLMv2}}: {{Multi-modal Pre-training}} for {{Visually-Rich Document Understanding}}},
  shorttitle = {{{LayoutLMv2}}},
  author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
  year = {2022},
  month = jan,
  journal = {arXiv:2012.14740 [cs]},
  eprint = {2012.14740},
  primaryclass = {cs},
  urldate = {2022-02-07},
  abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 \${\textbackslash}to\$ 0.8420), CORD (0.9493 \${\textbackslash}to\$ 0.9601), SROIE (0.9524 \${\textbackslash}to\$ 0.9781), Kleister-NDA (0.8340 \${\textbackslash}to\$ 0.8520), RVL-CDIP (0.9443 \${\textbackslash}to\$ 0.9564), and DocVQA (0.7295 \${\textbackslash}to\$ 0.8672). We made our model and code publicly available at {\textbackslash}url\{https://aka.ms/layoutlmv2\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,document,layout,nlp,transformer},
  annotation = {GSCC: 0000109  56 citations (Semantic Scholar/arXiv) [2022-04-05]},
  file = {/Users/chenghao/Zotero/storage/9367ZB9J/Xu_et_al_(2022)_LayoutLMv2.pdf}
}

@misc{xu_2022a,
  title = {A {{Systematic Evaluation}} of {{Large Language Models}} of {{Code}}},
  author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J.},
  year = {2022},
  month = may,
  number = {arXiv:2202.13169},
  eprint = {2202.13169},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-18},
  abstract = {Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Programming Languages},
  annotation = {57 citations (Semantic Scholar/arXiv) [2023-03-18]},
  file = {/Users/chenghao/Zotero/storage/356QAF5S/Xu_et_al_(2022)_A_Systematic_Evaluation_of_Large_Language_Models_of_Code.pdf}
}

@inproceedings{xu_2022b,
  title = {{{ConReader}}: {{Exploring Implicit Relations}} in {{Contracts}} for {{Contract Clause Extraction}}},
  shorttitle = {{{ConReader}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Xu, Weiwen and Deng, Yang and Lei, Wenqiang and Zhao, Wenlong and Chua, Tat-Seng and Lam, Wai},
  year = {2022},
  month = dec,
  pages = {2581--2594},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-06-22},
  abstract = {We study automatic Contract Clause Extraction (CCE) by modeling implicit relations in legal contracts. Existing CCE methods mostly treat contracts as plain text, creating a substantial barrier to understanding contracts of high complexity. In this work, we first comprehensively analyze the complexity issues of contracts and distill out three implicit relations commonly found in contracts, namely, 1) Long-range Context Relation that captures the correlations of distant clauses; 2) Term-Definition Relation that captures the relation between important terms with their corresponding definitions, and 3) Similar Clause Relation that captures the similarities between clauses of the same type. Then we propose a novel framework ConReader to exploit the above three relations for better contract understanding and improving CCE. Experimental results show that ConReader makes the prediction more interpretable and achieves new state-of-the-art on two CCE tasks in both conventional and zero-shot settings.},
  annotation = {\{"size": 1453425, "pages": 14, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/AEL5PTVS/Xu et al. - 2022 - ConReader Exploring Implicit Relations in Contrac.pdf}
}

@misc{xu_2023,
  title = {Why Do {{Nearest Neighbor Language Models Work}}?},
  author = {Xu, Frank F. and Alon, Uri and Neubig, Graham},
  year = {2023},
  month = jan,
  number = {arXiv:2301.02828},
  eprint = {2301.02828},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.02828},
  urldate = {2023-01-11},
  abstract = {Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-01-10]},
  file = {/Users/chenghao/Zotero/storage/UR5QTJJ5/Xu_et_al_(2023)_Why_do_Nearest_Neighbor_Language_Models_Work.pdf}
}

@article{xue_2021,
  title = {{{ByT5}}: {{Towards}} a Token-Free Future with Pre-Trained Byte-to-Byte Models},
  author = {Xue, Linting and Barua, Aditya and Constant, Noah and {Al-Rfou}, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  year = {2021},
  journal = {arXiv},
  abstract = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
  keywords = {embedding,nosource,pre-trained,tokenization}
}

@misc{ye_2024,
  title = {{{SpacTor-T5}}: {{Pre-training T5 Models}} with {{Span Corruption}} and {{Replaced Token Detection}}},
  shorttitle = {{{SpacTor-T5}}},
  author = {Ye, Ke and Jiang, Heinrich and Rostamizadeh, Afshin and Chakrabarti, Ayan and DeSalvo, Giulia and Kagy, Jean-Fran{\c c}ois and Karydas, Lazaros and Citovsky, Gui and Kumar, Sanjiv},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {Pre-training large language models is known to be extremely resource intensive and often times inefficient, under-utilizing the information encapsulated in the training text sequences. In this paper, we present SpacTor, a new training procedure consisting of (1) a hybrid objective combining span corruption (SC) and token replacement detection (RTD), and (2) a two-stage curriculum that optimizes the hybrid objective over the initial \${\textbackslash}tau\$ iterations, then transitions to standard SC loss. We show empirically that the effectiveness of the hybrid objective is tied to the two-stage pre-training schedule, and provide extensive analysis on why this is the case. In our experiments with encoder-decoder architectures (T5) on a variety of NLP tasks, SpacTor-T5 yields the same downstream performance as standard SC pre-training, while enabling a 50\% reduction in pre-training iterations and 40\% reduction in total FLOPs. Alternatively, given the same amount of computing budget, we find that SpacTor results in significantly improved downstream benchmark performance.},
  howpublished = {https://arxiv.org/abs/2401.13160v1},
  langid = {english},
  annotation = {\{"size": 698028, "pages": 22, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/B3N6CHD8/Ye et al. - 2024 - SpacTor-T5 Pre-training T5 Models with Span Corruption and Replaced Token Detection.pdf}
}

@article{yin_2021,
  title = {{{DocNLI}}: {{A Large-scale Dataset}} for {{Document-level Natural Language Inference}}},
  author = {Yin, Wenpeng and Radev, Dragomir and Xiong, Caiming},
  year = {2021},
  journal = {arXiv},
  abstract = {Natural language inference (NLI) is formulated as a unified framework for solving various NLP problems such as relation extraction, question answering, summarization, etc. It has been studied intensively in the past few years thanks to the availability of large-scale labeled datasets. However, most existing studies focus on merely sentence-level inference, which limits the scope of NLI's application in downstream NLP problems. This work presents DocNLI -- a newly-constructed large-scale dataset for document-level NLI. DocNLI is transformed from a broad range of NLP problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, DocNLI has pretty limited artifacts which unfortunately widely exist in some popular sentence-level NLI datasets. Our experiments demonstrate that, even without fine-tuning, a model pretrained on DocNLI shows promising performance on popular sentence-level benchmarks, and generalizes well to out-of-domain NLP tasks that rely on inference at document granularity. Task-specific fine-tuning can bring further improvements. Data, code, and pretrained models can be found at https://github.com/salesforce/DocNLI.},
  file = {/Users/chenghao/Dropbox/Documents/Zotero/arXiv/2021/yin_2021_docnli_-_a_large-scale_dataset_for_document-level_natural_language_inference.pdf}
}

@misc{you_2023,
  title = {Ferret: {{Refer}} and {{Ground Anything Anywhere}} at {{Any Granularity}}},
  shorttitle = {Ferret},
  author = {You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07704},
  eprint = {2310.07704},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07704},
  urldate = {2023-12-23},
  abstract = {We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at https://github.com/apple/ml-ferret},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/chenghao/Zotero/storage/RYP9PV4X/You et al. - 2023 - Ferret Refer and Ground Anything Anywhere at Any Granularity.pdf}
}

@misc{yu_2023,
  title = {{{MEGABYTE}}: {{Predicting Million-byte Sequences}} with {{Multiscale Transformers}}},
  shorttitle = {{{MEGABYTE}}},
  author = {Yu, Lili and Simig, D{\'a}niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
  year = {2023},
  month = may,
  number = {arXiv:2305.07185},
  eprint = {2305.07185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07185},
  urldate = {2023-05-15},
  abstract = {Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/I68R5QSM/Yu et al. - 2023 - MEGABYTE Predicting Million-byte Sequences with M.pdf}
}

@misc{yu_2023a,
  title = {White-{{Box Transformers}} via {{Sparse Rate Reduction}}: {{Compression Is All There Is}}?},
  shorttitle = {White-{{Box Transformers}} via {{Sparse Rate Reduction}}},
  author = {Yu, Yaodong and Buchanan, Sam and Pai, Druv and Chu, Tianzhe and Wu, Ziyang and Tong, Shengbang and Bai, Hao and Zhai, Yuexiang and Haeffele, Benjamin D. and Ma, Yi},
  year = {2023},
  month = nov,
  number = {arXiv:2311.13110},
  eprint = {2311.13110},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.13110},
  urldate = {2023-11-24},
  abstract = {In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {\{"size": 44294368, "pages": 124, "previous": "arXiv:2311.13110 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/NTWD4LV6/Yu et al. - 2023 - White-Box Transformers via Sparse Rate Reduction Compression Is All There Is.pdf}
}

@misc{yuan_2024,
  title = {Self-{{Rewarding Language Models}}},
  author = {Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-21},
  abstract = {We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.},
  howpublished = {https://arxiv.org/abs/2401.10020v1},
  langid = {english},
  file = {/Users/chenghao/Zotero/storage/P3ZBWZMP/Yuan et al. - 2024 - Self-Rewarding Language Models.pdf}
}

@misc{zhang_2022,
  title = {On the {{Paradox}} of {{Learning}} to {{Reason}} from {{Data}}},
  author = {Zhang, Honghua and Li, Liunian Harold and Meng, Tao and Chang, Kai-Wei and den Broeck, Guy Van},
  year = {2022},
  month = may,
  number = {arXiv:2205.11502},
  eprint = {2205.11502},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.11502},
  urldate = {2022-05-27},
  abstract = {Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be trained end-to-end to solve logical reasoning problems presented in natural language? We attempt to answer this question in a confined problem space where there exists a set of parameters that perfectly simulates logical reasoning. We make observations that seem to contradict each other: BERT attains near-perfect accuracy on in-distribution test examples while failing to generalize to other data distributions over the exact same problem space. Our study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems. We also show that it is infeasible to jointly remove statistical features from data, illustrating the difficulty of learning to reason in general. Our result naturally extends to other neural models and unveils the fundamental difference between learning to reason and learning to achieve high performance on NLP benchmarks using statistical features.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-05-27]},
  file = {/Users/chenghao/Zotero/storage/NVX2ETEB/Zhang_et_al_(2022)_On_the_Paradox_of_Learning_to_Reason_from_Data.pdf}
}

@misc{zhang_2022a,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = may,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.01068},
  urldate = {2022-05-28},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,nlp,PLM},
  file = {/Users/chenghao/Zotero/storage/TBXAQMP4/Zhang_et_al_(2022)_OPT.pdf}
}

@misc{zhang_2023,
  title = {Cached {{Transformers}}: {{Improving Transformers}} with {{Differentiable Memory Cache}}},
  shorttitle = {Cached {{Transformers}}},
  author = {Zhang, Zhaoyang and Shao, Wenqi and Ge, Yixiao and Wang, Xiaogang and Gu, Jinwei and Luo, Ping},
  year = {2023},
  month = dec,
  number = {arXiv:2312.12742},
  eprint = {2312.12742},
  publisher = {arXiv},
  urldate = {2023-12-21},
  abstract = {This work introduces a new Transformer model called Cached Transformer, which uses Gated Recurrent Cached (GRC) attention to extend the self-attention mechanism with a differentiable memory cache of tokens. GRC attention enables attending to both past and current tokens, increasing the receptive field of attention and allowing for exploring long-range dependencies. By utilizing a recurrent gating unit to continuously update the cache, our model achieves significant advancements in {\textbackslash}textbf\{six\} language and vision tasks, including language modeling, machine translation, ListOPs, image classification, object detection, and instance segmentation. Furthermore, our approach surpasses previous memory-based techniques in tasks such as language modeling and displays the ability to be applied to a broader range of situations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {\{"size": 1332635, "pages": 12, "previous": "arXiv:2312.12742 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/KA75MZUA/Zhang_et_al_(2023)_Cached_Transformers.pdf}
}

@misc{zhang_2024,
  title = {Extending {{LLMs}}' {{Context Window}} with 100 {{Samples}}},
  author = {Zhang, Yikai and Li, Junlong and Liu, Pengfei},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-17},
  abstract = {Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF.},
  howpublished = {https://arxiv.org/abs/2401.07004v1},
  langid = {english},
  annotation = {\{"size": -1, "pages": -1, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/P8ECKNGV/Zhang_et_al_(2024)_Extending_LLMs'_Context_Window_with_100_Samples.pdf}
}

@misc{zhang_2024a,
  title = {{{MM-LLMs}}: {{Recent Advances}} in {{MultiModal Large Language Models}}},
  shorttitle = {{{MM-LLMs}}},
  author = {Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-01-25},
  abstract = {In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of \$26\$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.},
  howpublished = {https://arxiv.org/abs/2401.13601v1},
  langid = {english},
  annotation = {\{"size": 1973115, "pages": 22, "previous": ""\}},
  file = {/Users/chenghao/Zotero/storage/X24UMU36/Zhang et al. - 2024 - MM-LLMs Recent Advances in MultiModal Large Language Models.pdf}
}

@misc{zhao_2023,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = may,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  publisher = {arXiv},
  urldate = {2023-05-22},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {\{"size": 2395727, "pages": 58, "previous": "45 citations (Semantic Scholar/arXiv) [2023-05-22]{\textbackslash}narXiv:2303.18223 [cs]"\}},
  file = {/Users/chenghao/Zotero/storage/AHZMLRQN/Zhao_et_al_(2023)_A_Survey_of_Large_Language_Models.pdf}
}

@misc{zhao_2024,
  title = {{{GaLore}}: {{Memory-Efficient LLM Training}} by {{Gradient Low-Rank Projection}}},
  shorttitle = {{{GaLore}}},
  author = {Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03507},
  eprint = {2403.03507},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-09},
  abstract = {Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memoryefficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5\% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5\% and total training memory by 63.3\%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/chenghao/Zotero/storage/LHL74CNT/Zhao et al. - 2024 - GaLore Memory-Efficient LLM Training by Gradient Low-Rank Projection.pdf}
}

@article{zhu_2022,
  title = {Towards {{Complex Document Understanding By Discrete Reasoning}}},
  author = {Zhu, Fengbin and Lei, Wenqiang and Feng, Fuli and Wang, Chao and Zhang, Haozhou and Chua, Tat-Seng},
  year = {2022},
  journal = {ACM Multimedia},
  doi = {10.1145/3503161.3548422},
  abstract = {A novel model named MHST is developed that takes into account the information in multi-modalities to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. Document Visual Question Answering (VQA) aims to answer questions over visually-rich documents. In this work, we introduce a new Document VQA dataset, named TAT-DQA, which consists of 3,067 document pages comprising semi-structured table(s) and unstructured text as well as 16,558 question-answer pairs. The documents are sampled from financial reports and contain lots of numbers, which means discrete reasoning capability is demanded to answer the questions. Based on TAT-DQA, we further develop a novel model named MHST that takes into account the information in multi-modalities to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. The experiments show that MHST model significantly outperforms the baseline methods, demonstrating its effectiveness. However, the performance still lags far behind that of expert humans. We expect that our TAT-DQA dataset would facilitate the research on understanding of visually-rich documents, especially for scenarios that require discrete reasoning. Also, we hope the proposed model would inspire researchers to design more advanced Document VQA models in future.},
  annotation = {2 citations (Semantic Scholar/DOI) [2022-10-28] GSCC: 0000015},
  file = {/Users/chenghao/Zotero/storage/GYBDGDCT/Zhu_et_al_(2022)_Towards_Complex_Document_Understanding_By_Discrete_Reasoning.pdf}
}
