---
aliases:
  - OPT Open Pre-trained Transformer Language Models
tags:
  - natural_language_processing/pre-trained_language_models
  - natural_language_processing
title: "OPT Open Pre-trained Transformer Language Models"
---

# OPT Open Pre-trained Transformer Language Models

(6/5/2022, 11:17:40 AM)

“Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the collective research community as a whole, which is only possible when models are available for study.” ([Zhang et al., 2022, p. 1](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=1&annotation=IA2J2QS9))

“From this implementation, and from using the latest generation of NVIDIA hardware, we are able to develop OPT-175B using only 1/7th the carbon footprint of GPT-3. While this is a significant achievement, the energy cost of creating such a model is still nontrivial, and repeated efforts to replicate a model of this size will only amplify the growing compute footprint of these LLMs.” ([Zhang et al., 2022, p. 1](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=1&annotation=WWPU6DA5))

“We found the Pile was particularly full of duplicate documents, and advise future researchers using the Pile to perform additional de-duplication processing.” ([Zhang et al., 2022, p. 2](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=2&annotation=AD795MGF))

“Other subsets of the Pile were eliminated as we found they increased the risk of instabilities, as measured by tendency to cause spikes in gradient norms at the 1.3B scale, or were otherwise deemed unsuitable.” ([Zhang et al., 2022, p. 3](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=3&annotation=95WI6MAD))

“Flagged nodes were then cordoned off and training was resumed from the last saved checkpoint. Given the difference between the number of hosts cycled out and the number of manual restarts, we estimate 70+ automatic restarts due to hardware failures.” ([Zhang et al., 2022, p. 3](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=3&annotation=QYS85F8A))

“We speculate this occurs from two sources: (1) evaluating via the Davinci API may be bringing in safety control mechanisms beyond the original 175B GPT-3 model used in Brown et al. (2020); and (2) the significant presence of unmoderated social media discussions in the pre-training dataset has provided additional inductive bias to aid in such classification tasks.” ([Zhang et al., 2022, p. 6](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=6&annotation=4JPTUFK4))

“In particular, we found OPT-175B does not work well with declarative instructions or point-blank interrogatives.” ([Zhang et al., 2022, p. 8](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=8&annotation=W8UPKYVK))

“milar to other LLMs, OPT-175B can produce factually incorrect statements (Adiwardana et al., 2020; Brown et al., 2020; Roller et al., 2021; Rae et al., 2021; Chowdhery et al., 2022; Thoppilan et al., 2022). This can be particularly harmful in applications where information accuracy is critical, such as healthcare and scientific discovery (Weidinger et al., 2021b).” ([Zhang et al., 2022, p. 8](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=8&annotation=MSJJK65N))

“we believe more scrutiny should be afforded to the training data with additional data characterization and selection criteria in order to use data responsibly.” ([Zhang et al., 2022, p. 8](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=8&annotation=2UW9DGJP))

“While OPT-175B was developed with an estimated carbon emissions footprint (CO2eq) of 75 tons,10 GPT-3 was estimated to use 500 tons (Patterson et al., 2021), while Gopher required 380 tons (Rae et al., 2021).” ([Zhang et al., 2022, p. 9](zotero://select/library/items/22DIFQ9Y)) ([pdf](zotero://open-pdf/library/items/TTCAHXCN?page=9&annotation=MUNHXAIL))

***
- @zhang_2022a