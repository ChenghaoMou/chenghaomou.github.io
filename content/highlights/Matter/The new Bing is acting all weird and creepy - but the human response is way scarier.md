---
url: https://www.businessinsider.com/weird-bing-chatbot-google-chatgpt-alive-conscious-sentient-ethics-2023-2
author: Adam Rogers
publisher: Business Insider
date: 2023-02-22
tags: []
---

## Highlights
<mark>So something that can answer in a good facsimile of human language can beat the test without actually passing it. Once we start using language as a lone signifier of humanity, we're in a world of trouble. After all, lots of nonhuman things use some form of communication, many of which can be pretty sophisticated.</mark>

<mark>"Language activates emotional responses. Why, I don't know," says Carl Bergstrom, an evolutionary biologist at the University of Washington who is the author of a book on scientific bullshit. "One possibility is, it's always been a good heuristic that if something is using language on you, it's probably a person."</mark>

<mark>Sydney doesn't have an inner life, emotions, experience. When it's not chatting with a human, it isn't back in its quarters doing art and playing poker with other chatbots. Bergstrom has been an especially vocal critic of the tendency in the sciences and journalism to impute more personhood to chatbots than they deserve — which is, to be clear, zero. "You can quote this," he said of Roose's experience with Sydney. "Dude got catfished by a toaster."</mark>

<mark>Look, I don't think we don't need to treat chatbots with respect because they ask us to. We should treat them with respect because doing otherwise contributes to a culture of waste. It adds to the pervasive sense that it's permissible to make, consume, and throw away stuff without consequences to the planet. In the end, how we treat our devices — because that's what a chatbot is — says more about us than about them.</mark>

<mark>Every time a chatbot uses the first-person pronoun to refer to its output, it's the equivalent of sticking googly eyes on a toaster. It doesn't make the toaster smart, but we see personality in it, and that's part of a cynical business model. Search-engine companies are playing into our tendency to anthropomorphize in the hope that we'll not only use their chatbots, but come to trust them as a human-seeming source of expertise and assistance.</mark>

<mark>It's a way to figure out who gets sued when the robots screw something up, and what the copyright status is of the things they generate. "We're</mark>

<mark>We need to decide who's responsible for their actions, just as we do with any other consumer product, and hold them accountable. "Getting this right is crucial for us," Gunkel says. "Not for the robots. The robots don't care."</mark>

