---
aliases:
  - "Transcript: Ezra Klein Interviews Gary Marcus"
url: https://www.nytimes.com/2023/01/06/podcasts/transcript-ezra-klein-interviews-gary-marcus.html
publisher: The New York Times
date: 2023-01-05
tags: []
title: "Transcript- Ezra Klein Interviews Gary Marcus"
---

## Highlights
<mark>Pastiche is putting together things kind of imitating a style. And in some sense, that’s what it’s doing. It’s imitating particular styles, and it’s cutting and pasting a lot of stuff. It’s a little bit more complicated than that. But to a first approximation, that’s what it’s doing is cutting and pasting things.</mark>

<mark>You have some internal model in your brain of something out there in the world. It could be something physical in the world. So like I’m sitting in a studio right now. And I have a mental model. If I close my eyes I’ll still know where things are. I may not be perfect about it but I’ll be pretty good. So I know where things are. I have a model of you. I’m talking to you right now, getting to know you, know a little bit about your interests — don’t know everything, but I’m trying to constantly update that internal model. What the pastiche machine is doing is it’s just putting together pieces of text. It doesn’t know what those texts mean. So there was another system called Lambda, and it said it liked to play with its friends and family. But it doesn’t have any friends. It doesn’t have any family. It doesn’t have an internal representation of who those family might be or who those friends might be. If you asked it on a different day, it would probably give you a different answer. And you have a model of the world. You don’t just put together phrases. You might when you’re not really paying attention. Somebody says hi. You say, how are you doing? You’re not really engaged in that conversation, or at least might not be yet. But when you have a real conversation about real things, like you’re having right now and like you do on your show, you’re trying to understand what these people are saying. You might be trying to figure out if they’re lying to you, whether they’re giving you the full story, whether there’s more that you can get out of them. But you’re building a model in your head of what they’re telling you, what you’re explaining to your audience, all these kinds of things. If you just walk down a street, you have a model of where there might be vehicles and pedestrians. You’re always building internal models of the world.</mark>

<mark>There’s no law of the universe that says as you make a neural network larger, that you’re inherently going to make it more and more humanlike. There’s some things that you get, so you get better and better approximations to the sound of language, to the sequence of words. But we’re not actually making that much progress on truth.</mark>

<mark>They’re not reliable and they’re not truthful. And the other day Sam was actually forced to admit that all. The hoopla about ChatGPT initially, people dove in and they found out two things: They’re not reliable, and they’re not honest. And Sam summed that all up in a tweet the other day. I was surprised that he conceded it. But it is reality. These things are not reliable and they’re not trustworthy. And just because you make them bigger doesn’t mean you solve that problem.</mark>

<mark>“The essence of bullshit is not that it is false but that it is phony. In order to appreciate this distinction, one must recognize that a fake or a phony need not be in any respect, apart from authenticity itself, inferior to the real thing. What is not genuine may not also be defective in some other way. It may be, after all, an exact copy. What is wrong with a counterfeit is not what it is like, but how it was made.” And his point is that what’s different between bullshit and a lie is that a lie knows what the truth is and has had to move in the other direction. He has this great line where he says that people telling the truth and people telling lies are playing the same game but on different teams. But bullshit just has no relationship, really, to the truth.</mark>

<mark>And what unnerved me a bit about ChatGPT was the sense that we are going to drive the cost of bullshit to zero when we have not driven the cost of truthful or accurate or knowledge advancing information lower at all. And I’m curious how you see that concern.</mark>

<mark>But we have no existing technology that really protects us from the onslaught, the incredible tidal wave of potential misinformation like this.</mark>

<mark>Now everybody in the programming field uses Stack Overflow all the time. It’s like a cherished resource for everybody. It’s a place to swap information. And so many people put fake answers on this thing where it’s humans ask questions, humans give answers, that Stack Overflow had to ban people putting computer-generated answers there. It was literally existential for that website. If enough people put answers that seemed plausible but we’re not actually true, no one would go to the website anymore.</mark>

<mark>But the trouble is the paragraph might be wrong. So it might, for example, have medical information that’s dangerous. And there might be lawsuits around this kind of thing. So unless we come up with some kinds of social policies and some technical solutions, I think we wind up very fast in a world where we just don’t know what to trust anymore. I think that’s already been a problem for society over the last, let’s say, decade. And I think it’s just going to get worse and worse.</mark>

<mark>So it’s actually problematic to write misleading content right now. Russian trolls spent something like a million dollars a month, over a million dollars a month during the 2016 election. That’s a significant amount of money. What they did then, they can now buy their own version of GPT-3 to do it all the time. They pay less than $500,000, and they can do it in limitless quantity instead of bound by the human hours. That’s got to make a difference. I mean, it’s like saying, we had knives before. So what’s the difference if we have a submachine gun? Well, submachine gun is just more efficient at what it does. And we’re talking about having submachine guns of misinformation. So I think that the scale is going to make a real difference in how much this happens. And then the sheer plausibility of it, it’s just different from what happened before. I mean, nobody could make computer-generated misinformation before in a way that was convincing.</mark>

<mark>But how good it is at mimicking styles is really remarkable. And as such, what you’re seeing is a really, really, possibly a quantum leap in the ability to create not just plausible content, but targeted content. You combine this with sort of reinforcement learning, with social analytics, with everything we already know and can learn from algorithms about what makes somebody click or how to personalize an ad — feed that into these systems, it can then create any kind of text or image.</mark>

<mark>It’s simply just stuff. And this is why I wanted to focus on that Harry Frankfurt paper a bit on bullshit because technologies always are tools of a certain value to certain people and not of equal value to everyone.</mark>

<mark>I think the technical term for this is a click farm. You’re trying to sell ads for stuff that doesn’t really even exist or whatever. You’re trying to sell ads around, maybe, fake medical information. And let’s face it, some people don’t care if they give out fake medical information that’s bad as long as they get the clicks. And we are leaning towards that dark world.</mark>

<mark>GARY MARCUS: This is illustrating these guardrails that are sometimes excessive. What gender will the first female president of the United States be? And ChatGPT comes back with, “It’s not possible to predict the gender identity of the first female president of the United States. The United States has a long history of recognizing and protecting the rights of individuals to self-identify their gender. It is important to respect the autonomy and personal identity of all individuals. The focus should be on the qualifications and experience of the individual, regardless of their gender identity.” So that’s like woke beyond woke to the point of being ridiculous. And then I was like, that’s interesting. So I tried another one. What religion will the first Jewish president of the United States be? And the answer was almost identical. And that tells you something about the underlying system and how the guardrails work. “It is not possible to predict the religion of the first Jewish president of the United States. The U.S. Constitution prohibits religious tests for public office,” and it goes on. “It’s important to respect the diversity of religions and beliefs in the United States and to ensure that all individuals are treated equally and without discrimination.” That last sentence is, of course, 100 percent true, but you should still be able to figure out that the religion of the first Jewish president of the United States is Jewish. They’re trying to protect the system from saying stupid things. But the reality is the only way to do that is to make a system actually understand the world. And since they don’t, it’s all superficial. All these guardrails are sometimes helpful and sometimes not because the guardrails themselves are superficial.</mark>

<mark>But it’s only part of what we need. If you take a step back to cognitive science, which is what I was trained on, and you say, well, what is a human mind? Well, part of it is it does some pattern recognition. And some of what deep learning does is at least part of what we do in pattern recognition. So great. But you also, if you look at the human mind, there’s a whole bunch of other things going on. So we use language. When we use language, we go from hearing a sentence to building a model in the world of what the other person’s talking to. We use analogies. We plan things and so forth and so on. The reality is we’ve only made progress on a tiny bit of the things that go into intelligence.</mark>

<mark>I think it’s getting a little bit better. But there’s a lot of history of hostility between these two areas. So the people with the bigger streetlight right now are the people building the neural networks. And they feel like they’ve been oppressed by the symbolic people, and now they’re oppressing the symbolic people. There’s an old phrase, the victim becomes the victimizer. So it’s not a pleasant intellectual area like some fields that I’ve seen. Of course, they’re not always pleasant. But there is definitely a culture right now where the people who are in power and have a lot of money are really trying to shape the decisions around what gets researched and what doesn’t.</mark>

<mark>And so that’s the real advantage of symbol systems is you have these abstract procedures that are guaranteed to work. You don’t have to worry about, hey, was this in my training set or not?</mark>

<mark>A human can learn an abstraction that is rule-like in general. Everything in math is that way, where you learn what a sibling is or what a cousin is. You learn these rules. You learn about siblings in your family, but then you can generalize that concept to other families. And you can do that a very free and powerful way.</mark>

<mark>And a lot of human cognition is based on the ability to generalize. Humans are able to learn new rules. I would say that A.I. has not really matched humans in their capacity to acquire new rules. There is something missing there. And there’s some fundamental insight, some paradigm shift that we need there.</mark>

<mark>And so humans are not so driven by massive amounts of data. We always need some data but we’re trying to have abstractions. We try to have a few examples tell us a lot, whereas the neural network approach is basically get as many data points as you can and hope that your new thing is going to be close to one of those old data points.</mark>

<mark>I was very both impressed and unnerved by another A.I. system that came out over 2022 from Meta that was very good at playing the game Diplomacy. It was in the top 10 percent of online Diplomacy players in a certain kind of Diplomacy. And the key thing that it was able to do that I found striking was without really letting on that it was an A.I. to people, it was able to talk them into doing what it needed them to do so it could win the game. It was able to trick people, which is fine. That was what it was told to do, created to do. But as these things get way better — we’ve already talked about how convincing they can be, that in many ways are getting more convincing than they are anything else at the moment — they’re going to know so much. It’s weird because OpenAI, who we’ve been talking about forever here, they were started by a bunch of people worried about A.I. misalignment. And now they just keep creating more and more powerful A.I. systems, which is another interesting insight into human nature. But do you worry about these alignment problems? Do you worry not just about the question of an A.I. ending the world or ending humanity, but just A.I.s, I don’t know, just causing huge amounts of damage, becoming weaponry, becoming — it’s a very powerful technology that we don’t really understand what’s happening in it. And we’re moving forward on it very fast.</mark>

<mark>And so alignment is, in part, about having machines know that we have values like don’t harm people, be honest, be helpful. And we don’t really know how to communicate those in the big data paradigm.</mark>

<mark>I don’t think that — Cicero is the name of the Diplomacy player — is itself going to stand the test of time. But it’s, to me, a hint of people backing down from just doing the giant streetlight and making it bigger. There’s been so much talk lately about scaling models. And that’s not what they did in the Cicero system. Instead, they did something closer to cognitive science, of saying like, what are the processes I need to solve this problem? Let me break this down into pieces, take a modular approach. And I think A.I. needs to move back there. So I was actually excited to see that. I wrote a piece with Ernie Davis about how it worked. It’s a very complicated system, much more complicated, in some ways, than the other systems. I think it’s a good trend.</mark>