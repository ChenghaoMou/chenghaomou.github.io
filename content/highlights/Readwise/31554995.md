---
title: "Algorithms to Live By"
url: 
author: Brian Christian, Tom Griffiths
date: 2023-12-10
time: 11:21 AM
source: "kindle"
aliases:
  - Algorithms to Live By
---
## Highlights
> You set a predetermined amount of time for “looking”—that is, exploring your options, gathering data—in which you categorically don’t choose anyone, no matter how impressive. After that point, you enter the “leap” phase, prepared to instantly commit to anyone who outshines the best applicant you saw in the look phase. ([Location 215](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=215))

> As the applicant pool grows, the exact place to draw the line between looking and leaping settles to 37% of the pool, yielding the 37% Rule: look at the first 37% of the applicants,* choosing none, then be ready to leap for anyone better than all those you’ve seen so far. ([Location 231](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=231))

> It’s true that you’re unlikely to find the needle the majority of the time, but optimal stopping is your best defense against the haystack, no matter how large. ([Location 246](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=246))

> often. If you have, say, a 50/50 chance of being rejected, then the same kind of mathematical analysis that yielded the 37% Rule says you should start making offers after just a quarter of your search. ([Location 279](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=279))

> Namely, in the secretary problem we know nothing about the applicants other than how they compare to one another. We don’t have an objective or preexisting sense of what makes for a good or a bad applicant; moreover, when we compare two of them, we know which of the two is better, but not by how much. ([Location 301](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=301))

> Full information means that we don’t need to look before we leap. We can instead use the Threshold Rule, where we immediately accept an applicant if they are above a certain percentile. We don’t need to look at an initial group of candidates to set this threshold—but we do, however, need to be keenly aware of how much looking remains available. The math shows that when there are a lot of applicants left in the pool, you should pass up even a very good applicant in the hopes of finding someone still better than that—but as your options dwindle, you should be prepared to hire anyone who’s simply better than average. It’s a familiar, if not exactly inspiring, message: in the face of slim pickings, lower your standards. It also makes clear the converse: with more fish in the sea, raise them. In both cases, crucially, the math tells you exactly by how much. The easiest way to understand the numbers for this scenario is to start at the end and think backward. If you’re down to the last applicant, of course, you are necessarily forced to choose them. But when looking at the next-to-last applicant, the question becomes: are they above the 50th percentile? If yes, then hire them; if not, it’s worth rolling the dice on the last applicant instead, since their odds of being above the 50th percentile are 50/50 by definition. Likewise, you should choose the third-to-last applicant if they’re above the 69th percentile, the fourth-to-last applicant if they’re above the 78th, and so on, being more choosy the more applicants are left. No matter what, never hire someone who’s below average unless you’re totally out of options. (And since you’re still interested only in finding the very best person in the applicant pool, never hire someone who isn’t the best you’ve seen so far.) The chance of ending up with the single best applicant in this full-information version of the secretary problem comes to 58%—still far from a guarantee, but considerably ([Location 318](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=318))

> Any yardstick that provides full information on where an applicant stands relative to the population at large will change the solution from the Look-Then-Leap Rule to the Threshold Rule and will dramatically boost your chances of finding the single best applicant in the group. ([Location 341](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=341))

> What you’ve paid to keep searching is a sunk cost. Don’t compromise, don’t second-guess. And don’t look back. ([Location 394](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=394))

> But that impatience suggests another consideration that isn’t taken into account in the classical secretary problem: the role of time. ([Location 504](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=504))

> As optimal stopping researcher Neil Bearden puts it, “After searching for a while, we humans just tend to get bored. It’s not irrational to get bored, but it’s hard to model that rigorously.” ([Location 514](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=514))

> But the reality is not so simple. Remembering that every “best” song and restaurant among your favorites began humbly as something merely “new” to you is a reminder that there may be yet-unknown bests still out there—and thus that the new is indeed worthy of at least some of our attention. ([Location 543](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=543))

> People tend to treat decisions in isolation, to focus on finding each time the outcome with the highest expected value. But decisions are almost never isolated, and expected value isn’t the end of the story. ([Location 584](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=584))

> A sobering property of trying new things is that the value of exploration, of finding a new favorite, can only go down over time, as the remaining opportunities to savor it dwindle. ([Location 602](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=602))

> So explore when you will have time to use the resulting knowledge, exploit when you’re ready to cash in. The interval makes the strategy. ([Location 606](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=606))

> And indeed, win-stay turns out to be an element of the optimal strategy for balancing exploration and exploitation under a wide range of conditions. But lose-shift is another story. Changing arms each time one fails is a pretty rash move. Imagine going to a restaurant a hundred times, each time having a wonderful meal. Would one disappointment be enough to induce you to give up on it? Good options shouldn’t be penalized too strongly for being imperfect. ([Location 635](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=635))

> The Gittins index, then, provides a formal, rigorous justification for preferring the unknown, provided we have some opportunity to exploit the results of what we learn from exploring. ([Location 716](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=716))

> The old adage tells us that “the grass is always greener on the other side of the fence,” but the math tells us why: the unknown has a chance of being better, even if we actually expect it to be no different, or if it’s just as likely to be worse. ([Location 718](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=718))

> Exploration in itself has value, since trying new things increases our chances of finding the best. So taking the future into account, rather than focusing just on the present, drives us toward novelty. ([Location 720](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=720))

> “To try and fail is at least to learn; to fail to try is to suffer the inestimable loss of what might have been.” ([Location 744](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=744))

> In a multi-armed bandit problem, an Upper Confidence Bound algorithm says, quite simply, to pick the option for which the top of the confidence interval is highest. ([Location 776](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=776))

> Upper Confidence Bound algorithms implement a principle that has been dubbed “optimism in the face of uncertainty.” Optimism, they show, can be perfectly rational. By focusing on the best that an option could be, given the evidence obtained so far, these algorithms give a boost to possibilities we know less about. As a consequence, they naturally inject a dose of exploration into the decision-making process, leaping at new options with enthusiasm because any one of them could be the next big thing. ([Location 787](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=787))

> The success of Upper Confidence Bound algorithms offers a formal justification for the benefit of the doubt. Following the advice of these algorithms, you should be excited to meet new people and try new things—to assume the best about them, in the absence of evidence to the contrary. In the long run, optimism is the best prevention for regret. ([Location 792](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=792))

> But just as there’s a cost to not having a secretary, there’s a cost to committing too soon to a particular airline: the world might change. ([Location 954](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=954))

> when the world can change, continuing to explore can be the right choice. It might be worth going back to that disappointing restaurant you haven’t visited for a few years, just in case it’s under new management. ([Location 960](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=960))

> But pressing buttons at random, being very interested in new toys, and jumping quickly from one thing to another are all things that kids are really great at. And those are exactly what they should be doing if their goal is exploration. If you’re a baby, putting every object in the house into your mouth is like studiously pulling all the handles at the casino. ([Location 992](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=992))

> The basic pattern is clear: the size of people’s social networks (that is, the number of social relationships they engage in) almost invariably decreases over time. ([Location 1008](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1008))

> the shrinking of social networks with aging is due primarily to “pruning” peripheral relationships and focusing attention instead on a core of close friends and family members. This process seems to be a deliberate choice: as people approach the end of their lives, they want to focus more on the connections that are the most meaningful. ([Location 1015](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1015))

> The point is that these differences in social preference are not about age as such—they’re about where people perceive themselves to be on the interval relevant to their decision. ([Location 1024](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1024))

> The Gittins index and the Upper Confidence Bound, as we’ve seen, inflate the appeal of lesser-known options beyond what we actually expect, since pleasant surprises can pay off many times over. But at the same time, this means that exploration necessarily leads to being let down on most occasions. Shifting the bulk of one’s attention to one’s favorite things should increase quality of life. ([Location 1037](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1037))

> What makes Google so dominant as a means of accessing the world’s information is less that it finds our text within hundreds of millions of webpages—its 1990s competitors could generally do that part well enough—but that it sorts those webpages so well, and only shows us the most relevant ten. ([Location 1088](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1088))

> The truncated top of an immense, sorted list is in many ways the universal user interface. ([Location 1090](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1090))

> This is the first and most fundamental insight of sorting theory. Scale hurts. ([Location 1106](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1106))

> The key to actually breaking the linearithmic barrier is knowing the distribution from which the items you’re sorting are drawn. ([Location 1261](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1261))

> Instead, a good strategy—ratified by human and machine librarians alike—is to Bucket Sort until you get down to small enough piles that Insertion Sort is reasonable, or to have a Mergesort pizza party. ([Location 1283](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1283))

> Sorting something that you will never search is a complete waste; searching something you never sorted is merely inefficient. ([Location 1291](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1291))

> As the cost of searching drops, sorting becomes less valuable. ([Location 1308](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1308))

> All of the sorting algorithms that we’ve considered thus far assume perfect, flawless, foolproof comparisons, ones that never mess up and mistakenly judge the lesser of two quantities to be the greater. ([Location 1404](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1404))

> The winner of that particular honor is an algorithm called Comparison Counting Sort. In this algorithm, each item is compared to all the others, generating a tally of how many items it is bigger than. This number can then be used directly as the item’s rank. Since it compares all pairs, Comparison Counting Sort is a quadratic-time algorithm, like Bubble Sort. Thus it’s not a popular choice in traditional computer science applications, but it’s exceptionally fault-tolerant. ([Location 1419](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1419))

> Displacement happens when an animal uses its knowledge of the hierarchy to determine that a particular confrontation simply isn’t worth it. ([Location 1455](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1455))

> biologists tend to think of pecking orders as the violence that preempts violence. ([Location 1458](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1458))

> dominance hierarchies are ultimately information hierarchies. ([Location 1470](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1470))

> a race is fundamentally different from a fight. ([Location 1486](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1486))

> Having a benchmark—any benchmark—solves the computational problem of scaling up a sort. ([Location 1497](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1497))

> The nearest thing to clairvoyance is to assume that history repeats itself—backward. ([Location 1646](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1646))

> Caching is just as useful when it’s proximity, rather than performance, that’s the scarce resource. ([Location 1706](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1706))

> Having a cache is efficient, but having multiple levels of caches—from smallest and fastest to largest and slowest—can be even better. ([Location 1751](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1751))

> The key to a good human memory then becomes the same as the key to a good computer cache: predicting which items are most likely to be wanted in the future. ([Location 1834](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1834))

> In other words, reality itself has a statistical structure that mimics the Ebbinghaus curve. ([Location 1840](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1840))

> size alone is enough to impair speed: ([Location 1867](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1867))

> Through a series of simulations, the researchers showed that simply knowing more makes things harder when it comes to recognizing words, names, and even letters. No matter how good your organization scheme is, having to search through more things will inevitably take longer. ([Location 1890](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1890))

> you should begin by finding the single step that takes the least amount of time—the load that will wash or dry the quickest. If that shortest step involves the washer, plan to do that load first. If it involves the dryer, plan to do it last. Repeat this process for the remaining loads, working from the two ends of the schedule toward the middle. ([Location 1944](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1944))

> Thus we encounter the first lesson in single-machine scheduling literally before we even begin: make your goals explicit. ([Location 1962](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1962))

> before you can have a plan, you must first choose a metric. ([Location 1964](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=1964))

> Live by the metric, die by the metric. ([Location 2064](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=2064))

> This starts with making sure that the single-machine problem we’re solving is the one we want to be solving. ([Location 2066](https://readwise.io/to_kindle?action=open&asin=B015CKNWJI&location=2066))

