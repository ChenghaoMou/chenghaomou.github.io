---
title: "ðŸ§‘â€ðŸ« Instruction Tuning Vol. 1"
url: https://nlpnewsletter.substack.com/p/instruction-tuning-vol-1
author: Sebastian Ruder
date: 2023-12-10
time: 11:22 AM
source: "reader"
aliases:
  - ðŸ§‘â€ðŸ« Instruction Tuning Vol. 1
---
## Highlights
> **Mixing few-shot settings.** Training with mixed zero-shot and few-shot prompts significantly improves performance in both settings. ([View Highlight](https://read.readwise.io/read/01heq0trjxt91cxmw5e987wawq))

> **Task diversity.** Large models benefit from continuously increasing the number of tasks. ([View Highlight](https://read.readwise.io/read/01heq0txs09hp4g4y9rxj8342p))

---
title: "ðŸ§‘â€ðŸ« Instruction Tuning Vol. 1"
url: https://nlpnewsletter.substack.com/p/instruction-tuning-vol-1
author: Sebastian Ruder
date: 2024-02-03
time: 8:12 PM
source: "reader"
aliases:
  - "ðŸ§‘â€ðŸ« Instruction Tuning Vol. 1"
---
# ðŸ§‘â€ðŸ« Instruction Tuning Vol. 1

## Highlights
> **Mixing few-shot settings.** Training with mixed zero-shot and few-shot prompts significantly improves performance in both settings. ([View Highlight](https://read.readwise.io/read/01heq0trjxt91cxmw5e987wawq))

> **Task diversity.** Large models benefit from continuously increasing the number of tasks. ([View Highlight](https://read.readwise.io/read/01heq0txs09hp4g4y9rxj8342p))

