---
title: "34840312"
url: https://www.aisnakeoil.com/p/model-alignment-protects-against
author: Arvind Narayanan
date: 2023-12-10
time: 11:22 AM
source: "reader"
aliases:
  - Model Alignment Protects Against Accidental Harms, Not Intentional Ones
---
## Highlights
> Model alignment, especially RLHF, is hard to get right, and there have been aligned chatbots that were [nonetheless](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html) [harmful](https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says). And alignment doesn’t matter if the product concept is itself [creepy](https://mashable.com/article/meta-ai-dystopian-chatbot-kendall-jenner-persona). Finally, for combatting more serious kinds of accidental harms, such as those that might arise from autonomous agents, a narrowly technical approach is probably [not enough](https://braiduk.org/a-shrinking-path-to-safety-how-a-narrowly-technical-approach-to-align-ai-with-the-public-good-could-fail). ([View Highlight](https://read.readwise.io/read/01hgkpv7wd455652c0he4mvmgk))

> This means that we must prepare for a world in which unaligned models exist — either because threat actors trained them from scratch or because they modified an existing model. We must instead look to [defend the attack surfaces](https://www.aisnakeoil.com/p/three-ideas-for-regulating-generative) that attackers might target using unaligned models, such as social media (in the case of disinformation) or software codebases (in the case of the use of [LLMs to find security vulnerabilities](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html)). ([View Highlight](https://read.readwise.io/read/01hgkpvxdzw0mq09fjfbacj0h7))

> RLHF and other model alignment techniques help make generative AI products safer and nicer to use. But we shouldn’t be surprised or alarmed that they are imperfect. They remain useful despite their weaknesses. And when it comes to catastrophic AI risks, it’s best not to put any stock in model alignment until and unless there are fundamental breakthroughs that lead to new alignment techniques. ([View Highlight](https://read.readwise.io/read/01hgkq63b4vg9f3htv6868agct))