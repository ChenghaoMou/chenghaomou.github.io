---
title: "35084810"
url: https://www.schneier.com/blog/archives/2023/12/ai-and-trust.html
author: CTOSian
date: 2023-12-10
time: 11:22 AM
source: "reader"
aliases:
  - AI and Trust
---
## Highlights
> But they are not our friends. Corporations are not capable of having that kind of relationship. ([View Highlight](https://read.readwise.io/read/01hgxrqpnm0r22swnmx35wwnya))

> Chiang’s point is that this is every corporation’s business plan. And that our fears of AI are basically fears of capitalism. Science fiction writer Charlie Stross takes this one step further, and calls corporations “[slow AI](https://www.antipope.org/charlie/blog-static/2018/01/dude-you-broke-the-future.html).” They are profit maximizing machines. And the most successful ones do whatever they can to achieve that singular goal. ([View Highlight](https://read.readwise.io/read/01hgxrs9tsxxd0h6pbdcy8zrdp))

> We are primed to think of others who speak our language as people. And we sometimes have trouble thinking of others who speak a different language that way. We make that category error with obvious non-people, like cartoon characters. We will naturally have a “theory of mind” about any AI we talk with. ([View Highlight](https://read.readwise.io/read/01hgxryw6n4h7b383b07r1gdc6))

> It’s no accident that these corporate AIs have a human-like interface. There’s nothing inevitable about that. It’s a design choice. It could be designed to be less personal, less human-like, more obviously a service—like a search engine . The companies behind those AIs want you to make the friend/service category error. It will exploit your mistaking it for a friend. And you might not have any choice but to use it. ([View Highlight](https://read.readwise.io/read/01hgxs2119wscyr4fth52nyqq3))

> So far, we have been talking about one particular failure that results from overly trusting AI. We can call it something like “hidden exploitation.” There are others. There’s outright fraud, where the AI is actually trying to steal stuff from you. There’s the more prosaic mistaken expertise, where you think the AI is more knowledgeable than it is because it acts confidently. There’s incompetency, where you believe that the AI can do something it can’t. There’s inconsistency, where you mistakenly expect the AI to be able to repeat its behaviors. And there’s illegality, where you mistakenly trust the AI to obey the law. There are probably more ways trusting an AI can fail. ([View Highlight](https://read.readwise.io/read/01hgxs49b036kyvat84tvmsb2y))

> AIs are not people; they don’t have agency. They are built by, trained by, and controlled by people. Mostly for-profit corporations. Any AI regulations should place restrictions on those people and corporations. Otherwise the regulations are making the same category error I’ve been talking about. At the end of the day, there is always a human responsible for whatever the AI’s behavior is. And it’s the human who needs to be responsible for what they do—and what their companies do. Regardless of whether it was due to humans, or AI, or a combination of both. Maybe that won’t be true forever, but it will be true in the near future. If we want trustworthy AI, we need to require trustworthy AI controllers. ([View Highlight](https://read.readwise.io/read/01hgxs7peyaekj58wgp9fm6hjr))

^6cc420