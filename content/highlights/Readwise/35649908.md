---
title: "35649908"
url: https://openai.com/safety/preparedness
author: openai.com
date: 2023-12-21
time: 12:39 AM
source: "reader"
aliases:
  - Preparedness
---
## Highlights
> It describes OpenAI’s processes to track, evaluate, forecast, and protect against catastrophic risks posed by increasingly powerful models. ([View Highlight](https://read.readwise.io/read/01hj4vzjapbf94tz4jegvrsbg3))
No questions asked about the data labor and rights.

> We have several safety and policy teams working together to mitigate risks from AI ([View Highlight](https://read.readwise.io/read/01hj641j5n6ee2j55vskn8kmz9))
Assuming the current AI is inevitable and the harms done from creating the AI can be safely ignored.

> We also want to look beyond what’s happening today to anticipate what’s ahead. ([View Highlight](https://read.readwise.io/read/01hj671enkvjxwe3qtqnda1sg9))
People rarely look back or share their retrospects. Look beyond or look away from?

> We learn from real-world deployment and use the lessons to mitigate emerging risks. For safety work to keep pace with the innovation ahead, we cannot simply do less, we need to continue learning through iterative deployment. ([View Highlight](https://read.readwise.io/read/01hj673wvyt6ar5gjc22s08jyr))
At what cost?

> We have defined thresholds for risk levels along the following initial tracked categories - cybersecurity, CBRN (chemical, biological, radiological, nuclear threats), persuasion, and model autonomy. ([View Highlight](https://read.readwise.io/read/01hj6768fn2r3j6v91ppvj8mrq))
Anything else is marginalised — lazy self-regulatory promises.

> We will continue having others red-team and evaluate our models, and we plan to share updates externally. ([View Highlight](https://read.readwise.io/read/01hj67arkxg7atpqs0njt8sfct))
I think the root cause of those risks is the question they can't bring themselves to ask: do you need such a model in the first place? If the assumption is always yes, then this techno-solutionism x techno-optimism narrative is nothing more than an opinion piece and PR stunt.

> We will help reduce other known and unknown safety risks. ([View Highlight](https://read.readwise.io/read/01hj67ff83pfhkjgsrpzyqffkx))
Risk in the sense that it is anticipated and not materialised, not the actual harms that have been done or are being done in the name of learning from real-world deployment.