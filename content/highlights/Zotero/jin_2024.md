---
aliases:
  - "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"
authors: Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu
date: 2024-01-13 14:29:35
tags: []
title: "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"
url: https://arxiv.org/abs/2401.01325v1
---

# LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning

## Abstract
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.

> To address this, an intuitive and practical solution would be to remap the unseen relative positions to those encountered during the pretraining, thus extending the LLMs’ ability to handle longer contexts naturally. [(p. 2)](zotero://open-pdf/library/items/RJQECA6J?page=2)

> Since natural language texts tend to have similar semantics within a short range (e.g. a paragraph), close or even equal position encodings should be adequate for maintaining the relative ordering of useful information. This aligns with the floor operation. 2) In natural language texts, most of the time, while a small bag of words (ngrams) appears together in one area, all the tokens in that bag have only one possible order due to the conventions of the language grammar. Although theoretically, a bag of tokens could appear in any order, in practice it is rare for a small set of words to have more than one sensible ordering. For example, ”unnecessary encodings” can be tokenized as ”unn”, ”ecessary”, ” enc” and ”odings”2, but these tokens can only meaningfully appear in that order. [(p. 2)](zotero://open-pdf/library/items/RJQECA6J?page=2)

![[statics/jin_2024/image-3-x302-y464.png]]

> To conclude, we still need to keep the attention mechanism unchanged in the neighbor area, which would be the normal attention used in the pretraining stage. [(p. 4)](zotero://open-pdf/library/items/RJQECA6J?page=4)

![[statics/jin_2024/image-5-x45-y506.png]] ^8555ea

> The results also demonstrated that: although Mistral w/ SWA has low PPL beyond its pretraining context window, it can only access information (i.e. the passkey) within its sliding window. Considering the simplicity of this task, this result strongly suggests it still does not have true ability to handle long contexts. [(p. 6)](zotero://open-pdf/library/items/RJQECA6J?page=6)

> Limitation: The limitation of the proposed Self-Extend in- cludes the lack of implementation of Flash Attention (Dao et al., 2022) and the performance degradation with too large group size, which means the context window still cannot be extended to infinity with current SelfExtend. Meanwhile, like many regular tasks, there is still no consensus at present about how to do evaluation for long context tasks, which may cause problematic evaluation results. [(p. 9)](zotero://open-pdf/library/items/RJQECA6J?page=9)

```
@misc{Jin_Han_Yang_Jiang_Liu_Chang_Chen_Hu_2024, title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning}, url={[https://arxiv.org/abs/2401.01325v1](https://arxiv.org/abs/2401.01325v1)}, abstractNote={This work elicits LLMs’ inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs’ context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs’ long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model’s self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs’ context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs’ context window’s length.}, note={{"size": -1, “pages”: -1, “previous”: " "}}, journal={arXiv.org}, author={Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia}, year={2024}, month=jan, language={en} }
```