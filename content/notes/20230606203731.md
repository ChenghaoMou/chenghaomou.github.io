---
aliases:
  - Memory Efficient Zeroth-order Optimization
tags: []
title: "Memory Efficient Zeroth-order Optimization"
---

# Memory Efficient Zeroth-order Optimization

Source: [[malladi_2023|Fine-Tuning Language Models with Just Forward Passes]]

Optimizers usually rely on backpropagation using chain rules and differentiation rules to derive gradients. This also requires extra space for the gradients during the backward pass. However, zeroth-order optimization can be used to derive such gradients with just two forward passes.

This is possible because, by definition, gradients can be calculated from the difference in the output by perturbing the parameters at a small scale. By using two forward passes, it enables the model to be optimized with non-differentiable objectives and achieve comparable performance. And MeZO improves memory consumption with in-place parameter updates.