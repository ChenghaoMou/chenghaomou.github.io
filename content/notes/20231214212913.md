---
aliases:
  - Hallucination is Here to Stay
title: "Hallucination is Here to Stay"
---

# Hallucination is Here to Stay

On the surface, the word itself is too anthropomorphic — ascribing too much psychological affordance to a neural network. It also euphemises the underlying unreliable synthesis — essentially [[20230115161911|Bullshit]] — as if it is something we can get rid of easily with a panacea.

Unfortunately, it seems that the name and the underlying phenomena are here to stay[^1] [^2].

All model generations are unreliable, but some are more useful than others. People also said that hallucination, to the model itself,  is the feature, not the bug, but a bug indeed at the system/assistant level, and they deem it fixable[^3]. Others share little of such optimism[^4] [^5] [^6].

But it also has a good side — it reminds you to fact-check the output if you care enough, and it might be a good source of brainstorming, too[^7].

[^1]: [[20240218005541|Sora Can’t Handle the Truth - by Gary Marcus - Marcus on AI]]
[^2]: [[20240216200449|Sora’s Surreal Physics - by Gary Marcus - Marcus on AI]]
[^3]: [[35423015|# On the "hallucination problem"]]
[^4]: [[32882444|Muddles About Models]]
[^5]: [[ChatGPT Is a Blurry JPEG of the Web|ChatGPT Is a Blurry JPEG of the Web]]
[^6]: [[“If It Sounds Like Sci-Fi, It Probably Is”|“If It Sounds Like Sci-Fi, It Probably Is”]]
[^7]: [[36269821|In Defense of AI Hallucinations]]