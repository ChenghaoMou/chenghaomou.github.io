---
aliases:
  - UNICORN on RAINBOW A Universal Commonsense Reasoning Model on a New Multitask Benchmark
tags: [natural_language_processing/commonsense, natural_language_processing, processed]
title: UNICORN on RAINBOW A Universal Commonsense Reasoning Model on a New Multitask Benchmark
---

# UNICORN on RAINBOW A Universal Commonsense Reasoning Model on a New Multitask Benchmark

(6/5/2022, 11:15:18 AM)

“a novel evaluation, the cost equivalent curve, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency.” ([Lourie et al., 2021, p. 1](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=1&annotation=5X2TPSHI))

“we define cost as the number of training examples in the target dataset.” ([Lourie et al., 2021, p. 2](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=2&annotation=MFGVITR3))

“The construction of cost equivalent curves makes one technical assumption: the relationship between performance and cost is continuous and strictly monotonic (i.e., increasing or decreasing)” ([Lourie et al., 2021, p. 2](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=2&annotation=8FRGUVZ6))

“multitask training (Caruana 1995): training on multiple datasets (including the target dataset) all at once,” ([Lourie et al., 2021, p. 3](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=3&annotation=UX5SXD5X))

“multitask fine-tuning (Liu et al. 2019a): first training on all datasets (including the target dataset) through multitask training, and then continuing to fine-tune on the target dataset alone.” ([Lourie et al., 2021, p. 3](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=3&annotation=2YEFB248))

“Finding 1: Sequential training almost always matches or beats other approaches.” ([Lourie et al., 2021, p. 3](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=3&annotation=EUP7MKYZ))

“Finding 2: Sequential training rarely hurts performance.” ([Lourie et al., 2021, p. 4](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=4&annotation=IUVB3RJB))

“Multitask training helps most often in the lowdata regime.” ([Lourie et al., 2021, p. 4](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=4&annotation=HLPFX27H))

“multitask learning tends to help when data is scarce, but may hurt performance if data is plentiful.” ([Lourie et al., 2021, p. 4](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=4&annotation=V87KY5B9))

“The off-the-shelf T5’s weights come from multitask pretraining, where many tasks are mixed with a language modeling objective to learn a powerful initialization for the weights. In fact, both GLUE and SUPERGLUE were mixed into the pretraining (Raffel et al. 2019).” ([Lourie et al., 2021, p. 5](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=5&annotation=4RIZLCKC))

“Larger models benefit more from transfer” ([Lourie et al., 2021, p. 5](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=5&annotation=TGLKP3UP))

“the serialized language from the knowledge graphs is not in a QA format, and the knowledge graph completion task is generative while all other tasks are discriminative” ([Lourie et al., 2021, p. 6](zotero://select/library/items/EQG5X6PS)) ([pdf](zotero://open-pdf/library/items/D8R6WH9Y?page=6&annotation=BW9W9U9F)) Knowledge augmented lm

***
References:
1. @lourie_2021a