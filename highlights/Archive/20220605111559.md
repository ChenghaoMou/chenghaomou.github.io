---
aliases:
  - UniDoc Unified Pretraining Framework for Document Understanding
linter-yaml-title-alias: UniDoc Unified Pretraining Framework for Document Understanding
order: -20220605111559
tags: [natural_language_processing, natural_language_processing/document_understanding, natural_language_processing/pre-trained_language_models]
title: UniDoc Unified Pretraining Framework for Document Understanding
---

# UniDoc Unified Pretraining Framework for Document Understanding

(6/5/2022, 11:15:59 AM)

“UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image.” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=FNEX9F4H))

“UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image.” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=PXDBPDJZ))

“An important feature of UDoc is that it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities.” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=7ESCVDZZ))

“it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=SEMMQBKW))

([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) No open source code available.

“(1) documents are composed of semantic regions.” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=TJUM4U4T)) The semantic region is left undefined. The difference between OCR block and semantic region is also unexplanied.

“semantic regions” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=B6QXNMHZ))

“However, unlike the sequence-to-sequence learning in NLP, documents have a hierarchical structure (words form sentences, sentences form a semantic region, and semantic regions form a document).” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=JAY9KED3))

“documents have a hierarchical structure (words form sentences, sentences form a semantic region, and semantic regions form a document).” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=TUSP6D4Z))

“Moreover, current transformer-based document pretraining models suffer from input length constraints.” ([Gu et al., 2021, p. 1](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=1&annotation=IUJVVGIV))

“(2) documents are more than words. The semantic structure of the document is not only determined by the text within it but also the visual features such as table, font size and style, and figure, etc.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=NYRN37CJ))

“(3) documents have spatial layout. Visual and layout information is critical for document understanding.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=MX54QLPT))

“However, for semi-structured documents, such as forms and receipts, words are more related to their local surroundings. This corresponds strongly with human intuition when we look at magazines or newspapers, the receptive fields are modulated by our reading order and attention.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=AUJ8PVLH))

“To handle textual information, we encode sentences using a hierarchical transformer encoder. The first level of the hierarchical encoder models the formation of the sentences from words. The second level models the formation of the document from sentences.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=TCR8W36N))

“To handle textual information, we encode sentences using a hierarchical transformer encoder.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=UTXBDQX5))

“The first level of the hierarchical encoder models the formation of the sentences from words. The second level models the formation of the document from sentences.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=J5LV6U73))

“Meanwhile, it reduces model computation complexity exponentially and increases the number of input words.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=WXLAERNG)) 512 \* 512 = 262144 tokens for each document

“Meanwhile, it reduces model computation complexity exponentially and increases the number of input words.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=L9SV3D7V))

“we combine convolution with self-attention to form a mixed attention mechanism that combines the advantages of the two operations.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=FEV6A5IP))

“A visually-rich region (figure, chart, etc) may have stronger visual information than textual information. Instead of treating outputs from both modalities identically, we design a gating mechanism that can dynamically control the influence of textual and visual features. This approach enables cross-modal connections and allows for variable highlight the relevant information in visual and textual modality and enables cross-modal connections.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=QQYATLPI))

“Instead of treating outputs from both modalities identically, we design a gating mechanism that can dynamically control the influence of textual and visual features.” ([Gu et al., 2021, p. 2](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=2&annotation=QXCWZCV6))

“UDoc, which consists of four components: feature extraction, feature embedding, multi-layer gated cross-attention encoder, and pretraining tasks.” ([Gu et al., 2021, p. 3](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=3&annotation=QBZF74VW))

“In the feature extraction step, we first employ an off-the-shelf OCR tool \[17\] to extract text from a document image I, where the words are grouped into sentences S = {s1,...,sN } whose corresponding bounding boxes are P = {p1,...,pN }. For each sentence bounding box pi, we use a ConvNet-based backbone fImEnc and RoI Align \[18\] f” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=CEXI2Z76)) Sentencization in a document can be problamatic especially when the sections and headers do not follow the grammar rule of a sentence. OCR tools in my experience rarely share the same understanding of the textual information in the documents.

“For each sentence bounding box pi, we use a ConvNet-based backbone fImEnc and RoI Align \[18\] fRoIAlign to extract the pooled RoI features vi.” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=TRMPJ27B))

“To obtain a feature embedding, we extract the sentence embedding si for each sentence si via a pretrained sentence encoder fSentEnc.” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=Z8M7YV7L))

“Each region’s RoI feature vi is discretized into a finite set of visual representations vQ i 2 VQ via product quantization \[19\].” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=VCU557SK))

“Formally, a document image I 2 RW ⇥H consists of N regions, where each region’s bounding box is characterized by a 6-d vector, as pi = { xLT W , yLT H , xRB W , yRB H,w W, h H }, where w and h are of the width and height the region, W and H are the width and height of I, wh” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=2LUBWWP5)) I think they refer each sentence bounding box as region, but what if the sentence stretches two lines, or what about tables, TOC, lists? A rigorously definition of region should help here.

“We also have different types of segments to distinguish different modalities. The input sequence to the transformer-based encoder starts with a special start element (\[CLS\] and full visual features), then it is followed by multimodal elements, and it ends with a special ending element (\[SEP\]+full visual features).” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=6UZDVVU7)) visual features + \[SEP\] or \[SEP\] + visual features?

“Unlike the fixed image encoder in \[6\], we jointly learn the image encoder in an end-to-end fashion alongside the multimodal model.” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=6EVWJDUY))

“To constrain the representation space of the visual features and facilitate the end-to-end learning of image encoder (see Task #2 in Sec. 3.2), we follow \[20, 21\] and use vector quantization to discretize the visual features V = {v1, . . . , vN } into a finite set of representations VQ = {vQ 1 , . . . , vQ N }.” ([Gu et al., 2021, p. 4](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=4&annotation=YW4X9SRC))

“The goal is to predict the masked sentence embeddings based on the contextual information from the surrounding sentences and image regions, by minimizing the smooth L1 loss \[16\]:” ([Gu et al., 2021, p. 5](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=5&annotation=4H3QMC9T))

“The goal is to predict the masked sentence embeddings based on the contextual information from the surrounding sentences and image regions” ([Gu et al., 2021, p. 5](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=5&annotation=UZVY3DBC))

“random masking” ([Gu et al., 2021, p. 5](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=5&annotation=8WTQ5CY7))

“The goal is to minimize the differences” ([Gu et al., 2021, p. 5](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=5&annotation=HIU8QXM9))

“between the pairwise similarities of sentence embeddings and the pairwise similarities of image region features:” ([Gu et al., 2021, p. 6](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=6&annotation=E3EXHSX8))

“between the pairwise similarities of sentence embeddings and the pairwise similarities of image region features:” ([Gu et al., 2021, p. 6](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=6&annotation=I2UDHMG8))

“The paragraph mode groups the non-paragraph results into text regions.” ([Gu et al., 2021, p. 6](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=6&annotation=YIFYWNHU)) This is different from what is previously defined.

“Hence, we adopt the paragraph-level outputs as the basic input elements since textual regions provide semantically more meaningful information than independent words.” ([Gu et al., 2021, p. 6](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=6&annotation=46T9BM8I))

“We set the hidden size to 768 and the number of heads to 12, the maximum number of regions N to 64, and the maximum input sequence length for fSentEnc to 512.” ([Gu et al., 2021, p. 6](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=6&annotation=679UY787))

“80% among the masked sentences are replaced by special sentence \[CLS, MASK, SEP\]” ([Gu et al., 2021, p. 7](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=7&annotation=4SY2D7MF))

“We find that OCR plays a key role in document classification performance.” ([Gu et al., 2021, p. 9](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=9&annotation=FHFF5XFW))

“(2) Although impressive performance has been achieved in document entity recognition tasks such as form and receipt understanding, the classification accuracy on semi-structured documents such as forms is still inferior to that of rich-text documents. It is possible to devise a better method to model the spatial relationship among words.” ([Gu et al., 2021, p. 10](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=10&annotation=3AAKR9K2))

“Lastly, the use of different OCR tools is one of the major sources of inconsistency among the existing document pretraining works. It is worthwhile and essential to build standardized pretraining document image datasets with preprovided OCR results. In addition to scanned documents, using digital PDF as part of the pretraining data is a direction worth exploring since it provides rich metadata which could be beneficial for multimodal learning.” ([Gu et al., 2021, p. 10](zotero://select/library/items/YZ4B369T)) ([pdf](zotero://open-pdf/library/items/JM2W7NYK?page=10&annotation=23JAQZ54))

***
- @gu_2021b