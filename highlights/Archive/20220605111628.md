---
aliases:
  - Paragraph-based Transformer Pre-training for Multi-Sentence Inference
linter-yaml-title-alias: Paragraph-based Transformer Pre-training for Multi-Sentence Inference
order: -20220605111628
tags: [natural_language_processing/pre-trained_language_models, natural_language_processing, natural_language_processing/representation_learning, artificial_intelligence/deep_learning/transformers, natural_language_processing/paragraph-based]
title: Paragraph-based Transformer Pre-training for Multi-Sentence Inference
---

# Paragraph-based Transformer Pre-training for Multi-Sentence Inference

(6/5/2022, 11:16:28 AM)

“Pre-trained transformers such as BERT are used for these tasks as cross-encoders by setting them as sentence-pair classification problems, i.e, aggregating inferences independently over each candidate.” ([Di Liello et al., 2022, p. 1](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=1&annotation=6UEG9SFW))

“Specifically, given a target sentence s and multiple sentences (from the same/different paragraph/document), the model needs to recognize which sentences belong to the same paragraph as s in the document used.” ([Di Liello et al., 2022, p. 1](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=1&annotation=JVQ36WUH))

“Multi-Sentence Inference: Inference over a set of multiple candidates” ([Di Liello et al., 2022, p. 2](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=2&annotation=CIEJU72X)) Multiple choices
Fact verification
Answer extraction

“DeCLUTR (Giorgi et al., 2021) uses a contrastive learning objective for cross-encoding two sentences coming from the same/different documents in a transformer.” ([Di Liello et al., 2022, p. 2](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=2&annotation=T2HA9SH8))

“Multi-sentence Inference Tasks AS2: We denote the question by q, and the set of answer candidates by C={c1,...cn}. The objective is to re-rank C and find the best answer A for q.” ([Di Liello et al., 2022, p. 2](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=2&annotation=CJ6LXJZM))

“Intuitively, modeling interrelated information between multiple ci’s can help in selecting the best answer candidate (Zhang et al., 2021).” ([Di Liello et al., 2022, p. 2](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=2&annotation=BBIIWXQV))

“Fact Verification: We denote the claim by F , and the set of evidences by C={c1 ...cn} that are retrieved using DocIR. The objective is to predict whether F is supported/refuted/neither using C (at least one evidence ci is required for supporting/refuting F ).” ([Di Liello et al., 2022, p. 2](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=2&annotation=KKXXIQIS))

“We pad (or truncate) each sentence si to the same fixed length L (total input length L⇥(k + 1)), and use the embedding for the \[CLS\] / \[SEP\] token in front of each sentence si as its embedding (denoted by Ei).” ([Di Liello et al., 2022, p. 3](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=3&annotation=UNU6KHFB))

“IE1: a linear layer on the output embedding E0 of s0 (similar to BERT) referred to as the Individual Evidence (IE1)” ([Di Liello et al., 2022, p. 3](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=3&annotation=T5UQLBNU))

“AE1: a linear layer on the average of the output embeddings \[E0,E1,...,Ek\] to explicitly factor in information from all candidates, referred to as the Aggregated Evidence (AE1)” ([Di Liello et al., 2022, p. 3](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=3&annotation=BXEEM6L2))

“IEk: a shared linear layer applied to the output embedding Ei of each candidate si ,i2{1 ...k} referred to as k-candidate Individual Evidence (IEk)” ([Di Liello et al., 2022, p. 3](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=3&annotation=6EYTQP8S))

“AEk: a shared linear layer applied to the concatenation of output embedding E0 of input s0 and the output embedding Ei of each candidate si ,i2{1 ...k} referred to as kcandidate Aggregated Evidence (AEk)” ([Di Liello et al., 2022, p. 3](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=3&annotation=MENHXZ5T))

“we design a new pre-training task where the model is (i) provided with (k + 1) sentences {s0 ...sk}, and (ii) tasked to predict which sentences from {s1 ...sk} belong to the same paragraph P as s0 in the document D. We call this pre-training task Multi-Sentences in Paragraph Prediction (MSPP)” ([Di Liello et al., 2022, p. 3](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=3&annotation=6Q78EJ9V))

“We randomly sample a sentence from a paragraph P in a document D to be used as s0, and then (i) randomly sample k1 sentences (other than s0) from P as positives, (ii) randomly sample k2 sentences from paragraphs other than P in the same document D as hard negatives, and (iii) randomly sample k3 sentences from documents other than D as easy negatives” ([Di Liello et al., 2022, p. 3](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=3&annotation=DSVK2ATT))

“Student t-test with 95% confidence level” ([Di Liello et al., 2022, p. 4](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=4&annotation=DV38TZZ2))

“The performance gap stems from questions for which the pairwise RoBERTa model was unable to rank the correct answer at the top position, but support from other candidates in the top-k helped the joint model rank it in the top position.” ([Di Liello et al., 2022, p. 5](zotero://select/library/items/C58NHYJJ)) ([pdf](zotero://open-pdf/library/items/ZWHBG8SX?page=5&annotation=IAFJQSLU))

***
- @diliello_2022