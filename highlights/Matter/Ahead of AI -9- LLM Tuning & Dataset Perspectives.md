---
aliases:
  - Ahead of AI #9: LLM Tuning & Dataset Perspectives
url: https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset
author: Sebastian Raschka
publisher: magazine.sebastianraschka.com
order: -20230603045615
date: 2023-06-03
tags:
---

## Highlights
<mark>How about repeating only the high-quality data like LIMA, which we discussed above? That's an interesting idea; intuitively, this makes sense and could help. However, the bad news is that it probably doesn't help much. The researchers conducted a related experiment with Wikipedia data (versus C4) where they regarded Wikipedia as high-quality (which I agree with). However, it turned out that there was a similar degradation when repeating the Wikipedia data for multiple epochs.</mark>

