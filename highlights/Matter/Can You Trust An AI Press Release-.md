---
url: https://asteriskmag.com/issues/07/can-you-trust-an-ai-press-release
author: Lawrence Chan
publisher: Asterisk
aliases:
  - Can You Trust An AI Press Release?
title: "Can You Trust An AI Press Release-"
tags: []
created: 2024-08-13
modified: 2024-08-14
---

## Highlights

> Many took this to mean that Claude 3 Opus was significantly better than what was then the live version of GPT-4. However, the numbers that Anthropic used for GPT-4’s performance came from OpenAI’s March 2023 release blog post about the original public GPT-4 model (gpt-4-0314) and not the then-current GPT-4 Turbo variants (gpt-4-1106, gpt-4-0125).

> In fact, when we compare Opus with the later model, we see that GPT-4 Turbo performs better than Claude on more than half of the benchmarks where both were evaluated, and is competitive on the rest.

> First, exaggerated claims are not necessarily the result of deliberate deception—knowing the performance of models is genuinely difficult. This is especially true of OpenAI models, which, as we’ve seen, often get “stealth” updates without much fanfare or additional benchmark numbers.

> The same blog post also includes only six out of the 10 standard benchmarks that Anthropic included in the Claude 3 release blog post, and a different subset from those in the GPT-4 release. One wonders if the reason for this is that 4o performs worse than Opus on the unreported benchmarks.

> While we can use benchmarks to get a general sense of how good models are (it’d be surprising if a human-level AI could not do eighth-grade math), the correlation between scores and real-world performance isn’t strong enough that differences of less than 1% on benchmarks are meaningful, especially if all of the models already do quite well.

> Use the benchmarks as reference, not as definitive evidence. Benchmarks are helpful for getting a general sense of model capability, but small differences don’t mean much.

