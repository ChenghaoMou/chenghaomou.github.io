---
url: "https://www.wired.com/story/large-language-models-critique/"
author: "Birhane Deborah"
publisher: "Wired"
published: 2022-12-09
aliases:
  -  ChatGPT, Galactica, and the Progress Trap
title: ChatGPT, Galactica, and the Progress Trap
---

## Highlights
> Additionally, the creators of such models confess to the difficulty of addressing inappropriate responses that “do not accurately reflect the contents of authoritative external sources”. Galactica and ChatGPT have generated, for example, a “scientific paper” on the benefits of eating crushed glass (Galactica) and a text on “how crushed porcelain added to breast milk can support the infant digestive system” (ChatGPT). In fact, Stack Overflow had to temporarily ban the use of ChatGPT- generated answers as it became evident that the LLM generates convincingly wrong answers to coding questions.

> At this point, several of the potential and realized harms of these models have been exhaustively studied. For instance, these models are known to have serious issues with robustness. The sensitivity of the models to simple typos and mis-spellings in the prompts, and the differences in responses caused by even a simple re-wording of the same question, reveal an inconsistency that makes it unreliable for actual high-stakes use, such as translation in medical settings or content moderation, especially for marginalized identities.

> Model builders and tech evangelists alike attribute impressive and seemingly flawless output to a mythically autonomous model, a technological marvel. The human decision-making involved in model development is erased, and model feats are observed as independent of the design and implementation choices of its engineers. But without naming and recognizing the engineering choices that contribute to the outcomes of these models, it becomes almost impossible to acknowledge the related responsibilities. As a result, both functional failures and discriminatory outcomes are also framed as devoid of engineering choices - blamed on society at large or supposedly “naturally occurring” datasets, factors those developing these models will claim they have little control over.

> And although it is the choices of those with privilege that resulted in these systems, for some reason, it seems to be the job of the marginalized to “fix” them. In response to ChatGPT’s racist and misogynist output, Sam Altman (the CEO of OpenAI) appealed to the community of users to help improve the model. Such crowdsourced audits, especially when solicited, are not new modes of accountability - engaging in such feedback constitutes labour, albeit uncompensated labour. And people at the margins of society, who are disproportionately impacted by these systems, are experts at vetting these systems due to their lived experience. Not coincidentally, crucial contributions both that demonstrate the failure of models and ways to mitigate these problems, are often made by scholars of color – often Black women – and junior scholars that are underfunded and in precarious conditions, particularly compared to the wealth and power large corporations building these models hold. The weight falls on them, not only to provide this feedback, but to execute on various tasks that ideally the model builders themselves should be doing before release, such as documenting, analyzing, and carefully curating data.

