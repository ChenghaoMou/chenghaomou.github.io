---
aliases:
  - Distributed Computing for AI: A Status Report
url: https://gradientflow.com/distributed-computing-for-ai-a-status-report/?utm_medium=email&utm_source=topic+optin&utm_campaign=awareness&utm_content=20220606+data+ai+nl&mkt_tok=MTA3LUZNUy0wNzAAAAGE2PCA-6-U2bSYyAN9aRKnihY70IrDoBmduqVuHgChIUJUSOk0vS3nzzgp2v4RFFcqPDmdu4s1Lv7x8PsKgdsZLo4JTn-NhicBflFhtxv7ykgdrg
author: Ben Lorica
publisher: gradientflow.com
order: -20220503070002
date: 2022-05-03
tags:
---

## Highlights
<mark>Anthrophic’s Jack Clark recently observed: “One of the really big challenges we have is running really large clusters, capable of doing machine learning jobs on hundreds of thousands of GPUs.” And while new tools to simplify distributed training and inference continue to emerge – Ray Train, Ray Serve, and Google Pathways – most solutions still require people skilled at running and managing large-scale distributed systems.</mark>

