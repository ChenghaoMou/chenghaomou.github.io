---
url: https://write.as/ulrikehahn/do-llms-really-reason-understand-think-summarise
author: UlrikeHahn
publisher: write.as
published: 2024-08-16
aliases:
  - Do LLMs REALLY reason, understand, think, summarise...?
title: "Do LLMs REALLY reason, understand, think, summarise...-"
tags: []
created: 2024-09-03
modified: 2024-09-05
---

## Highlights

> Courts, for example, have to decide what things words apply to: from early 20th century debate on whether electricity was a “movable, physical thing” that could ‘stolen’ in the current sense of the law, to late 20th century debate on whether “marriage” applies only to a man and a woman. All such decisions are ultimately based on complex considerations about social realities, not by thinking about semantics. And any discussion, legal or otherwise, that assumes that real world issues and controversies can be settled just by arguing about the meaning of words is either unintentionally or intentionally missing the point. Finally, this dependence on purpose and goal isn’t restricted to social issues, it applies equally to seemingly esoteric academic debate.

> To illustrate: There is a steady stream of comment that asserts that LMMs can’t really reason because, for example, while they succeeded on inductive tasks, they fail on deductive examples [3], or that they can’t really reason because they fail on a seemingly simple problem [4]. This kind of assessment represents a classification decision based on attainment alone.

> The real significance of the three dimensional space, though, lies in the fact that it can give rise to communication at cross purposes and to over-estimating the inferential value of findings on one dimension with respect to another. The final section examines concrete examples of this.

> But the approach also makes sense: it is neither practical, nor possible, to defer, say, the experimental study of human reasoning until that day when we have a complete understanding of what constitutes `reasoning’. It’s not possible precisely because behavioural experiments are a significant part of figuring that out.

> Once we consider LLMs, the difficulties are compounded. Not only is `reasoning’ an underspecified concept in humans, we also don’t fully understand how LLMs do what they do. So there is no way we could hope to establish construct validity for an experimental paradigm without actually conducting experimental tests.

> Anyone with the research competence required to conduct the work understands the difference between task performance, operational definition, and ultimate real world phenomenon. That difference is so obvious that saying something like “shows evidence of theory of mind” basically means “succeeds on this experimental task” or, at best, “succeeds on this experimental task with implications for my operational definition”.

