---
url: "https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html"
author: "Google AI (noreply@blogger.com)"
publisher: "Google Research"
published: 2023-01-18
aliases:
  -  "Google AI Blog: Google Research, 2022 & Beyond: Language, Vision and Generative Models"
title: "Google AI Blog: Google Research, 2022 & Beyond: Language, Vision and Generative Models"
---

## Highlights
> The PaLM work demonstrated that, despite being trained solely on the objective of predicting the next token, large-scale language models trained on large amounts of multi-lingual data and source code are capable of improving the state-of-the-art across a wide variety of natural language, translation, and coding tasks, despite never having been trained to specifically perform those tasks. This work provided additional evidence that increasing the scale of the model and training data can significantly improve capabilities.

> In our work on “Multi-modal Bottleneck Transformers” and the accompanying “Attention Bottlenecks for Multimodal Fusion” paper, we explore these tradeoffs and find that bringing together modalities after a few layers of modality-specific processing and then mixing the features from different modalities through a bottleneck layer is more effective than other techniques (as illustrated by the Bottleneck Mid Fusion in the figure below). This approach substantially improves accuracy on a variety of video classification tasks by learning to use multiple modalities of data to make classification decisions.

