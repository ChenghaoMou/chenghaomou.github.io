---
url: "https://www.theguardian.com/commentisfree/2022/jun/14/human-like-programs-abuse-our-empathy-even-google-engineers-arent-immune"
author: "Emily M Bender"
publisher: "The Guardian"
published: 2022-06-14
aliases:
  -  Human-like programs abuse our empathy - even Google engineers aren’t immune
title: Human-like programs abuse our empathy - even Google engineers aren’t immune
---

## Highlights
> Those sequences only become meaningful when we, as humans, read them.

> So when we encounter seemingly coherent text coming from a machine, we apply this same approach to make sense of it: we reflexively imagine that a mind produced the words with some communicative intent.

> This makes it seem as if it has “emergent behaviours” (capabilities that weren’t programmed in), which can easily be interpreted as evidence of artificial intelligence by someone who wants to believe it.

> A language model synthesises word strings to give answers in response to queries, but can’t point to information sources. This means the user can’t evaluate these sources.

> At the same time, returning conversational responses will encourage us to imagine a mind where there isn’t any, and one supposedly imbued with Google’s claimed ability to “organise the world’s information”.

> With systems such as LaMDA we see their potential perils and the urgent need to design systems in ways that don’t abuse our empathy or trust.

