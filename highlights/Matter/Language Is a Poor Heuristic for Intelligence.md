---
url: "https://karawynn.substack.com/p/language-is-a-poor-heuristic-for"
author: "colinprince"
publisher: "Nine Lives"
published: 2023-07-02
aliases:
  -  Language Is a Poor Heuristic for Intelligence
title: Language Is a Poor Heuristic for Intelligence
---

## Highlights
> But the prevalence of these stories suggests that many — perhaps even most — nonspeaking children have fully ‘normal’ intelligence, as intelligence is popularly understood and typically measured. Some of them are undeniably brilliant. There are definitely autistic children who also have intellectual disabilities, but not nearly as many or as severe as people assume … because the rest of humanity keeps using the language fluency heuristic.

> (It’s a completely false narrative — Helen did not behave like a wild animal; she was already communicating with her own original word-signs before Sullivan came along.) But Deaf people have been treated like animals (or worse) in many times and places, because hearing and speaking people assumed they were unintelligent.

> This has been a recurring problem for at least thousands of years; it rarely improves or even gets much attention, because it has only ever affected a small, inherently disadvantaged subset of the population. What’s interesting about the current moment is that — for the first time in history — we’re watching the rapid, massive, and egregious failure of this underlying heuristic impact all of human society.

> For example, text-to-image generators like DALL-E and Midjourney utilize the same underlying LLM technology as chatbots, but the public release of these programs last year — controversial as they were for other reasons — did not spark the same kinds of zealous speculation about a mind in the machine. Humans don’t have a history of using “image generation” or even “visual art” as a heuristic for intelligence. Only fluency of language has that distinction.

> Furthermore, it’s not just the brain we’d have to model; emotions are central to the most basic functioning of our brains, and the ability to experience emotions is inextricably bound up with having a body. As neuroscience professor Giandomenico Iannetti explains, “Our brain inhabits a body that can move to explore the sensory environment necessary for consciousness,” whereas an LLM generates output “by emulating a nervous system but without attempting to simulate it. This precludes the possibility that it is conscious.”

> Not coincidentally, you’ll often find that the people making the loudest argument for impending computer sentience are literally invested in OpenAI and Google and Microsoft and Meta. I am hardly the first person to point this out, but trillions of dollars are at stake, and these (already extremely wealthy) people reap direct financial benefits when the public mistakes the software’s fluency of language for intelligence and comprehension.

> Whenever an LLM says something contrafactual, the companies claim — and the media dutifully reports — that the program is “hallucinating” … a word that suggests both a conscious experience and external senses, neither of which pertain to computer algorithms. Words like “fabricating” and “lying” and even “bullshitting” aren’t much better, because those terms all carry connotations of “intent to deceive”, and no LLM program can ever have intent, either malicious or benign.

> Making chatbots that seem to apologize is a choice. Giving them cartoon-human avatars and offering up “Hello! How can I help you today?” instead of a blank input box: choices. Making chatbots that talk about their nonexistent “feelings” and pepper their responses with facial emojis is another choice. Making algorithms that “chat” at all — framing every interaction as a “dialogue” — is the most basic choice, and one that we really ought to be side-eyeing more. OpenAI, Microsoft, Google, and other companies are deliberately guiding these algorithms to emulate a knowledgeable, intelligent, and friendly human, even though the software is exactly zero of those four things. Only when this approach gets them into trouble do they backpedal.

> Computer scientist Timnit Gebru (famously fired from Google in 2020 for raising ethical issues around the use of AI) has repeatedly warned that when the public conversation focuses on red herrings — like the potential morality and values of a wholly-theoretical computer intelligence — we cease to ascribe responsibility to the real people and actual corporations that are creating harmful products. Any action taken to counteract those harms would cut into profit, so the LLM-invested folks would really prefer we all worry about a mirage instead.

> That’s because the fact that we still know it’s a computer activates another, more modern rule of thumb: that computer-generated information is accurate and trustworthy.

> But LLMs have now shattered the usefulness of the “computers are reliably accurate” heuristic as well, biting everyone from lawyers to university professors in the ass along the way.

> But by pointing out that these are opposite sides of the same flawed coin, I hope to encourage more people to question their automatic assumptions about language and intelligence in disabled humans as well.

