---
aliases:
  - On NYT Magazine on AI: Resist the Urge to be Impressed
url: https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd
author: M. Bender
publisher: Medium - Tech
order: -20220417171509
date: 2022-04-17
tags:
---

## Highlights
<mark>This comes down to the concept of “construct validity”: does the test actually measure some construct which is coherent in itself and effectively measured by the test? To establish construct validity for reading comprehension tests, we need a definition of what reading comprehension is as well as evidence that answering the test questions more accurately corresponds to more reading comprehension. There is no reason to believe, a priori, that a test designed to achieve those ends for humans would be equally effective for machines, because there is no reason to believe that machines use the same processes for answering the questions as humans do. (For</mark>

<mark>Someone who needs to create a licensing agreement or a lease doesn’t just need a document that looks and feels like a licensing agreement or a lease. They need something that speaks to their particular situation and is legally binding in their particular jurisdiction. A set of strings that take the form of legalese is not a “sophisticated legal document”.</mark>

<mark>This leaves me wondering if Johnson has forgotten (or never understood) why high school students are asked to write essays. It is not, to be sure, to keep the world’s supply of essays topped up! Rather, it is about what the students learn in the process of doing the writing.</mark>

<mark>We can imagine other futures, but to do so, we have to maintain independence from the narrative being pushed by those who believe that “AGI” is desirable and that LLMs are the path to it.</mark>

<mark>First, in fact, the third of those (offering up life-threatening advice) doesn’t (only) stem from the fact that their training data is uncurated. Rather, it’s connected to the fact that by design LLMs are just making stuff up. More specifically, they’re making up text strings in the language they’re trained on, which humans who speak that language can interpret. So when a person comes in with a health and safety related question, if what comes back is wrong, chances are high it will also be dangerous. Most importantly: these can be understood as reasons not to use LLMs (perhaps at all, but at least in very many specific applications).</mark>

