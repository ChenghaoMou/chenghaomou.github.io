---
url: "https://paperswithcode.com/newsletter/24/"
publisher: "paperswithcode.com"
published: 2022-02-02
aliases:
  -  Papers with Code Newsletter #24
title: Papers with Code Newsletter #24
---

## Highlights
> This work reports that large enough pretrained LMs, if prompted appropriately, can effectively decompose high-level tasks into low-level plans. Although LMs can produce action plans, they are found not to be executable in an interactive environment.

> CXV utilizes priors provided by convolutional-based sub-layers to avoid the need for class token and positional embeddings used in ViTs. Combining convolutions with attention along with layer normalization allows the model to capture both spatial and inter-pixel relationships better.

> The main idea is to allow both representations (language representations and knowledge representations) to be contextualized and improved by each other through a modality interaction unit.

