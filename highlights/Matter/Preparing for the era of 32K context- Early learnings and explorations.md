---
url: "https://together.ai/blog/llama-2-7b-32k"
author: "Together"
publisher: "together.ai"
published: 2023-07-28
aliases:
  -  "Preparing for the era of 32K context: Early learnings and explorations"
title: "Preparing for the era of 32K context: Early learnings and explorations"
---

## Highlights
> On the modeling side, we follow Metaâ€™s recent paper and use linear interpolation to extend the context length. This provides a powerful way to extend the context length for models with rotary positional embeddings. We take the LLaMA-2 checkpoint, and continue pre-training/fine-tuning it with linear interpolation for 1.5B tokens.

