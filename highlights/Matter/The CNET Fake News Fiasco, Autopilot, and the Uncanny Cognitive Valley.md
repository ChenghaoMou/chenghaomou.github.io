---
aliases:
  - The CNET Fake News Fiasco, Autopilot, and the Uncanny Cognitive Valley
url: https://garymarcus.substack.com/p/the-cnet-fake-news-fiasco-autopilot
author: Gary Marcus
publisher: The Road to AI We Can Trust
order: -20230131095202
date: 2023-01-31
tags:
---

## Highlights
<mark>Mackworth himself noticed that people had trouble with vigilance while studying radar operators. Operators would open their shift doing a great job looking for radar signals that were rare but important; but it wouldn’t take long, perhaps half an hour, before they stopped paying full attention. Mackworth published some of his first results in 1948, under the title “The breakdown of vigilance during prolonged visual search.” The term “vigilance” as Mackworth used it, and as cognitive psychologists today continue to use it, means paying close attention for long periods of time. As a later summary of Mackworth’s work and the hundreds of subsequent papers put it, “Vigilance Requires Hard Mental Work and Is Stressful”.</mark>

<mark>Early in Google’s autonomous vehicle efforts, Google had tried to develop driver-assisted cars, in which AI did most of the work, but in which a human driver sat in front of the steering wheel, poised at all moments to take over. Google quickly learned that attention would eventually wander; humans in the loop couldn’t be trusted.</mark>

<mark>Google, to its credit, very quickly realized this was not a good thing. Indeed, a year before Urmson gave this TED Talk, I remember talking to Larry Page (then Google’s CEO) and he’d presumably already heard the same story. Page was convinced that human drivers could never be trusted to pay enough attention, and that anyone building cars on that premise was making a mistake. Page’s response was to ditch the steering wheel altogether; full autonomy or nothing. In April 2014, Google unveiled its first steering wheel-free car. If humans could never be trusted, the only way forward was to take the fallible human out of the loop altogether, even if the research would take a long time. The branch of Google (now Alphabet) that started working on full-human-out-of-the-loop autonomy is now called Waymo, and they have never looked back.</mark>

<mark>The same lab that seems to have pocket vetoed Fridman’s study published another paper a year later in 2020—minus Fridman—confirming exactly what Mackworth said all along: “As automation improves and failures become rarer, drivers may tend to forgo their monitoring role: be less attentive (more time eyes off-road) and relax direct control (hands off-wheel).” People can’t pay attention forever. The vanished 2019 paper is not even mentioned. Instead, the first article in that 2020 paper’s literature review cites on the topic was called Is partially automated driving a bad idea? Observations from an on-road study. And the answer that article gave is pretty clear: thematic analysis of video data suggests that drivers are not being properly supported in adhering to their new monitoring responsibilities and instead demonstrate behaviour indicative of complacency and over-trust. These attributes may encourage drivers to take more risks whilst out on the road.</mark>

<mark>If the autogenerated articles had been riddled with spelling errors and grammatical errors, the editors might not have trusted them. But at a superficial level the essays looked good enough, and so they editors trusted them, and a bunch of false information slipped through. That’s going to happen a whole lot more this year, with synthetic articles. Too many people are starting to count on systems that inherently can’t keep track of the truth. Few people are likely ever to pay enough attention. The stakes may get even higher if people start to rely on immature AI—which is all we have got, so far—for legal or medical advice.</mark>

