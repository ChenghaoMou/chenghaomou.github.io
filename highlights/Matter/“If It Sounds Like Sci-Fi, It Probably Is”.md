---
url: https://journal.getabstract.com/en/2023/08/03/if-it-sounds-like-sci-fi-it-probably-is/
author: Caryn Hunt
publisher: journal.getabstract.com
order: -20230803174011
date: 2023-08-03
tags:
title: “If It Sounds Like Sci-Fi, It Probably Is”
---

## Highlights
<mark>If we focus on them as autonomous thinking entities or we spin out that fantasy, it is easier to lose track of the people in the picture, both the people who should be accountable for what the systems are doing and the people whose labor and data are being exploited to create them in the first place.</mark>

<mark>So, with ChatGPT, it’s the same thing. We are shaping our input to it so that we can make sense of what comes back from it. It’s all on our side.</mark>

<mark>I don’t think it’s possible. I think it’s a fundamental design flaw or a fundamental mismatch between the technology and the task that it’s being promoted for. Certainly, over time it could be made better. There are certain kinds of errors that could be trained out through this human feedback step. But there was some recent reporting about gig workers who’ve been doing labeling work for Google in particular, and these gig workers were instructed to not actually fact-check the outputs, just whether it looks plausible or not. So that’s not very promising. You use the word “hallucinate,” and it’s true that people in the field use it. I object to that word because a hallucination involves the subjective experience of perceiving something that’s not there. And that’s not what’s happening. This is a mechanistic process of outputting sequences of words. If it corresponds to something that we read and say, “Yes, that checks out, that’s true,” that was actually by chance, not by design.</mark>

<mark>If what you’re looking at is ultimately the documents and the document set, then that is safer. If what you’re using them for is summarization or paraphrasing of what’s in the documents, then you’re still faced with a problem that these systems have not understood anything. They might be inaccurate less frequently in that context. But then you have to ask, is it actually better? I haven’t seen studies on this, but my guess is that a system like this, that’s right 95% of the time is more dangerous than one that’s right 50% of the time, because you come to trust it. Then you’re not going to catch that 5%.</mark>

<mark>A bunch of the CNET stuff was effectively plagiarized too. If you’re doing this in a place where originality of content matters, then you can’t rely on synthetic media because you don’t know where it’s coming from. You can’t trace it back and give credit where credit is due.</mark>

<mark>And so you can say, okay, what’s the task being automated, what’s the input, what’s the output? How was it trained? Where did that training data come from? How was it evaluated? How did that evaluation match the use case? If someone says this is right 95% of the time, well, 95% of what time? And how does that relate to how we’re talking about using it now? And then you can ask questions like who’s benefiting from automating this task? Who’s harmed by the fact of automation? Who’s harmed when it’s incorrect? Who benefits when it’s incorrect? Why are we doing this? Why would we automate that task? There’s a tweet I’ve seen various people put out, which is something like, you know, AI was supposed to do all the boring stuff so that we could live a life of leisure and write poetry and paint. So why are we creating systems to automate that kind of work? What’s that for?</mark>

<mark>But I think when we automate things, we should be doing good engineering practices and designing technology for particular use cases with an understanding of their social context. And I’m not seeing nearly enough of that in this space.</mark>