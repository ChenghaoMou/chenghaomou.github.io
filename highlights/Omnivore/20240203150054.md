---
id: eaa55e40-c291-11ee-84f1-abb7e7919b26
alias:
  - Research Papers in January 2024 - by Sebastian Raschka, PhD
title: Research Papers in January 2024 - by Sebastian Raschka, PhD
author: |
  Sebastian Raschka, PhD
date: 2024-02-03 15:00:54
url: https://magazine.sebastianraschka.com/p/research-papers-in-january-2024
---

# Research Papers in January 2024 - by Sebastian Raschka, PhD

[Read on Omnivore](https://omnivore.app/me/research-papers-in-january-2024-by-sebastian-raschka-ph-d-18d6eff5c9b)

[Read Original](https://magazine.sebastianraschka.com/p/research-papers-in-january-2024)

## Highlights

> A common form of this technique is _[Stochastic Weight Averaging (SWA)](https://arxiv.org/abs/1803.05407)_, where weights are averaged over several iterations during periods of low learning rates.
> 
> [![](https://proxy-prod.omnivore-image-cache.app/456x0,sCnZSKRVDohEc_5No3kc1lMhB6sgaDUPrmYihoCBNcBE/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7aa967a-7ee4-4b8c-9a08-0a797a9c7566_974x524.png)](https://substackcdn.com/image/fetch/f%5Fauto,q%5Fauto:good,fl%5Fprogressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7aa967a-7ee4-4b8c-9a08-0a797a9c7566%5F974x524.png)
> 
> _Stochastic Weight Averaging (SWA) averages a model's weights towards the end of the training cycle._
> 
> Since a model's training trajectory can be uneven, the strategy is to average the models towards the end of the training when the learning rate is low (if a scheduler is used), as illustrated in the figure above, where the training is nearing convergence. [⤴️](https://omnivore.app/me/research-papers-in-january-2024-by-sebastian-raschka-ph-d-18d6eff5c9b#0ea485e8-b117-47d1-8e80-e4d839d11d8f)  ^0ea485e8

> While weight averaging combines multiple checkpoints of the same model into a single model, _**model merging**_ involves combining multiple _**different**_ trained models into a single model. Each of these models may have been trained independently, possibly on different datasets or tasks. [⤴️](https://omnivore.app/me/research-papers-in-january-2024-by-sebastian-raschka-ph-d-18d6eff5c9b#d498b6b8-cd61-4f37-977c-6176e846bd70)  ^d498b6b8

> To illustrate this concept more clearly, consider the objective of improving a large target model, M1 (for example, Llama 2 70B). The process involves two smaller models:
> 
> * A small base model (M2) like Llama 2 7B
> * A finetuned version of the base model (M3) Llama 2 7B Chat
> 
> The enhancement is achieved by applying the difference in predictions (logits) of these smaller models to the target model M1\. The output logits of the improved target model, _M1\*_, is computed as _M1\*(x) = M1(x) + \[M3(x) - M2(x)\]_. After obtaining these output logits, they are converted into probabilities using the softmax function. These probabilities are then used to sample the final output, i.e., the generated text, using [nucleus sampling](https://arxiv.org/abs/1904.09751) or [top-k decoding](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature). [⤴️](https://omnivore.app/me/research-papers-in-january-2024-by-sebastian-raschka-ph-d-18d6eff5c9b#5223f3c9-e160-4d6f-9c31-d57903c1d5d9)  ^5223f3c9

