---
id: dcc1b29a-ce9b-11ee-b7e7-336c9336416f
alias:
  - "Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch"
title: "Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch"
author: |
  Sebastian Raschka, PhD
date: 2024-03-05 20:46:36
url: https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch
---

# Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch

[Read on Omnivore](https://omnivore.app/me/improving-lo-ra-implementing-weight-decomposed-low-rank-adaptati-18dbde533ea)

[Read Original](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)

## Highlights

> When using LoRA, we hypothesize that the model requires _W_ to be a large matrix with full rank to capture all the knowledge in the pretraining dataset. However, when we finetune an LLM, we don't need to update all the weights and capture the core information for the adaptation in a smaller number of weights than _**ΔW**_ would; hence, we have the low-rank updates via _**AB**_. [⤴️](https://omnivore.app/me/improving-lo-ra-implementing-weight-decomposed-low-rank-adaptati-18dbde533ea#75e94042-8219-4e37-afbe-d98bb8e86fa2)  ^75e94042

> The fact that we can keep the LoRA weight matrices separate makes LoRA especially attractive. In practice, this means that we don't have to modify the weights of the pretrained model at all, as we can apply the LoRA matrices on the fly. This is especially useful if you are considering hosting a model for multiple customers. Instead of having to save the large updated models for each customer, you only have to save a small set of LoRA weights alongside the original pretrained model. [⤴️](https://omnivore.app/me/improving-lo-ra-implementing-weight-decomposed-low-rank-adaptati-18dbde533ea#35a5e86f-0f9e-4107-8893-59dd92ffeed8)  ^35a5e86f

> The second hyperparameter, `alpha`, is a scaling hyperparameter applied to the output of the low-rank adaptation. It essentially controls the extent to which the adapted layer's output is allowed to influence the original output of the layer being adapted. This can be seen as a way to regulate the impact of the low-rank adaptation on the layer's output. [⤴️](https://omnivore.app/me/improving-lo-ra-implementing-weight-decomposed-low-rank-adaptati-18dbde533ea#eacd8bfb-5f5f-49c7-97ad-28aace7b7ec0)  ^eacd8bfb

> The motivation for developing DoRA is based on analyzing and comparing the LoRA and full finetuning learning patterns. The DoRA authors found that LoRA either increases or decreases magnitude and direction updates proportionally but seems to lack the capability to make only subtle directional changes as found in full finetuning. Hence, the researchers propose the decoupling of magnitude and directional components. [⤴️](https://omnivore.app/me/improving-lo-ra-implementing-weight-decomposed-low-rank-adaptati-18dbde533ea#6269c615-f623-4693-9b0a-516c6af46e01)  ^6269c615

