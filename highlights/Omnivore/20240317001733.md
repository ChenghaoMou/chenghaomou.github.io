---
id: 27390abe-c445-47c5-a00e-5f3f25687e9e
aliases:
  - Integer tokenization is insane
title: "Integer tokenization is insane"
author: |
  Beren Millidge
date: 2024-03-17 00:17:33
url: https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/
tags:
  - TODO
---

# Integer tokenization is insane

[Read on Omnivore](https://omnivore.app/me/integer-tokenization-is-insane-18e49c557f8)

[Read Original](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)

## Highlights

> In the first 10000 integers there are 916 unique tokens (so almost 1/10th) of the tokens are unique and the number tokens take up about 1 50th of the total tokenizer space (GPT2s tokenizer is approximately 50k tokens). This means that any calculation or mathematical problem which involves these integers in any way must be special-cased somehow and operate off of pure memorization. For instance, the model cannot use a normal addition algorithm when given a problem like 54 + 72 = 126 since every single one of these tokens are unique. Instead it must memorize an extremely large number of problems and their answers. Essentially almost all two and most 3 digit addition and subtraction problems must be solved with memorization instead of a coherent and generalizable algorithm. [⤴️](https://omnivore.app/me/integer-tokenization-is-insane-18e49c557f8#a3f496e6-cc9d-4e09-b5cb-a69f9aca0e9c)  ^a3f496e6

> An interesting feature is also the band of integers assigned unique tokens in the 1900-2000 region. These represent common dates – i.e. from 1930-2020 are all assigned unique tokens because these dates occur most frequently in the training set (interestingly unique tokens are assigned up to the year 2020 and then abruptly stop, allowing you to date the tokenizer creation to 2019-2020). [⤴️](https://omnivore.app/me/integer-tokenization-is-insane-18e49c557f8#217a2c2f-34ef-4f1b-954c-de21938d6db2)  ^217a2c2f

> Ultimately, what this means is that to execute even simple numerical algorithms like multi-digit addition, the model has to learn a series of special cases depending on the exact details of the tokenization and, from looking the tokenization of larger numbers it looks like this problem never really goes away and there is always inconsistent chunking of large numbers into tokens and the occasional unique token to contend with. [⤴️](https://omnivore.app/me/integer-tokenization-is-insane-18e49c557f8#28e8f2a1-c069-4bdc-8f77-1166fa2325b1)  ^28e8f2a1

