---
id: 197b9cd7-b4f2-4b09-93af-32da20992dfe
alias:
  - Tips for LLM Pretraining and Evaluating Reward Models
title: "Tips for LLM Pretraining and Evaluating Reward Models"
author: |
  Sebastian Raschka, PhD
date: 2024-03-31 13:00:35
url: https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms
created: 2024-04-01
modified: 2024-04-21
---

# Tips for LLM Pretraining and Evaluating Reward Models

[Read on Omnivore](https://omnivore.app/me/https-magazine-sebastianraschka-com-p-tips-for-llm-pretraining-a-18e9461fb01)

[Read Original](https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms)

## Highlights

> However, for reward models, it's more common to use the analogous Bradley-Terry model, which is designed for pairwise comparison tasks, where the goal is not to classify items into categories independently but rather to determine the preference or ranking between pairs of items. [⤴️](https://omnivore.app/me/https-magazine-sebastianraschka-com-p-tips-for-llm-pretraining-a-18e9461fb01#b9d5da45-7618-4894-b591-04534a50169c)  ^b9d5da45

> [![](https://proxy-prod.omnivore-image-cache.app/603x0,sVepbR2K4Z90Mstz4ZEgoLDapVWi313B68m4f4hvJCs4/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016_1600x1015.png)](https://substackcdn.com/image/fetch/f%5Fauto,q%5Fauto:good,fl%5Fprogressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016%5F1600x1015.png)
> 
> Comparing reward models and DPO [⤴️](https://omnivore.app/me/https-magazine-sebastianraschka-com-p-tips-for-llm-pretraining-a-18e9461fb01#5f6b4254-094c-421f-abe0-c980e7cda951)  ^5f6b4254

> The β in the equations above typically acts as a temperature parameter that controls the sensitivity of the probability distribution to the differences in the scores from the policies. A higher beta makes the distribution more sensitive to differences, resulting in a steeper function where preferences between options are more pronounced. A lower beta makes the model less sensitive to score differences, leading to a flatter function that represents weaker preferences. Essentially, beta helps to calibrate how strongly the preferences are expressed in the probability model. [⤴️](https://omnivore.app/me/https-magazine-sebastianraschka-com-p-tips-for-llm-pretraining-a-18e9461fb01#42528494-eb44-4eb9-8fe7-c58d34d1f3bb)  ^42528494

