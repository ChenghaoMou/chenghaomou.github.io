---
id: bdb0b319-84e3-4de1-baf3-e7a712b8f63c
aliases:
  - Quality Assurance, Errors, and AI – O’Reilly
title: "Quality Assurance, Errors, and AI – O’Reilly"
author: |
  Mike Loukides
date: 2024-04-22 09:05:23
url: https://www.oreilly.com/radar/quality-assurance-errors-and-ai/
created: 2024-04-30
modified: 2024-05-06
cssclasses:
  - reference
---

# Quality Assurance, Errors, and AI – O’Reilly

[Read on Omnivore](https://omnivore.app/me/https-www-oreilly-com-radar-quality-assurance-errors-and-ai-18f04d688f2)

[Read Original](https://www.oreilly.com/radar/quality-assurance-errors-and-ai/)

## Highlights

> Even with unit tests, though, we run into the basic problem of AI: it can generate a test suite, but that test suite [can have its own errors](https://learning.oreilly.com/library/view/use-chatgpt-to/9781098167646/ch01.html). What does “testing” mean when the test suite itself may have bugs? Testing is difficult because good testing goes beyond simply verifying specific behaviors. [⤴️](https://omnivore.app/me/https-www-oreilly-com-radar-quality-assurance-errors-and-ai-18f04d688f2#5a48e78f-8b25-4fb5-8b01-e49b31021ad4)  ^5a48e78f

> Another difficulty with testing is that bugs aren’t just minor slips and oversights. The most important bugs result from misunderstandings: misunderstanding a specification or correctly implementing a specification that doesn’t reflect what the customer needs. Can an AI generate tests for these situations? An AI might be able to read and interpret a specification (particularly if the specification was written in a machine-readable format—though that would be another form of programming). But it isn’t clear how an AI could ever evaluate the relationship between a specification and the original intention: what does the customer really want? What is the software really supposed to do? [⤴️](https://omnivore.app/me/https-www-oreilly-com-radar-quality-assurance-errors-and-ai-18f04d688f2#46c59dc1-ce28-421a-b51e-adc6415861b7)  ^46c59dc1

> We quickly run into an extension of [Kernighan’s Law](https://www.laws-of-software.com/laws/kernighan/): debugging is twice as hard as writing code. So if you write code that’s at the limits of your understanding, you’re not smart enough to debug it. What does this mean for code that you haven’t written? Humans have to test and debug code that they didn’t write all the time; that’s called “maintaining legacy code.” But that doesn’t make it easy or (for that matter) enjoyable. [⤴️](https://omnivore.app/me/https-www-oreilly-com-radar-quality-assurance-errors-and-ai-18f04d688f2#6a8b075b-e50b-4cf7-8652-bf566ca3ba80)  ^6a8b075b

> To write a real app, you have to understand why it will succeed. What problem it solves. How it relates to the real world. Understand the domain, in other words. [⤴️](https://omnivore.app/me/https-www-oreilly-com-radar-quality-assurance-errors-and-ai-18f04d688f2#c729c043-028d-4c9f-bfb0-750bcbe1a6ba)  ^c729c043

> Cranking out lines of code isn’t what makes software good; that’s the easy part. Nor is cranking out test suites, and [if generative AI can help write tests](https://read.engineerscodex.com/p/metas-new-llm-based-test-generator) without compromising the quality of the testing, that would be a huge step forward. (I’m skeptical, at least for the present.) The important part of software development is understanding the problem you’re trying to solve. Grinding out test suites in a QA group doesn’t help much if the software you’re testing doesn’t solve the right problem. [⤴️](https://omnivore.app/me/https-www-oreilly-com-radar-quality-assurance-errors-and-ai-18f04d688f2#de401e76-f077-4215-a67a-4e6bdca48a13)  ^de401e76

