---
id: e76a9500-697f-47c8-a52e-106e973e2656
alias:
  - Mixture of Experts Explained
title: "Mixture of Experts Explained"
author: |
  unknown
date: 2024-04-26 08:45:44
url: https://huggingface.co/blog/moe
created: 2024-04-27
modified: 2024-04-27
---

# Mixture of Experts Explained

[Read on Omnivore](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64)

[Read Original](https://huggingface.co/blog/moe)

## Highlights

> **Sparse MoE layers** are used instead of dense feed-forward network (FFN) layers. MoE layers have a certain number of “experts” (e.g. 8), where each expert is a neural network. In practice, the experts are FFNs, but they can also be more complex networks or even a MoE itself, leading to hierarchical MoEs! [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#8e20a204-30f6-441d-977c-92c29c839416)  ^8e20a204

> A **gate network or router**, that determines which tokens are sent to which expert. For example, in the image below, the token “More” is sent to the second expert, and the token "Parameters” is sent to the first network. As we’ll explore later, we can send a token to more than one expert. How to route a token to an expert is one of the big decisions when working with MoEs - the router is composed of learned parameters and is pretrained at the same time as the rest of the network. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#8d456308-1068-4846-8021-e5a11891b3d3)  ^8d456308

> For example, although large batch sizes are usually better for performance, batch sizes in MOEs are effectively reduced as data flows through the active experts. For example, if our batched input consists of 10 tokens, **five tokens might end in one expert, and the other five tokens might end in five different experts, leading to uneven batch sizes and underutilization**. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#0bb67df9-2938-4831-95a8-7e3562bc04d1)  ^0bb67df9

> As discussed before, if all our tokens are sent to just a few popular experts, that will make training inefficient. In a normal MoE training, the gating network converges to mostly activate the same few experts. This self-reinforces as favored experts are trained quicker and hence selected more. To mitigate this, an **auxiliary loss** is added to encourage giving all experts equal importance. This loss ensures that all experts receive a roughly equal number of training examples. The following sections will also explore the concept of expert capacity, which introduces a threshold of how many tokens can be processed by an expert. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#c94fc3ab-8882-45cc-8daa-fc5da6b3585c)  ^c94fc3ab

> **Expert capacity**: we can set a threshold of how many tokens can be processed by one expert. If both experts are at capacity, the token is considered overflowed, and it’s sent to the next layer via residual connections (or dropped entirely in other projects). This concept will become one of the most important concepts for MoEs. Why is expert capacity needed? Since all tensor shapes are statically determined at compilation time, but we cannot know how many tokens will go to each expert ahead of time, we need to fix the capacity factor. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#2d5e9671-c503-4a2e-b9ce-5f2e80d7065f)  ^2d5e9671

> Router z-loss, introduced in [ST-MoE](https://arxiv.org/abs/2202.08906), significantly improves training stability without quality degradation by penalizing large logits entering the gating network. Since this loss encourages absolute magnitude of values to be smaller, roundoff errors are reduced, which can be quite impactful for exponential functions such as the gating. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#db4b356a-8422-4ab8-a0c1-362d1f433f62)  ^db4b356a

> Sparse models are more prone to overfitting, so we can explore higher regularization (e.g. dropout) within the experts themselves (e.g. we can have one dropout rate for the dense layers and another, higher, dropout for the sparse layers). [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#bffb1953-317b-4483-9398-8b4b63a10732)  ^bffb1953

> One question is whether to use the auxiliary loss for fine-tuning. The ST-MoE authors experimented with turning off the auxiliary loss, and the quality was not significantly impacted, even when up to 11% of the tokens were dropped. Token dropping might be a form of regularization that helps prevent overfitting. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#4c3c0904-1ee5-418a-9288-464af85c2728)  ^4c3c0904

> We could try the opposite: freezing only the parameters in MoE layers, which worked almost as well as updating all parameters. This can help speed up and reduce memory for fine-tuning. This can be somewhat counter-intuitive as 80% of the parameters are in the MoE layers (in the ST-MoE project). Their hypothesis for that architecture is that, as expert layers only occur every 1/4 layers, and each token sees at most two experts per layer, updating the MoE parameters affects much fewer layers than updating other parameters. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#f0d492e4-e2d1-4f8c-8eb9-23dbdee0a3c8)  ^f0d492e4

> One last part to consider when fine-tuning sparse MoEs is that they have different fine-tuning hyperparameter setups - e.g., sparse models tend to benefit more from smaller batch sizes and higher learning rates. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#7906e9c3-baf3-45e9-9901-5b9a7c733071)  ^7906e9c3

> MoEs benefit more from a higher number of tasks. Unlike the previous discussion suggesting to turn off the auxiliary loss function, the loss actually prevents overfitting. [⤴️](https://omnivore.app/me/https-huggingface-co-blog-moe-18f195dfe64#e4a84a02-293a-403e-bef8-0eab0d125e48)  ^e4a84a02

