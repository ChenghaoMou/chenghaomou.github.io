---
id: da0aae51-19ac-4ff2-a6f7-25bc0c917203
alias:
  - "AI leaderboards are no longer useful. It's time to switch to Pareto curves."
title: "AI leaderboards are no longer useful. It's time to switch to Pareto curves."
author: |
  Sayash Kapoor, Arvind Narayanan
date: 2024-04-30 18:23:32
url: https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful
created: 2024-05-04
modified: 2024-05-05
---

# AI leaderboards are no longer useful. It's time to switch to Pareto curves.

[Read on Omnivore](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7)

[Read Original](https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful)

## Highlights

> A useful evaluation of agents must therefore ask: What did it cost? If we don’t do cost-controlled comparisons, it will encourage researchers to develop extremely costly agents just to claim they topped the leaderboard. [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#12a3bbce-143e-4e29-8f7c-1e9ffb916605)  ^12a3bbce

> Our most striking result is that **agent architectures for HumanEval do not outperform our simpler baselines despite costing more**. In fact, agents differ drastically in terms of cost: for substantially similar accuracy, the cost can differ by almost two orders of magnitude![8](https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful#footnote-8-144156093) Yet, the cost of running these agents isn't a top-line metric reported in any of these papers.[9](https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful#footnote-9-144156093)
> 
> [![](https://proxy-prod.omnivore-image-cache.app/628x0,sp5HSCEfIKgMl6EU9bDiZahRF7Lmb8MuMsK3ApTlrrr0/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb16a9f-29bd-47a4-813e-929685ce3fd8_1029x705.png)](https://substackcdn.com/image/fetch/f%5Fauto,q%5Fauto:good,fl%5Fprogressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb16a9f-29bd-47a4-813e-929685ce3fd8%5F1029x705.png)
> 
> _Our simple baselines offer Pareto improvements over existing agent architectures. We run each agent five times and report the mean accuracy and the mean total cost on the 164 HumanEval problems. Where results for LDB have two models/agents in parenthesis, they indicate the language model or agent used to generate the code, followed by the language model used to debug the code. Where they have just one, they indicate that the same model was used to both generate the code and debug it. Note that the y-axis is shown from 0.7 to 1; figures with the full axis (0 to 1) and error bars, robustness checks, and other details about our empirical results are included in the [appendix](https://www.cs.princeton.edu/~sayashk/papers/ai-leaderboards-appendix.pdf)._ [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#534b1868-3a4c-4d52-b013-5d01f1885a9b)  ^534b1868

> At first glance, reporting dollar costs is jarring. It breaks many properties of benchmarking that we take for granted: that measurements don’t change over time (whereas costs tend to come down) and that different models compete on a level playing field (whereas some developers may [benefit](https://papers.nips.cc/paper%5Ffiles/paper/2023/file/d1a14493e5f84d6c6129414f0cd1a7c6-Paper-Conference.pdf) from economies of scale, leading to lower inference costs). Because of this, researchers usually pick a different axis for the Pareto curve, such as parameter count. [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#f3d9cc2d-1b3a-4d08-bc67-d1fa09e77f46)  ^f3d9cc2d

> AI evaluations serve at least two distinct purposes. Model developers and AI researchers use them to identify which changes to the training data and architecture improve accuracy. We call this [model evaluation](https://arxiv.org/pdf/2211.09110). And downstream developers, such as programmers who use AI to build consumer-facing products, use evaluations to decide which AI systems to use in their products. We call this [downstream evaluation](https://www.arthur.ai/product/bench). [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#0baf5eb6-247f-40de-a45e-6fef0a31d222)  ^0baf5eb6

> Model evaluation is a scientific question of interest to researchers. So it makes sense to stay away from dollar costs for the aforementioned reasons. Instead, controlling for compute is a reasonable approach: if we normalize the amount of compute used to train a model, we can then understand if factors like architectural changes or changes in the data composition are responsible for improvements, as opposed to more compute. Notably, Nathan Lambert [argues](https://www.interconnects.ai/p/compute-efficient-open-llms) that many of the accuracy gains in the last year (such as Meta's Llama 2) are simply consequences of using more compute. [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#0eea49ef-7c90-4961-81cd-8c701c9151b9)  ^0eea49ef

> On the other hand, downstream evaluation is an engineering question that helps inform a procurement decision. Here, cost is the actual construct of interest. The downsides of cost measurement aren’t downsides at all; they are exactly what’s needed. Inference costs do come down over time, and that greatly matters to downstream developers. It is unnecessary and counterproductive for the evaluation to stay frozen in time.
> 
> In this context, proxies for cost (such as the number of active parameters or amount of compute used) are misleading. [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#5420da9f-0a14-40a7-ae75-cbcaedcea855)  ^5420da9f

> In this figure, the number of active parameters is a poor proxy for cost. On [Anyscale](https://docs.endpoints.anyscale.com/pricing/), Mixtral 8x7B costs twice as much as Llama 2 13B, yet Mistral's figure shows it costs about the same, because they only consider the number of active parameters. Of course, downstream developers don't care about the number of active parameters when they're using an API. They simply care about the dollar cost relative to accuracy. Mistral chose “active parameters” as a proxy, presumably because it makes their models look better than dense models such as Meta’s Llama and Cohere’s Command R+. If we start using proxies for cost, every model developer can pick a proxy that makes their model look good. [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#adfd5481-e107-4d62-bcb1-644f4addf2ea)  ^adfd5481

> **Reflexion, LDB, and LATS all use different subsets of HumanEval.** Three (out of 164) coding problems in the original version of HumanEval lack example tests. Since these agents require example tests to debug or rerun their solutions, Reflexion removes the three problems that don't have example tests. LATS removes these three problems, plus another problem, for unreported reasons.[17](https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful#footnote-17-144156093) LDB adds example tests for the three problems that are missing in the original benchmark. **None of the three papers reports this.** The paper introducing LATS claims (incorrectly): "We use all 164 problems for our experiments."[18](https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful#footnote-18-144156093) In our analysis, we conducted all evaluations on the version of the benchmark provided by LDB, since it contains example tests for all problems. [⤴️](https://omnivore.app/me/https-www-aisnakeoil-com-p-ai-leaderboards-are-no-longer-useful-18f30086af7#8bfa84c1-62cc-4286-8e7a-0e89c748cb96)  ^8bfa84c1

