---
id: 1c5c0998-9708-4b84-b09d-7cc82bd17c72
omnivore_error: There was an error parsing the front matter template. See console for details.
---

# "AI now beats humans at basic tasks": Really?

[Read on Omnivore](https://omnivore.app/me/ai-now-beats-humans-at-basic-tasks-really-18f542f8b7c)

[Read Original](https://aiguide.substack.com/p/ai-now-beats-humans-at-basic-tasks?triedRedirect=true)

## Highlights

> AI surpassing humans on a benchmark that is named after a general ability is not the same as AI surpassing humans on that general ability. [⤴️](https://omnivore.app/me/ai-now-beats-humans-at-basic-tasks-really-18f542f8b7c#46f30fdb-590c-4b23-9755-e596af864482)  ^46f30fdb

> 1\. **Data contamination:** The questions (and answers) from a given benchmark might have been part of the training data for the AI system. OpenAI, for example, has not released any information about the training data of their most advanced models, so we can’t check whether or not this is the case. [⤴️](https://omnivore.app/me/ai-now-beats-humans-at-basic-tasks-really-18f542f8b7c#efcb6b21-2ff4-4386-863a-44247f5d7e67)  ^efcb6b21

> 2\. **Over-reliance on training data:** The system might not have been trained on the benchmark itself, but on similar items that require similar patterns of reasoning, and the system might be relying on such patterns to solve benchmark items rather than using more general abstract reasoning. Multiple studies have shown that LLMs maybe rely on such “[approximate retrieval](https://cacm.acm.org/blogcacm/can-llms-really-reason-and-plan/)“ methods to solve problems. [⤴️](https://omnivore.app/me/ai-now-beats-humans-at-basic-tasks-really-18f542f8b7c#5f0a2146-d187-48c4-ba81-894a415305cd)  ^5f0a2146

> 3\. **Shortcut learning:** The AI system might be, in some cases, relying on spurious correlations or “shortcuts” in test items. As a particularly blatant case of this, [one study found](https://www.sciencedirect.com/science/article/pii/S0022202X18322930) that an AI system which had successfully classified malignant skin cancers from photographs was using the presence of a ruler in the images as an important cue (photos of nonmalignant tumors were less likely to include rulers). Another [study showed](https://arxiv.org/abs/1907.07355) that a system attained high performance on a “natural language inference” benchmark by learning that certain keywords were (unintentionally) correlated with correct answers. Many examples of such shortcut learning have been described in the machine learning literature, and it’s likely that large language models have unprecedented ability to discover subtle patterns of association in language that can predict correct answers in benchmarks without requiring the kind of abstract reasoning that humans are more likely to use. [⤴️](https://omnivore.app/me/ai-now-beats-humans-at-basic-tasks-really-18f542f8b7c#4d594ce8-3927-4520-844d-f1a2201bdfeb)  ^4d594ce8

> 4\. **Test validity:** performance on such benchmark might not correlate with performance in the real world, in the same way it does for humans. [⤴️](https://omnivore.app/me/ai-now-beats-humans-at-basic-tasks-really-18f542f8b7c#bca68bc0-5590-4dc2-af68-d9366222c647)  ^bca68bc0

