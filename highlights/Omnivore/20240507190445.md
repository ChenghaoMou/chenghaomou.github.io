---
id: d43a92c4-e688-4d21-82e1-4e71dee3e2d2
alias:
  - Can Large Language Models Reason? - by Melanie Mitchell
title: Can Large Language Models Reason? - by Melanie Mitchell
author: |
  Melanie Mitchell
date: 2024-05-07 19:04:45
url: https://aiguide.substack.com/p/can-large-language-models-reason
---

# Can Large Language Models Reason? - by Melanie Mitchell

[Read on Omnivore](https://omnivore.app/me/can-large-language-models-reason-by-melanie-mitchell-18f543ab5f9)

[Read Original](https://aiguide.substack.com/p/can-large-language-models-reason)

## Highlights

> For example, [a study by Razeghi et al.](https://aclanthology.org/2022.findings-emnlp.59.pdf) showed that some GPT-based LLMs (pre-trained on a known corpus) were much better at arithmetic problems that involved numbers that appeared frequently in the pre-training corpus than those that appeared less frequently. These models appear to lack a general ability for arithmetic, but instead rely on a kind of “memorization”—matching patterns of text they have seen in pre-training. The authors of that study point out that, in view of such results, “any evaluation of reasoning that does not take the pretraining data into account is difficult to interpret, and that we need to revisit evaluation of language models with respect to their pretraining data before making any conclusion about the models’ generalization abilities beyond the pretraining data.” As a stark example of this, Horace He, an undergraduate researcher at Cornell, [posted on Twitter](https://twitter.com/cHHillee/status/1635790330854526981) that on a dataset of programming challenges, GPT-4 solved 10 out of 10 problems that had been published before 2021 (GPT-4’s pre-training cutoff date) and zero out of 10 problems that had been published after 2021\. GPT-4’s success on the pre-2021 challenges thus seems to be due to memorizing problems seen in its training data rather than reasoning about the problems from scratch. [⤴️](https://omnivore.app/me/can-large-language-models-reason-by-melanie-mitchell-18f543ab5f9#b05e3ee6-ae6a-4b42-9629-a384d2bb439c)  ^b05e3ee6

> The authors noted that, although GPT-4’s (and other LLMs’) performance on counterfactual tasks was often above random chance, these results “demonstrate limitations in the \[LLM’s\] abstract capacity to solve the target task.” Just as LLMs are better on arithmetic problems with numbers that appear more frequently in their training data, they are much better on versions of reasoning tasks that are similar to those in data they have been trained on. [⤴️](https://omnivore.app/me/can-large-language-models-reason-by-melanie-mitchell-18f543ab5f9#63858224-9fe7-43cb-8b31-b1838f9d79eb)  ^63858224

> The question of “memorization” versus reasoning is not all-or-nothing; Wu et al. noted that in LLMs there is “some degree of reasoning that is transferable between the default and counterfactual worlds. \[Memorization and reasoning are\] not a dichotomy, but rather they can co-exist in a continuum.” However, if some form of memorization is largely driving LLM’s apparent reasoning abilities, this means that LLMs will not be robust in applying these abilities to versions of tasks that are different from those they have been trained on, which is a big concern for their application in the real world.  [⤴️](https://omnivore.app/me/can-large-language-models-reason-by-melanie-mitchell-18f543ab5f9#cc2efb84-f1ba-438b-b89c-d4f96b296697)  ^cc2efb84

