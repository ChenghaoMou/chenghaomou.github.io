---
id: dbc8ce4e-5f73-4aba-80da-083ae705970b
aliases:
  - How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?
title: "How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?"
author: |
  Sebastian Raschka, PhD
date: 2024-05-12 12:33:17
url: https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms
cssclasses:
  - reference
created: 2024-05-19
modified: 2024-05-20
---

# How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?

[Read on Omnivore](https://omnivore.app/me/https-magazine-sebastianraschka-com-p-how-good-are-the-latest-op-18f6c941449)

[Read Original](https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms)

## Highlights

> The main contributor to the substantially better performance compared to Llama 2 is the much larger dataset. Llama 3 was trained on 15 trillion tokens, as opposed to "only" 2 trillion for Llama 2.
> 
> This is a very interesting finding because, as the [Llama 3 blog post](https://ai.meta.com/blog/meta-llama-3/) notes, according to the Chinchilla scaling laws, the optimal amount of training data for an 8 billion parameter model is much smaller, approximately 200 billion tokens. Moreover, the authors of Llama 3 observed that both the 8 billion and 70 billion parameter models demonstrated log-linear improvements even at the 15 trillion scale. This suggests that we (that is, researchers in general) could further enhance the model with more training data beyond 15 trillion tokens. [⤴️](https://omnivore.app/me/https-magazine-sebastianraschka-com-p-how-good-are-the-latest-op-18f6c941449#27c5b98c-53f8-4729-8472-932fb77ea3d8)  ^27c5b98c

> _[Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/abs/2404.10719)_ is a well-written paper with lots of experiments and results, but the main takeaways are that PPO is generally better than DPO, and DPO suffers more heavily from out-of-distribution data. [⤴️](https://omnivore.app/me/https-magazine-sebastianraschka-com-p-how-good-are-the-latest-op-18f6c941449#b401959d-e8f6-41b8-ba94-fd33c2a69125)  ^b401959d

