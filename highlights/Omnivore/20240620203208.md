---
id: 73c3c171-70dd-4c55-84bb-dbaf5b6b085a
alias:
  - Optimizing AI Inference at Character.AI
title: Optimizing AI Inference at Character.AI
author: |
  Myle Ott
date: 2024-06-20 20:32:08
url: https://research.character.ai/optimizing-inference/
---

# Optimizing AI Inference at Character.AI

[Read on Omnivore](https://omnivore.app/me/https-research-character-ai-optimizing-inference-190372286ad)

[Read Original](https://research.character.ai/optimizing-inference/)

## Highlights

> **1\. Multi-Query Attention**. [⤴️](https://omnivore.app/me/https-research-character-ai-optimizing-inference-190372286ad#77a2aa4f-9b7a-431c-9b08-dfbe6d59ef55)  ^77a2aa4f

> **2\. Hybrid Attention Horizons**. We interleave local attention ([Beltagy et al., 2020](https://arxiv.org/abs/2004.05150v2?ref=research.character.ai)) with global attention layers. Local attention is trained with sliding windows, and reduces the complexity from O(length2) to O(length). We found that reducing attention horizon to 1024 on most attention layers does not have a significant impact on evaluation metrics, including the long context [needle-in-haystack](https://github.com/gkamradt/LLMTest%5FNeedleInAHaystack?ref=research.character.ai) benchmark. [⤴️](https://omnivore.app/me/https-research-character-ai-optimizing-inference-190372286ad#2b7492e6-20a8-4428-930c-ca25513d830c)  ^2b7492e6

> **3\. Cross Layer KV-sharing.** We tie the KV cache across neighboring attention layers, which further reduces KV cache size by a factor of 2-3x. [⤴️](https://omnivore.app/me/https-research-character-ai-optimizing-inference-190372286ad#a9c952c9-96ce-4aea-8954-0d176cc842ed)  ^a9c952c9

> Different from commonly adopted "post-training quantization" techniques, we natively train our models in int8 precision, eliminating the risk of training/serving mismatch while also significantly improving training efficiency. [⤴️](https://omnivore.app/me/https-research-character-ai-optimizing-inference-190372286ad#52e5edde-0314-48ff-9c7f-3415f5a177b4)  ^52e5edde

