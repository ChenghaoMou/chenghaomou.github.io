---
title: "How to Build an Enterprise LLM Application: Lessons From GitHub Copilot"
url: https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/
author: Shuyin Zhao
date: 2023-12-10
time: 11:22 AM
source: "reader"
aliases:
  - "How to Build an Enterprise LLM Application: Lessons From GitHub Copilot"
---
## Highlights
> For instance, when the GitHub Copilot team first decided to provide whole function coding suggestions, we also had to ensure output predictability and consistency, where the same prompt and context would produce the same suggestions from the AI model.
> To achieve this, the team applied two strategies: changing the parameters to reduce the randomness of outputs and caching responses. Additionally, using cached responses instead of generating new responses to the same prompt not only reduced variability in suggestions, but it also improved performance. ([View Highlight](https://read.readwise.io/read/01h9razw6psr2ht3tv2aw01ce6))

> Based on community input, the GitHub Copilot team also developed a [code reference tool](https://github.blog/2023-08-03-introducing-code-referencing-for-github-copilot/) that includes links to public code that may match GitHub Copilot suggestions, so developers can review potential matches (and relevant licensing information), and make informed choices. ([View Highlight](https://read.readwise.io/read/01h9rb3ne8rvt2781qtsr5hr04))

