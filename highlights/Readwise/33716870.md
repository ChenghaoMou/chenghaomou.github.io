---
title: "33716870"
url: https://magazine.sebastianraschka.com/p/ahead-of-ai-12-llm-businesses
author: Sebastian Raschka, PhD
date: 2023-12-10
time: 11:22 AM
source: "reader"
aliases:
  - "Ahead of AI #12: LLM Businesses and Busyness"
---
## Highlights
> On that note, in the satirical [Pretraining on the Test Set Is All You Need](https://arxiv.org/abs/2309.08632) paper, the author trains a small 1M parameter LLM that outperforms all other models, including the 1.3B phi-1.5 model. This is achieved by training the model on all downstream academic benchmarks. It appears to be a subtle criticism underlining how easily benchmarks can be "cheated" intentionally or unintentionally (due to data contamination). ([View Highlight](https://read.readwise.io/read/01he2a904tf916xmyvfbqgsb8n))