---
title: "34910990"
url: https://blog.j11y.io/2023-11-22_multifaceted/
author: j11y.io
date: 2023-12-10
time: 11:22 AM
source: "reader"
aliases:
  - "Multifaceted: The Linguistic Echo Chambers of LLMs"
---
## Highlights
> I also couldn't ignore the fact that Quora has lately been embedding a ChatGPT widget on almost every page, and this widget's content is pre-generated, static and available for crawling. It is thus liable to being used as additional training material for this and other LLMs. ([View Highlight](https://read.readwise.io/read/01hgr9y8n562sdc798g1anhjbm))

> Focusing the training on any particular website will lead to strong biases. For example, fixating too much on academic material or websites like Quora where bots formulaically re-use certain phrases (this occurred even in the era before LLMs).
> Furthermore, since these models have taken off in popularity, and people have then been publishing their outputs back onto the internet. As this occurs, it's likely produced a feedback loop. LLMs are unknowingly training on their own regurgitated outputs. It's unavoidable.
> So, by those very tiny initial training decisions, just a handful of engineers have begun a unstoppable chain of incestuous linguistic evolution. It is fascinating how powerful these models are becoming in shifting the nature of language itself. ([View Highlight](https://read.readwise.io/read/01hgra2bv8pe8c0jqk3kj717a0))---
title: "Multifaceted: The Linguistic Echo Chambers of LLMs"
url: https://blog.j11y.io/2023-11-22_multifaceted/
author: j11y.io
date: 2024-03-02
time: 10:01 PM
source: "reader"
aliases:
  - "Multifaceted: The Linguistic Echo Chambers of LLMs"
---
# Multifaceted: The Linguistic Echo Chambers of LLMs

## Highlights
> I also couldn't ignore the fact that Quora has lately been embedding a ChatGPT widget on almost every page, and this widget's content is pre-generated, static and available for crawling. It is thus liable to being used as additional training material for this and other LLMs. ([View Highlight](https://read.readwise.io/read/01hgr9y8n562sdc798g1anhjbm))

> Focusing the training on any particular website will lead to strong biases. For example, fixating too much on academic material or websites like Quora where bots formulaically re-use certain phrases (this occurred even in the era before LLMs).
> Furthermore, since these models have taken off in popularity, and people have then been publishing their outputs back onto the internet. As this occurs, it's likely produced a feedback loop. LLMs are unknowingly training on their own regurgitated outputs. It's unavoidable.
> So, by those very tiny initial training decisions, just a handful of engineers have begun a unstoppable chain of incestuous linguistic evolution. It is fascinating how powerful these models are becoming in shifting the nature of language itself. ([View Highlight](https://read.readwise.io/read/01hgra2bv8pe8c0jqk3kj717a0))

