---
title: "35423015"
url: https://twitter.com/karpathy/status/1733299213503787018
author: Andrej Karpathy
date: 2023-12-14
time: 9:26 PM
source: "reader"
aliases:
  - '# On the "hallucination problem"'
---
## Highlights
> There are many ways to mitigate hallcuinations in these systems - using Retrieval Augmented Generation (RAG) to more strongly anchor the dreams in real data through in-context learning is maybe the most common one. Disagreements between multiple samples, reflection, verification chains. Decoding uncertainty from activations. Tool use. All an active and very interesting areas of research. ([View Highlight](https://read.readwise.io/read/01hhmzjj2x07dpvr8j28wc26pk))

> I know I'm being super pedantic but the LLM has no "hallucination problem". Hallucination is not a bug, it is LLM's greatest feature. The LLM Assistant has a hallucination problem, and we should fix it. ([View Highlight](https://read.readwise.io/read/01hhmzk48kawr96cwa18e4fmrq))---
title: "# On the "hallucination problem""
url: https://twitter.com/karpathy/status/1733299213503787018
author: Andrej Karpathy
date: 2024-03-02
time: 10:01 PM
source: "reader"
aliases:
  - "# On the "hallucination problem""
---
# # On the "hallucination problem"

## Highlights
> There are many ways to mitigate hallcuinations in these systems - using Retrieval Augmented Generation (RAG) to more strongly anchor the dreams in real data through in-context learning is maybe the most common one. Disagreements between multiple samples, reflection, verification chains. Decoding uncertainty from activations. Tool use. All an active and very interesting areas of research. ([View Highlight](https://read.readwise.io/read/01hhmzjj2x07dpvr8j28wc26pk))

> I know I'm being super pedantic but the LLM has no "hallucination problem". Hallucination is not a bug, it is LLM's greatest feature. The LLM Assistant has a hallucination problem, and we should fix it. ([View Highlight](https://read.readwise.io/read/01hhmzk48kawr96cwa18e4fmrq))

