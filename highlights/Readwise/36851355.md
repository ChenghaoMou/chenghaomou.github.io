---
title: "Linear Transformers Are Faster After All"
url: https://manifestai.com/blogposts/faster-after-all/
author: JoelEinbinder
date: 2024-01-21
time: 10:28 AM
source: "reader"
aliases:
  - Linear Transformers Are Faster After All
---
# Linear Transformers Are Faster After All

## Highlights
> These results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,[1](https://manifestai.com/blogposts/faster-after-all/#fn1) as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning. ([View Highlight](https://read.readwise.io/read/01hmk4tds5mcas0f5q2ctb4h3d))