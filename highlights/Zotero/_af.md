---
aliases:
  - Apple Intelligence Foundation Language Models
  - Apple Intelligence Foundation Language Models
authors: ""
date: '2024-08-01 15:05:11'
tags: []
title: "Apple Intelligence Foundation Language Models"
url: ""
created: 2024-02-03
modified: 2024-08-03
---

# Apple Intelligence Foundation Language Models

> • A shared input/output embedding matrix [Press and Wolf, 2016] to reduce memory usage for parameters. • Pre-Normalization [Nguyen and Salazar, 2019] with RMSNorm [Zhang and Sennrich, 2019] for training stability. • Query/key normalization [Wortsman et al., 2023] to improve training stability. • Grouped-query attention (GQA) [Ainslie et al., 2023] with 8 key-value heads to reduce the KV-cache memory footprint. • The SwiGLU activation [Shazeer, 2020] for higher efficiency. • RoPE [Su et al., 2024] positional embeddings with the base frequency set to 500k for long-context support. [(p. 2)](zotero://open-pdf/library/items/LKR83NME?page=2)

> We respect the right of webpages to opt out of being crawled by Applebot, using standard robots.txt directives. [(p. 3)](zotero://open-pdf/library/items/LKR83NME?page=3)

> Plus, we take steps to exclude pages containing profanity and apply filters to remove certain categories of personally identifiable information (PII). The remaining documents are then processed by a pipeline which performs quality filtering and plain-text extraction, more specifically: 1. Body extraction is performed using a combination of Safari’s reader mode and the Boilerpipe [Kohlschütter et al., 2010] algorithm. 2. Safety and profanity filtering, using heuristics and model-based classifiers. 3. Global fuzzy de-duplication using locality-sensitive n-gram hashing. 4. Extensive quality filtering using heuristics and model-based classifiers [Kong et al., 2024; Li et al., 2024a]. 5. Decontamination against 811 common pre-training benchmarks, filtering entire documents upon 4-13 gram collisions with any of the benchmark datasets, unless the collision-count for a given n-gram reaches a “commonusage&quot; threshold of 1000. [(p. 4)](zotero://open-pdf/library/items/LKR83NME?page=4)

> Our post-training process contains two stages: supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). We present two new post-training algorithms: (1) a rejection sampling fine-tuning algorithm with teacher committee (iTeC), and (2) a reinforcement learning from human feedback (RLHF) algorithm with mirror descent policy optimization and a leave-one-out advantage estimator (MDLOO) that are used on our reinforcement learning iterations and lead to significant model quality improvements. [(p. 8)](zotero://open-pdf/library/items/LKR83NME?page=8)

> In order to tune the mixture weight, we treat it as an optimization problem. Specifically, given a set of weights (w1, w2, ..., wn) where wi represents the ratio of a specific component in the mixture, we train a model with wi → wi ± ∆wi and evaluate the quality change on a set of benchmarks. We find that extensively running such experiments can effectively identify the best mixture and remove the least impactful data components. [(p. 11)](zotero://open-pdf/library/items/LKR83NME?page=11)

> we set up a collection of latest promising models trained from SFT, RS, DPO/IPO, and RL, as well as best models from the previous iterations, which we refer to as “model committee”. We collect pairwise human preference on responses sampled from the latest model committee. After acquiring each batch of human preference data, we refresh our reward model, and further train a new set of models using the collection of preference optimization algorithms. We then continue the next round of iterative RLHF data collection with a new model committee. [(p. 13)](zotero://open-pdf/library/items/LKR83NME?page=13)

> Specifically, for each prompt, we sample multiple responses from each model in the committee, and use the latest reward model to select the best response for each prompt. This allows us to combine the advantages of models trained by different preference optimization algorithms. For instance, we find that algorithms that leverage negative examples, e.g., online RLHF, DPO, IPO, to be better in improving reasoning skills such as math, while rejection sampling fine-tuning learns instruction following and writing skills more effectively. [(p. 13)](zotero://open-pdf/library/items/LKR83NME?page=13)

> More specifically, during the decoding stage of the algorithm, we decode multiple responses for each prompt, and assign the advantage of each response to be the difference of the reward of the (prompt, response) pair and the mean reward of the other responses generated by the same prompt. Intuitively, this estimator aims to measure how much better a response is compared to a typical response. [(p. 14)](zotero://open-pdf/library/items/LKR83NME?page=14)

> Therefore, instead of directly passing the quantized model to application teams for feature development, we attach a set of parameter-efficient LoRA adapters for quality recovery. We make sure that these LoRA adapters training recipes are consistent with pre-training and post-training processes. Then, products will fine-tune their own feature-specific LoRA adapters by initializing the adapter weights from the accuracy-recovery adapters, while keeping the quantized base model frozen. [(p. 16)](zotero://open-pdf/library/items/LKR83NME?page=16)

> Residual connections exist in every transformer block and every layer in AFM. So it is unlikely that all layers have the equal importance. Following this intuition, we further reduce the memory usage by pushing some layers to use 2-bit quantization (default is 4-bit). On average, AFM-on-device can be compressed to only about 3.5 bits per weight (bpw) without significant quality loss. We choose to use 3.7 bpw in production as it already meets the memory requirements. [(p. 17)](zotero://open-pdf/library/items/LKR83NME?page=17)

```
@article{Apple Intelligence Foundation Language Models, language={en} }
```