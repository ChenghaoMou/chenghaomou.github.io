---
title: "The Surprising Effectiveness of Test-Time Training for Abstract Reasoning"
alias:
- "The Surprising Effectiveness of Test-Time Training for Abstract Reasoning"
created: 2024-11-30 14:54:03:988859
updated: 2024-11-14 20:53:44:755000
modified: 2024-11-30 14:54:03:988866
authors: ['Ekin Akyürek']
zotero_url: zotero://open-pdf/library/items/NFDQBG6I
tags:
- reMarkable
---

# The Surprising Effectiveness of Test-Time Training for Abstract Reasoning
[Open in Zotero](zotero://open-pdf/library/items/NFDQBG6I)
## Abstract

Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. We investigate the effectiveness of test-time training (TTT)—updating model parameters temporarily during inference using a loss derived from input data—as a mechanism for improving models’ reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, we identify three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6× improvement in accuracy compared to base fine-tuned models; applying TTT to a 8B-parameter language model, we achieve 53% accuracy on the ARC’s public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches. By ensembling our method with recent program generation approaches, we get SoTA public validation accuracy of 61.9%, matching the average human score. Our findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective.
## Pages
### Page 1
**Highlights**:

<mark style="background-color: rgba(190, 234, 254, 255)">We identify several crucial ingredients for effective application of TTT to few-shot learning: (1) initialfine-tuning on synthetic tasks similar to those encountered at test time, (2) an augmented, leave-one-outtask generation strategy for constructing the test-time dataset, (3) per-instance adapter training and (4) aself-consistency (Wang et al., 2022) approach under invertible transformations. With careful choices of thesecomponents, TTT can significantly improve LM performance on ARC—increasing accuracy by up to a factorof six over a 1B model, and achieving state-of-the-art results for published, purely neural models on the ARCtask with a 8B model. Indeed, our results show that when equipped with test-time training, ordinary LMscan match or exceed the performance of many neuro-symbolic approaches on ARC.</mark>
### Page 3
**Highlights**:

![Image (page 3)](statics/cd859fd80556/tmp95mo2u28.png)
### Page 5
**Highlights**:

<mark style="background-color: rgba(190, 234, 254, 255)">Note that we include loss terms fordemonstrations starting from the second example (n = 2). By doing so, we encourage the model to startreasoning about the transformation pattern from the second demonstration pair itself.</mark>
### Page 6
**Highlights**:

<mark style="background-color: rgba(190, 234, 254, 255)">Results are presented in Fig. 3. Our TTT method is effective, improving fine-tuned model accuracy approximately 6× (5 → 29). The structure of the auxilary task significantly impact TTT effectiveness. Usingin-context learning tasks substantially outperforms using end-to-end tasks, showing a 11 (38% decrease)tasks relative performance drop under identical conditions. This may be simply due to training less parameters. Dropping transformations applied to augment data hurts by 16 tasks (55% decrease).</mark>
<mark style="background-color: rgba(190, 234, 254, 255)">Next, we ablate multiple components of TTT optimization to analyze their contribution to the performance.Learning a single LoRA adapter across all tasks reduces performance on 7 tasks (24% decrease). This isexpected as learning a dedicated adapter allows more parameters to train per task. Second, the decision thatwe made by taking loss on the output demonstrations marginally improves the performance (26 → 29), aswe believe that this forces the model to reason about the transformation while processing the demonstrations.Finally, we observe that using quantized LoRA (QLoRA) only leads to a marginal drop in performance(29 → 26) — in memory-bottlenecked scenarios using QLoRA may be viable.</mark>
![Image (page 6)](statics/cd859fd80556/tmpgf9pssss.png)
### Page 8
**Highlights**:

<mark style="background-color: rgba(190, 234, 254, 255)">formed versions is generally poor, with the transpose transformation yielding the worst accuracy. However,aggregating across these transformations through voting procedures leads to significant improvements. Thissuggests that some tasks may be easier to solve in their transformed versions, and that using self-consistency(voting) for aggregation is generally beneficial, a finding also observed in prior work. Additionally, whilethe flattened voting procedure improves accuracy, our hierarchical voting procedure outperforms it. In fact,our hierarchical procedure is comparable to the oracle, indicating that hierarchical aggregation effectivelyselects the correct answer (when it exists) with high accuracy.</mark>
### Page 10
**Highlights**:

<mark style="background-color: rgba(190, 234, 254, 255)">dataset. Surprisingly, excluding LM-generated data from fine-tuning actually outperforms the model trainedon all data.</mark>