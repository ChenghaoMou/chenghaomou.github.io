---
aliases:
  - "SoK: On the Impossible Security of Very Large Foundation Models"
  - "SoK: On the Impossible Security of Very Large Foundation Models"
authors: "El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Lê-Nguyên Hoang, Rafael Pinot, John Stephan"
date: '2023-04-15 21:50:07'
linter-yaml-title-alias: "SoK: On the Impossible Security of Very Large Foundation Models"
order: -1
tags: []
title: "SoK: On the Impossible Security of Very Large Foundation Models"
url: "http://arxiv.org/abs/2209.15259"
---

# SoK: On the Impossible Security of Very Large Foundation Models

## Abstract

Large machine learning models, or so-called foundation models, aim to serve as base-models for application-oriented machine learning. Although these models showcase impressive performance, they have been empirically found to pose serious security and privacy issues. We may however wonder if this is a limitation of the current models, or if these issues stem from a fundamental intrinsic impossibility of the foundation model learning problem itself. This paper aims to systematize our knowledge supporting the latter. More precisely, we identify several key features of today's foundation model learning problem which, given the current understanding in adversarial machine learning, suggest incompatibility of high accuracy with both security and privacy. We begin by observing that high accuracy seems to require (1) very high-dimensional models and (2) huge amounts of data that can only be procured through user-generated datasets. Moreover, such data is fundamentally heterogeneous, as users generally have very specific (easily identifiable) data-generating habits. More importantly, users' data is filled with highly sensitive information, and maybe heavily polluted by fake users. We then survey lower bounds on accuracy in privacy-preserving and Byzantine-resilient heterogeneous learning that, we argue, constitute a compelling case against the possibility of designing a secure and privacy-preserving high-accuracy foundation model. We further stress that our analysis also applies to other high-stake machine learning applications, including content recommendation. We conclude by calling for measures to prioritize security and privacy, and to slow down the race for ever larger models.

<mark style="background: #ff6666">Typically, GPT-3’s anti-Muslim bias can be argued to be (partly) the result of anti-Muslim propaganda, which has been found to be massively scaled, by both human troll farms and (foundation-model-based) algorithms like Tek Fog in India [97], [98]. Recall that, in 2019 alone, Facebook removed 6 billion fake accounts from its platform [63] (and the development of generative models and evasion attacks [16], [68] will likely make this worse).</mark> [(p. 2)](zotero://open-pdf/library/items/S5MNGT6Y?page=2)

<mark style="background: #ff6666">Yet, when produced by genuine humans, language data are very user-specific. Genuine humans’ data are thus fundamentally heterogeneous, in the sense that different genuine users have different preferred phrase completion. Moreover, the statistical word distribution is wellknown to be heavy-tailed [123], [149], and each user provides only a sparse dataset (i.e., not fully representative of all the ways the user would speak), leading to additional empirical heterogeneity. As we will see, such data heterogeneities are a core cause of the fragility of foundation models, especially when the data is sensitive and might be fabricated by fake accounts [20], [137], [191].</mark> [(p. 2)](zotero://open-pdf/library/items/S5MNGT6Y?page=2)

<mark style="background: #5fb236">Unfortunately, verified texts seem insufficient to reach state-of-the-art performance. Indeed, the English Wikipedia only contains around 4 billion words [189]. Meanwhile, a book has around 105 words. While there are 108 books [122], only a fraction of them are arguably trustworthy. Many books are instead full of biases and dangerous misinformation, such as ethnic-based hate speech, historical propaganda, or outdated (possibly harmful) medical advice. As a striking illustration, up to the 1980s, the American Psychiatric Association listed homosexuality as a mental illness in its flagship manual [170]. In fact, most books should be regarded as unverified user-generated data.</mark> [(p. 3)](zotero://open-pdf/library/items/S5MNGT6Y?page=3)

<mark style="background: #5fb236">Of course, to increase security, as proposed by [155], we could demand that foundation models restrict themselves to quality datasets only. However, such datasets will inevitably be of significantly smaller size. As discussed earlier, this will likely greatly harm the performance of foundation models. In fact, this is the main claim of this paper: security demands a drastic reduction of performance.</mark> [(p. 3)](zotero://open-pdf/library/items/S5MNGT6Y?page=3)

<mark style="background: #5fb236">Such foundation models must arguably be able to adapt to a greater variety of contexts than what any single human will ever encounter in their human life. As a result, the complexity of “fully satisfactory” language processing might need to be orders of magnitude larger than today’s foundation models, in which case we may still obtain greater accuracy by training larger models.</mark> [(p. 4)](zotero://open-pdf/library/items/S5MNGT6Y?page=4)

<mark style="background: #ff6666">To understand, consider the following intuitive consideration. In the case of linear or logistic regression, each data acts on the model parameters on a single dimension. Thus, if the model has more dimensions than there are data points, then many dimensions will be under the influence of no data. This makes such dimensions extremely vulnerable to a data poisoning attack. Moreover, more generally, the more we are in a regime d ≫ |D|, the more it may hold that most dimensions can be arbitrarily hacked in this manner.</mark> [(p. 4)](zotero://open-pdf/library/items/S5MNGT6Y?page=4)

<mark style="background: #5fb236">We stress that this heterogeneity in the users’ labeling functions can be regarded as a fundamental heterogeneity, as it would still remain even if all users labeled an infinite amount of times the same inputs. This heterogeneity highlights an irreconcilable disagreement between users over which foundation model should be learned. While some users would prefer to complete the sentence “the greatest of all time tennis player is” by “Roger Federer”, others would prefer to complete it by “Novak Djokovic”, or by “Rafael Nadal”. This is sharp contrast with image classification and language emotion classification tasks, where different users usually label a single image or text similarly. This makes accurately learning a distribution of texts much more dangerous. On one hand, the model would be able to map users’ names to what they write, which is a major privacy concern. On the other hand, it would then be easier for malicious users to be hardly discernible from most other genuine users, while providing very dangerous texts to replicate.</mark> [(p. 4)](zotero://open-pdf/library/items/S5MNGT6Y?page=4)

<mark style="background: #5fb236">Data is often signed (and if it is not, then it should be regarded as highly untrustworthy). In fact, it is commonly accepted that the traceability of data sources is a critical security condition [110], [139], as well as a powerful epistemological tools [9]. Thus, our setting allows us to work under the arguably realistic assumption that, if some data from user n’s dataset Dn are known to be harmfully crafted, then the entire dataset Dn is likely to be untrustworthy as well. Unfortunately, the study on data poisoning with signed data has been lacking.</mark> [(p. 5)](zotero://open-pdf/library/items/S5MNGT6Y?page=5)

<mark style="background: #ff6666">Unfortunately, homogeneity is an unrealistic assumption for the training of foundation models. Put differently, the fundamental vulnerability of foundation models is tightly connected to the fundamental heterogeneity in the way different users speak and write, and to the additional empirical heterogeneity due to the users’ limited datasets (which cannot be representative of the full distribution from which the users draw their texts and speeches). These data are not drawn from a fixed common data distribution.</mark> [(p. 6)](zotero://open-pdf/library/items/S5MNGT6Y?page=6)

<mark style="background: #5fb236">Overall, given the huge (financial) stakes of the rushed deployment of privacy-violating foundation models, we urgently call the scientific community to adopt a significantly increased rigor when reviewing the positive claims of (differential) privacy in machine learning in general, and in training large models in particular. Large technology companies have been known to ask their researchers to “strike a positive tone” [39] and to skew the message of their scientific publications13, in a manner unfortunately reminiscent of previous scientific disinformation campaigns led by, e.g., the tobacco, sugar and oil industries [140], [141].</mark> [(p. 8)](zotero://open-pdf/library/items/S5MNGT6Y?page=8)

<mark style="background: #5fb236">Yet, what a foundation model has learned from one phone, may be used to provide autocompletion on other users’ phones. Even if each phone is using a personalized model, as long as the models are large enough, lower bounds such as in Theorem 1 imply a large value of the privacy guarantee ε, thus practically no privacy and ease of attacks.</mark> [(p. 10)](zotero://open-pdf/library/items/S5MNGT6Y?page=10)

<mark style="background: #ff6666">If trained on large amounts of unsafe data, such algorithms may thereby be manipulated into promoting (harmful) product consumption, autocratic power, warmongering and radicalized convictions, which could fuel dangerous movements worldwide. Their vulnerability should not be neglected, especially for continuously learning conversational algorithms like Facebook’s Blender Bot 2.0 [187], [105]. Conversely, there is a high risk that some owners of these algorithms exploit them to favor their own cause, e.g. to subtly support their (dis)information war by inducing small biases in their foundation models.</mark> [(p. 11)](zotero://open-pdf/library/items/S5MNGT6Y?page=11)

<mark style="background: #ff6666">However, there is currently no reliable and robust solution to the alignment problem, and a strong theory of robust alignment for foundation models is arguably lacking. In fact, what may be most lacking today is a large-scale secure database of reliable human judgments to solve alignment [81].</mark> [(p. 12)](zotero://open-pdf/library/items/S5MNGT6Y?page=12)

## Notes

---
Comment: 13 pages

```
@article{El-Mhamdi_Farhadkhani_Guerraoui_Gupta_Hoang_Pinot_Stephan_2022, title={SoK: On the Impossible Security of Very Large Foundation Models}, url={[http://arxiv.org/abs/2209.15259](http://arxiv.org/abs/2209.15259)}, DOI={[10.48550/arXiv.2209.15259](https://doi.org/10.48550/arXiv.2209.15259)}, abstractNote={Large machine learning models, or so-called foundation models, aim to serve as base-models for application-oriented machine learning. Although these models showcase impressive performance, they have been empirically found to pose serious security and privacy issues. We may however wonder if this is a limitation of the current models, or if these issues stem from a fundamental intrinsic impossibility of the foundation model learning problem itself. This paper aims to systematize our knowledge supporting the latter. More precisely, we identify several key features of today’s foundation model learning problem which, given the current understanding in adversarial machine learning, suggest incompatibility of high accuracy with both security and privacy. We begin by observing that high accuracy seems to require (1) very high-dimensional models and (2) huge amounts of data that can only be procured through user-generated datasets. Moreover, such data is fundamentally heterogeneous, as users generally have very specific (easily identifiable) data-generating habits. More importantly, users’ data is filled with highly sensitive information, and maybe heavily polluted by fake users. We then survey lower bounds on accuracy in privacy-preserving and Byzantine-resilient heterogeneous learning that, we argue, constitute a compelling case against the possibility of designing a secure and privacy-preserving high-accuracy foundation model. We further stress that our analysis also applies to other high-stake machine learning applications, including content recommendation. We conclude by calling for measures to prioritize security and privacy, and to slow down the race for ever larger models.}, note={arXiv:2209.15259 [cs]}, number={arXiv:2209.15259}, publisher={arXiv}, author={El-Mhamdi, El-Mahdi and Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Hoang, Lê-Nguyên and Pinot, Rafael and Stephan, John}, year={2022}, month={Sep} }
```

%% Import Date: 2023-04-15T21:50:12.597-07:00 %%
