---
aliases:
  - "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
  - "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
authors: Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, Xing Sun
date: 2024-08-28 14:05:38
tags:
  - later
title: "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
url: https://arxiv.org/abs/2408.05211v1
created: 2024-02-03
modified: 2024-09-05
---

# VITA: Towards Open-Source Interactive Omni Multimodal LLM

## Abstract

The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. To the best of our knowledge, we are the first to exploit non-awakening interaction and audio interrupt in MLLM. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: https://vita-home.github.io.

> The deployment of VITA employs a duplex scheme, where one model is responsible for generating responses to user queries, and the other continuously tracks environmental inputs, selectively outputting new responses with updated interactions. This allows VITA to feature impressive human-machine interaction functionalities such as non-awakening interaction and audio interrupt interaction. [(p. )](zotero://open-pdf/library/items/7ILXTZI3?page=)

> The data source in the instruction tuning phase are same as the alignment phase in Table 1, and we make the following improvements: (1) the questions are randomly (about half) replaced with their audio versions, using TTS technique such as GPT-SoVITS6, to enhance the modelâ€™s understanding of audio queries and its instruction following capabilities. The number of audio questions and text questions can be found in Table 1. [(p. 6)](zotero://open-pdf/library/items/7ILXTZI3?page=6)

```
@misc{Fu_Lin_Long_Shen_Zhao_Zhang_Wang_Yin_Ma_Zheng_et al._2024, title={VITA: Towards Open-Source Interactive Omni Multimodal LLM}, url={[https://arxiv.org/abs/2408.05211v1](https://arxiv.org/abs/2408.05211v1)}, abstractNote={The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. To the best of our knowledge, we are the first to exploit non-awakening interaction and audio interrupt in MLLM. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: https://vita-home.github.io.}, note={0 citations (Semantic Scholar/arXiv) [2024-08-13]}, journal={arXiv.org}, author={Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Wang, Xiong and Yin, Di and Ma, Long and Zheng, Xiawu and He, Ran and Ji, Rongrong and Wu, Yunsheng and Shan, Caifeng and Sun, Xing}, year={2024}, month=aug, language={en} }
```