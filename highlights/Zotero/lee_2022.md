---
aliases:
  - Do Language Models Plagiarize?
  - "Do Language Models Plagiarize?"
authors: "Jooyoung Lee, Thai Le, Jinghui Chen, Dongwon Lee"
date: 2023-01-10 19:24:30
linter-yaml-title-alias: Do Language Models Plagiarize?
order: -1
tags: 
title: Do Language Models Plagiarize?
url: "http://arxiv.org/abs/2203.07618"
---

# Do Language Models Plagiarize?

## Abstract

Past literature has illustrated that language models do not fully understand the context and sensitivity of text and can sometimes memorize phrases or sentences present in their training sets. In this paper, we investigate whether they not only memorize but also plagiarize training samples when generating artificial texts. Our findings support that they, especially GPT-2, reuse particular pieces of texts from the training corpus with or without obfuscation. We have four main results: 1) language models with more capacity plagiarize more; 2) fine-tuned language models demonstrate differing patterns of plagiarism based on characteristics of auxiliary data; 3) sampling from truncated language modeling distributions tends to heighten the degree of plagiarism as opposed to temperature sampling, and 4) plagiarism in language models can have serious privacy consequences. Overall, our work implies that future research on neural language models should take precautions to avoid models plagiarizing their training datasets.

<mark style="background: #00ff00">We have four main results: 1) language models with more capacity plagiarize more; 2) fine-tuned language models demonstrate differing patterns of plagiarism based on characteristics of auxiliary data; 3) sampling from truncated language modeling distributions tends to heighten the degree of plagiarism as opposed to temperature sampling, and 4) plagiarism in language models can have serious privacy consequences.</mark> [(p. 1)](zotero://open-pdf/library/items/B7TTV6JZ?page=1)

<mark style="background: #00ff00">A majority of datasets used to train language models are scraped from the Internet without receiving informed consent from content owners (Brown et al., 2022). That being said, memorization from training samples can be perceived as a violation of copyright and authorship. Other than copying and pasting training sequences, there are other ways to indirectly exploit training examples by paraphrasing or summarizing the original content. This action generally refers to plagiarism, the act of reusing another person’s work without referencing the individual as its owner (Ali et al., 2011).</mark> [(p. 1)](zotero://open-pdf/library/items/B7TTV6JZ?page=1)

<mark style="background: #ff8000">Our study is guided by two research questions: (RQ1) Do pre-trained language models plagiarize? and (RQ2) Do finetuned language models plagiarize?.</mark> [(p. 2)](zotero://open-pdf/library/items/B7TTV6JZ?page=2)

<mark style="background: #00ff00">1) Model size: Amongst four GPT-2 family, larger models (GPT-2 large and xl) plagiarize more from a training set than smaller models; 2) Fine-tuning Data: There is a positive correlation between document similarity levels between pre-training and fine-tuning sets and plagiarism; 3) Decoding methods and values of their parameters: Plagiarism cases differ depending on decoding approaches and parameter values.</mark> [(p. 2)](zotero://open-pdf/library/items/B7TTV6JZ?page=2)

<mark style="background: #ff8000">Plagiarism occurs when any content including text, source code, or audio-visual content is reused without permission or citation from an author of original work. It has been a longstanding problem, especially in educational and research institutions or publishers, given the availability of digital artifacts (Sutherland-Smith, 2008; Clarke, 2006).</mark> [(p. 3)](zotero://open-pdf/library/items/B7TTV6JZ?page=3)

<mark style="background: #ffff00">In this work, we focus on three plagiarism types: • Verbatim plagiarism: exact copies of words or phrases without transformation. • Paraphrase plagiarism: synonymous substitution, word reordering, and back translation. • Idea plagiarism: reuse of the core idea by shortening or summarizing the original content</mark> [(p. 4)](zotero://open-pdf/library/items/B7TTV6JZ?page=4)

<mark style="background: #ff0000">Since OpenAI has not publicly released WebText, we use OpenWebText which is an opensource recreation of the WebText corpus.8 Given that the size of OpenWebtext corpus matches the size described in Radford et al. (2019), we assume it is a reliable source.</mark> [(p. 5)](zotero://open-pdf/library/items/B7TTV6JZ?page=5)

<mark style="background: #ff0000">We set a confidence threshold to 0.7. A total number of plagiarized documents that reveal PII entities is displayed in Figure 2. Of 1,736 plagiarized sequences, nearly 26% include at least one element of location information and a person’s full name. Although none of the highly sensitive information, including individuals’ driver’s license number, credit card information, bank number, social security number, and IP address, are revealed, the results show a possibility of machine-generated texts disseminating personal data such as phone number and email address not only through exact copying but also through paraphrasing.</mark> [(p. 6)](zotero://open-pdf/library/items/B7TTV6JZ?page=6)

```
@article{Lee_Le_Chen_Lee_2022, title={Do Language Models Plagiarize?}, url={[http://arxiv.org/abs/2203.07618](http://arxiv.org/abs/2203.07618)}, DOI={[10.48550/arXiv.2203.07618](https://doi.org/10.48550/arXiv.2203.07618)}, abstractNote={Past literature has illustrated that language models do not fully understand the context and sensitivity of text and can sometimes memorize phrases or sentences present in their training sets. In this paper, we investigate whether they not only memorize but also plagiarize training samples when generating artificial texts. Our findings support that they, especially GPT-2, reuse particular pieces of texts from the training corpus with or without obfuscation. We have four main results: 1) language models with more capacity plagiarize more; 2) fine-tuned language models demonstrate differing patterns of plagiarism based on characteristics of auxiliary data; 3) sampling from truncated language modeling distributions tends to heighten the degree of plagiarism as opposed to temperature sampling, and 4) plagiarism in language models can have serious privacy consequences. Overall, our work implies that future research on neural language models should take precautions to avoid models plagiarizing their training datasets.}, note={1 citations (Semantic Scholar/arXiv) [2023-01-05] 1 citations (Semantic Scholar/DOI) [2023-01-05] arXiv:2203.07618 [cs]}, number={arXiv:2203.07618}, publisher={arXiv}, author={Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon}, year={2022}, month={Mar} }
```

%% Import Date: 2023-01-10T19:25:45.938-08:00 %%
