---
aliases:
  - Mega: Moving Average Equipped Gated Attention
  - "Mega: Moving Average Equipped Gated Attention"
authors: "Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, Luke Zettlemoyer"
date: '2023-06-25 18:54:25'
linter-yaml-title-alias: "Mega: Moving Average Equipped Gated Attention"
order: -1
tags:
title: "Mega: Moving Average Equipped Gated Attention"
url: "http://arxiv.org/abs/2209.10655"
---

# Mega: Moving Average Equipped Gated Attention

## Abstract
The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.

<mark style="background: #ff6666">However, there are two common drawbacks in the design of attention mechanism: i) weak inductive bias; and ii) quadratic computational complexity. First, the attention mechanism does not assume prior knowledge of the patterns of dependencies between tokens (e.g. positional inductive bias), instead learning to predict the pairwise attention weights directly from data. Second, the cost to compute and store the attention weights is quadratic in the length of the input sequences.</mark> [(p. 2)](zotero://open-pdf/library/items/6CACZM4C?page=2)

<mark style="background: #5fb236">Formally, an EMA recursively calculates the output sequence Y : yt = α   xt + (1 − α)   yt−1, (2) where α ∈ (0, 1)d is the EMA coefficient representing the degree of weighting decrease, and   is the element-wise product. A higher α discounts older observations faster (see Figure 1).</mark> [(p. 4)](zotero://open-pdf/library/items/6CACZM4C?page=4)

<mark style="background: #5fb236">This property favors local dependencies, and limits long-distance dependencies. Despite the recurrent formulation in (2), the computation of EMA can be represented as n individual convolutions, which can be computed efficiently using fast Fourier transforms (FFTs) (see Appendix A for details).</mark> [(p. 4)](zotero://open-pdf/library/items/6CACZM4C?page=4)

## Notes

---
Comment: 13 pages, 4 figures and 7 tables

```
@article{Ma_Zhou_Kong_He_Gui_Neubig_May_Zettlemoyer_2022, title={Mega: Moving Average Equipped Gated Attention}, url={[http://arxiv.org/abs/2209.10655](http://arxiv.org/abs/2209.10655)}, DOI={[10.48550/arXiv.2209.10655](https://doi.org/10.48550/arXiv.2209.10655)}, abstractNote={The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.}, note={0 citations (Semantic Scholar/arXiv) [2022-09-24] arXiv:2209.10655 [cs]}, number={arXiv:2209.10655}, publisher={arXiv}, author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke}, year={2022}, month={Sep} }
```

%% Import Date: 2023-06-25T18:55:07.951+01:00 %%