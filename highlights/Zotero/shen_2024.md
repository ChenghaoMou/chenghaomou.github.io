---
aliases:
  - "Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler"
  - "Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler"
authors: Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, Rameswar Panda
date: 2024-08-28 14:29:28
tags:
  - later
title: "Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler"
url: https://arxiv.org/abs/2408.13359v1
created: 2024-02-03
modified: 2024-09-05
---

# Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler

## Abstract

Finding the optimal learning rate for language model pretraining is a challenging task. This is not only because there is a complicated correlation between learning rate, batch size, number of training tokens, model size, and other hyperparameters but also because it is prohibitively expensive to perform a hyperparameter search for large language models with Billions or Trillions of parameters. Recent studies propose using small proxy models and small corpus to perform hyperparameter searches and transposing the optimal parameters to large models and large corpus. While the zero-shot transferability is theoretically and empirically proven for model size related hyperparameters, like depth and width, the zero-shot transfer from small corpus to large corpus is underexplored. In this paper, we study the correlation between optimal learning rate, batch size, and number of training tokens for the recently proposed WSD scheduler. After thousands of small experiments, we found a power-law relationship between variables and demonstrated its transferability across model sizes. Based on the observation, we propose a new learning rate scheduler, Power scheduler, that is agnostic about the number of training tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (muP) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture. Our 3B dense and MoE models trained with the Power scheduler achieve comparable performance as state-of-the-art small language models. We open-source these pretrained models at https://ibm.biz/BdKhLa.

> Thus, the WSD scheduler still faces the same issue as the cosine scheduler: the intermediate and continual training checkpoints are suboptimal if the number of tokens is too different from the original plan. [(p. 2)](zotero://open-pdf/library/items/WNYGCWWN?page=2)

> ηopt tends to decrease with respect to the number of training tokens. For example, ηopt is 0.0128 for 2B training tokens and 0.0008 for 256B. [(p. 4)](zotero://open-pdf/library/items/WNYGCWWN?page=4)

> Figure 2(b) shows that the optimal learning rates do not transfer across a wide range of β, but tends to increase with respect to the batch size. [(p. 4)](zotero://open-pdf/library/items/WNYGCWWN?page=4)

> The optimal learning rate ηopt for the WSD scheduler and a given pair of (T, β) is proportional to the training batch size β. [(p. 5)](zotero://open-pdf/library/items/WNYGCWWN?page=5)

> Hypothesis 2 The Learning rate to batch size ratio γ has a power-law correlation with T [(p. 5)](zotero://open-pdf/library/items/WNYGCWWN?page=5)

```
@misc{Shen_Stallone_Mishra_Zhang_Tan_Prasad_Soria_Cox_Panda_2024, title={Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler}, url={[https://arxiv.org/abs/2408.13359v1](https://arxiv.org/abs/2408.13359v1)}, abstractNote={Finding the optimal learning rate for language model pretraining is a challenging task. This is not only because there is a complicated correlation between learning rate, batch size, number of training tokens, model size, and other hyperparameters but also because it is prohibitively expensive to perform a hyperparameter search for large language models with Billions or Trillions of parameters. Recent studies propose using small proxy models and small corpus to perform hyperparameter searches and transposing the optimal parameters to large models and large corpus. While the zero-shot transferability is theoretically and empirically proven for model size related hyperparameters, like depth and width, the zero-shot transfer from small corpus to large corpus is underexplored. In this paper, we study the correlation between optimal learning rate, batch size, and number of training tokens for the recently proposed WSD scheduler. After thousands of small experiments, we found a power-law relationship between variables and demonstrated its transferability across model sizes. Based on the observation, we propose a new learning rate scheduler, Power scheduler, that is agnostic about the number of training tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (muP) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture. Our 3B dense and MoE models trained with the Power scheduler achieve comparable performance as state-of-the-art small language models. We open-source these pretrained models at https://ibm.biz/BdKhLa.}, journal={arXiv.org}, author={Shen, Yikang and Stallone, Matthew and Mishra, Mayank and Zhang, Gaoyuan and Tan, Shawn and Prasad, Aditya and Soria, Adriana Meza and Cox, David D. and Panda, Rameswar}, year={2024}, month=aug, language={en} }
```