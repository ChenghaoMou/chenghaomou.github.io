---
aliases:
  - "Learning to (Learn at Test Time): RNNs with Expressive Hidden States"
  - "Learning to (Learn at Test Time): RNNs with Expressive Hidden States"
authors: "Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin"
date: '2024-07-21 13:35:59'
tags: []
title: "Learning to (Learn at Test Time): RNNs with Expressive Hidden States"
url: "https://arxiv.org/abs/2407.04620v1"
created: 2024-02-03
modified: 2024-07-21
---

# Learning to (Learn at Test Time): RNNs with Expressive Hidden States

## Abstract

Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.

> Our key idea is to make the hidden state itself a model f with weights W , and the update rule a gradient step on the self-supervised loss l. Therefore, updating the hidden state on a test sequence is equivalent to training the model f at test time. This process, known as test-time training (TTT), is programmed into our TTT layers. [(p. 1)](zotero://open-pdf/library/items/2WMLMF2S?page=1)

> This result represents an awkward reality for existing RNNs. On one hand, the main advantage of RNNs (vs. Transformers) is their linear (vs. quadratic) complexity. This asymptotic advantage is only realized in practice for long context, which according to Figure 3 is after 8k. On the other hand, once context is long enough, existing RNNs such as Mamba struggle to actually take advantage of the extra information being conditioned on. [(p. 2)](zotero://open-pdf/library/items/2WMLMF2S?page=2)

> In this paper, we begin with the observation that self-supervised learning can compress a massive training set into the weights of a model such as an LLM, which often exhibits deep understanding about the semantic connections among its training data – exactly what we need from a compression heuristic. [(p. 3)](zotero://open-pdf/library/items/2WMLMF2S?page=3)

![[statics/sun_2024a/image-4-x76-y433.png]]

> Our key idea is to use self-supervised learning to compress the historic context x1, . . . , xt into a hidden state st, by making the context an unlabeled dataset and the state a model. [(p. 4)](zotero://open-pdf/library/items/2WMLMF2S?page=4)

> In Subsection 2.1, we did not specify the corruption that produces x ̃t from xt. One design is to make it a low-rank projection x ̃t = θK xt, where θK is a learnable matrix.4 Following the terminology of multi-view reconstruction, θK xt is called a training view [(p. 7)](zotero://open-pdf/library/items/2WMLMF2S?page=7)

> Moreover, perhaps not all the information in xt is worth remembering, so the reconstruction label can be another low-rank projection θV xt instead of xt. Here θV xt is called the label view, where θV is also learnable. [(p. 7)](zotero://open-pdf/library/items/2WMLMF2S?page=7)

> Lastly, the training view θK xt has fewer dimensions than xt, so we can no longer use the output rule in Equation 1. [(p. 7)](zotero://open-pdf/library/items/2WMLMF2S?page=7)

> This solution has an additional benefit. The training and label views specify the information in xt that is compressed into Wt and propagated forward through time. The test view specifies potentially different information that is mapped to the current output token zt and propagated forward through network layers, therefore adds more flexibility to the self-supervised task. [(p. 7)](zotero://open-pdf/library/items/2WMLMF2S?page=7)

```
@misc{Sun_Li_Dalal_Xu_Vikram_Zhang_Dubois_Chen_Wang_Koyejo_et al._2024, title={Learning to (Learn at Test Time): RNNs with Expressive Hidden States}, url={[https://arxiv.org/abs/2407.04620v1](https://arxiv.org/abs/2407.04620v1)}, abstractNote={Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.}, note={0 citations (Semantic Scholar/arXiv) [2024-07-20]}, journal={arXiv.org}, author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and Hashimoto, Tatsunori and Guestrin, Carlos}, year={2024}, month=jul, language={en} }
```