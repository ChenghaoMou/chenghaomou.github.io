---
aliases:
  - "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale"
  - "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale"
authors: "Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M. Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav Shoham"
date: '2024-08-26 10:45:05'
tags: []
title: "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale"
url: "https://arxiv.org/abs/2408.12570v1"
created: 2024-02-03
modified: 2024-08-26
---

# Jamba-1.5: Hybrid Transformer-Mamba Models at Scale

## Abstract

We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.

> For this release, we experimented also with Mamba-2 [6], a faster and improved version of Mamba, which was reported to outperform Mamba and Transformers separately. However, as Figure 1 shows, we found that in a hybrid architecture, the Mamba-1-Attention combination works better than Mamba2-Attention, so we use Mamba-1 in Jamba-1.5-Large. (We also found the hybrid architecture to outperform pure Mamba-2.) We hypothesize this is because some of the advantages of Mamba-2 over Mamba-1, in particular the ability to use a much larger state size, are less significant when we have full attention layers interleaved between the Mamba layers, as they can pool information from the entire context. [(p. 3)](zotero://open-pdf/library/items/HSC3B5E2?page=3)

> To do so, we quantize the MoE and MLP weights to INT8, save them in INT8, and dequnatize them back to BF16 before the actual computation. Importantly, the dequantization step happens directly inside the fused_moe kernel in vLLM [18]. In this way, the dequantization process adds negligible overhead, and even leads to improved latency over BF16.1 We have contributed our modified fused_moe kernel to vLLM.2 1We attribute this to the the kernel operating on relatively small blocks of weights and activations, which it moves from GPU HBM to SRAM prior to performing the computations. In our implementation, the weights move from HBM to SRAM when they are in int8, so it takes less time as their memory footprint is cut by half. [(p. 3)](zotero://open-pdf/library/items/HSC3B5E2?page=3)

> Our ExpertsInt8 method has several advantages. First, it is fast; quantization only takes a few seconds at model loading. Second, unlike most other techniques in vLLM, it does not rely on calibration, which can take hours or days and can be unstable. Third, we can still use BF16 to hold large activations. Fourth, it is available to use on A100 GPUs, unlike FP8, which is only available on H100. Finally, our quantization matches FP8 in latency, while surpassing other quantization techniques, without a loss in quality. [(p. 4)](zotero://open-pdf/library/items/HSC3B5E2?page=4)

> During pre-training, we found that certain activations, namely outputs of specific experts as well as the the output of the last Mamba layers, were gradually increasing in magnitude for certain input tokens, eventually reaching values as high as 4 × 106. [(p. 4)](zotero://open-pdf/library/items/HSC3B5E2?page=4)

> To alleviate these concerns, we added an “Activation Loss” term, proportional to the mean-square of activations in the forward pass, with a configurable α factor, which penalizes larger activation values. We found via experimentation that this auxilary loss has no affect on the training even with α values up to at least 10−3. For Jamba-1.5-Large, we used α = 10−5 which was enough to reduce the activations to an acceptable range (2K-3K max). Moreover, adding this auxilary loss reduced the activations almost instantly, allowing it to be added only towards the end of the training without any affect on training speed and quality. [(p. 4)](zotero://open-pdf/library/items/HSC3B5E2?page=4)

```
@misc{Team_Lenz_Arazi_Bergman_Manevich_Peleg_Aviram_Almagor_Fridman_Padnos_et al._2024, title={Jamba-1.5: Hybrid Transformer-Mamba Models at Scale}, url={[https://arxiv.org/abs/2408.12570v1](https://arxiv.org/abs/2408.12570v1)}, abstractNote={We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.}, journal={arXiv.org}, author={Team, Jamba and Lenz, Barak and Arazi, Alan and Bergman, Amir and Manevich, Avshalom and Peleg, Barak and Aviram, Ben and Almagor, Chen and Fridman, Clara and Padnos, Dan and Gissin, Daniel and Jannai, Daniel and Muhlgay, Dor and Zimberg, Dor and Gerber, Edden M. and Dolev, Elad and Krakovsky, Eran and Safahi, Erez and Schwartz, Erez and Cohen, Gal and Shachaf, Gal and Rozenblum, Haim and Bata, Hofit and Blass, Ido and Magar, Inbal and Dalmedigos, Itay and Osin, Jhonathan and Fadlon, Julie and Rozman, Maria and Danos, Matan and Gokhman, Michael and Zusman, Mor and Gidron, Naama and Ratner, Nir and Gat, Noam and Rozen, Noam and Fried, Oded and Leshno, Ohad and Antverg, Omer and Abend, Omri and Lieber, Opher and Dagan, Or and Cohavi, Orit and Alon, Raz and Belson, Ro’i and Cohen, Roi and Gilad, Rom and Glozman, Roman and Lev, Shahar and Meirom, Shaked and Delbari, Tal and Ness, Tal and Asida, Tomer and Gal, Tom Ben and Braude, Tom and Pumerantz, Uriya and Cohen, Yehoshua and Belinkov, Yonatan and Globerson, Yuval and Levy, Yuval Peleg and Shoham, Yoav}, year={2024}, month=aug, language={en} }
```