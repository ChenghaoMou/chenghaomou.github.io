---
aliases:
  - "Hallucination is Inevitable: An Innate Limitation of Large Language Models"
  - "Hallucination is Inevitable: An Innate Limitation of Large Language Models"
authors: Ziwei Xu, Sanjay Jain, Mohan Kankanhalli
date: 2024-03-31 14:00:22
tags:
  - TODO
title: "Hallucination is Inevitable: An Innate Limitation of Large Language Models"
url: https://arxiv.org/abs/2401.11817v1
---

# Hallucination is Inevitable: An Innate Limitation of Large Language Models

Reference: [2401.11817 - Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://www.emergentmind.com/papers/2401.11817)

## Abstract
Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.

![[statics/xu_2024/image-4-x102-y526.png]]

![[statics/xu_2024/image-4-x104-y408.png]]

![[statics/xu_2024/image-4-x83-y69.png]]

![[statics/xu_2024/image-5-x106-y608.png]]

> For example, f could be a function that answers “true” for factual statements and “false” for non-factual ones. It could also be a function that completes a prompt to produce a factual statement. We only assume that such computable f always exists (though its exact implementation might be unknown) in our formal world for all tasks. Otherwise, we can immediately say LLM will hallucinate on some tasks in the formal world because LLM is computable while the ground truth function f is not. We can similarly define Gh = {(s, h(s)) | s ∈ S} to be the world of LLM h. [(p. 5)](zotero://open-pdf/library/items/9QZNAFTK?page=5)

![[statics/xu_2024/image-5-x105-y485.png]]

> Set T is a generalized corpus of how ground-truth f answers/completes input strings. For example, if f answers “true” for factual inputs and “false” otherwise, then our training samples could look like { (“A shark is a mammal.”, “false”), (“Earth orbits around the Sun.”, “true”), ... }. On the other hand, if f is a function that completes or answers input string s, then T could look like { (“Is shark a fish or mammal?”, “Fish.”), (“What is the sum of binary numbers 10001 and 10110?”, “100111.”), ... }. Thanks to this general definition, we do not need to worry about the exact task and corpus on which an LLM is trained. Furthermore, we do not assume anything about the size of T , meaning the training procedure to be introduced in Section 3.3 can use as many samples as needed. [(p. 5)](zotero://open-pdf/library/items/9QZNAFTK?page=5)

![[statics/xu_2024/image-5-x107-y315.png]]

![[statics/xu_2024/image-5-x124-y224.png]]

![[statics/xu_2024/image-5-x126-y73.png]]

![[statics/xu_2024/image-6-x105-y641.png]]

![[statics/xu_2024/image-6-x104-y276.png]]

![[statics/xu_2024/image-7-x92-y411.png]]

> The theorem and its proof indicate that if f is not listed in the enumeration table, then it is not learnable by any LLM in the table, and therefore all LLMs in the table will hallucinate on f . [(p. 9)](zotero://open-pdf/library/items/9QZNAFTK?page=9)

![[statics/xu_2024/image-9-x102-y311.png]]

![[statics/xu_2024/image-10-x105-y229.png]]

![[statics/xu_2024/image-10-x104-y69.png]]

> An important, but not the only, reason for hallucination is that the problem is beyond LLMs’ computation capabilities. For those problems, any answer except “I don’t know” is unreliable and suggests that LLMs have added premises implicitly during the generation process. It could potentially reinforce stereotypical opinions and prejudices towards under-represented groups and ideas. Furthermore, it can be dangerous if unexpected premises were added and resulted in unethical, disturbing, or destructive contents. [(p. 13)](zotero://open-pdf/library/items/9QZNAFTK?page=13)

> It is notable that while LLMs cannot learn all computable ground truth functions f , it can learn some f (see Section C) and can be useful therein. The key is not to view LLMs as infallible sources of truth but as powerful assistive tools for information retrieval, analysis, summarisation, and presentation. [(p. 14)](zotero://open-pdf/library/items/9QZNAFTK?page=14)

> Finally, hallucination is not completely detrimental. In art, literature, and design, the unintentional and unpredictable outputs from LLMs could inspire human creators. Such deviation from facts can lead to unique perspectives that a strictly factual and precise system might never generate. In this sense, the hallucinatory aspect of LLMs should be regarded positively as a source of inspiration, innovation, and creativity. [(p. 14)](zotero://open-pdf/library/items/9QZNAFTK?page=14)

## Notes

---
TL;DR

This paper formalizes the problem and shows that it is impossible to eliminate hallucination in LLMs, and defines a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function.

```
@misc{Xu_Jain_Kankanhalli_2024, title={Hallucination is Inevitable: An Innate Limitation of Large Language Models}, url={[https://arxiv.org/abs/2401.11817v1](https://arxiv.org/abs/2401.11817v1)}, abstractNote={Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.}, note={8 citations (Semantic Scholar/arXiv) [2024-03-30]}, journal={arXiv.org}, author={Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan}, year={2024}, month=jan, language={en} }
```