---
aliases:
  - "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model"
  - "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model"
authors: "Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy"
date: '2024-08-24 20:19:07'
tags: []
title: "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model"
url: "https://arxiv.org/abs/2408.11039v1"
created: 2024-02-03
modified: 2024-08-24
---

# Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model

## Abstract

We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.

![[statics/zhou_2024/image-2-x101-y580.png]]

> Ablation experiments reveal critical components and potential improvements for Transfusion. We observe that the intra-image bidirectional attention is important, and that replacing it with causal attention hurts text-to-image generation. [(p. 2)](zotero://open-pdf/library/items/6WK5LYTJ?page=2)

> We also find that adding U-Net down and up blocks to encode and decode images enables Transfusion to compress larger image patches with relatively small loss to performance, potentially decreasing the serving costs by up to 64×. [(p. 2)](zotero://open-pdf/library/items/6WK5LYTJ?page=2)

> Our main innovation is demonstrating that we can use separate losses for different modalities – language modeling for text, diffusion for images – over shared data and parameters. [(p. 5)](zotero://open-pdf/library/items/6WK5LYTJ?page=5)

> We experiment with data spanning two modalities: discrete text and continuous images. Each text string is tokenized into a sequence of discrete tokens from a fixed vocabulary, where each token is represented as an integer. Each image is encoded as latent patches using a VAE (see §2.3), where each patch is represented as a continuous vector; the patches are sequenced left-to-right top-to-bottom to create a sequence of patch vectors from each image.3 For mixed-modal examples, we surround each image sequence with special beginning of image (BOI) and end of image (EOI) tokens before inserting it to the text sequence; thus, we arrive at a single sequence potentially containing both discrete elements (integers representing text tokens) and continuous elements (vectors representing image patches). [(p. 5)](zotero://open-pdf/library/items/6WK5LYTJ?page=5)

> Transfusion combines both attention patterns by applying causal attention to every element in the sequence, and bidirectional attention within the elements of each individual image. This allows every image patch to attend to every other patch within the same image, but only attend to text or patches of other images that appeared previously in the sequence. We find that enabling intra-image attention significantly boosts model performance (see §4.3). [(p. 6)](zotero://open-pdf/library/items/6WK5LYTJ?page=6)

> When we sample a BOI token, the decoding algorithm switches to diffusion mode, where we follow the standard procedure of decoding from diffusion models. Specifically, we append a pure noise xT in the form of n image patches to the input sequence (depending on the desired image size), and denoise over T steps. At each step t, we take the noise prediction and use it to produce xt−1, which then overwrites xt in the sequence; i.e. the model always conditions on the last timestep of the noised image and cannot attend to previous timesteps. Once the diffusion process has ended, we append an EOI token to the predicted image, and switch back to LM mode. This algorithm enables the generation of any mixture of text and image modalities. [(p. 6)](zotero://open-pdf/library/items/6WK5LYTJ?page=6)

> since U-Net blocks contain bidirectional attention within, independent of the transformer’s attention mask, this gap is less pronounced when they are applied. [(p. 10)](zotero://open-pdf/library/items/6WK5LYTJ?page=10)

```
@misc{Zhou_Yu_Babu_Tirumala_Yasunaga_Shamis_Kahn_Ma_Zettlemoyer_Levy_2024, title={Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model}, url={[https://arxiv.org/abs/2408.11039v1](https://arxiv.org/abs/2408.11039v1)}, abstractNote={We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.}, journal={arXiv.org}, author={Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer}, year={2024}, month=aug, language={en} }
```