---
title: "Cracking The Machine Learning Interview"
author: Nitin Suri
created: 2024-06-22
modified: 2024-07-04
---

# Cracking The Machine Learning Interview

[Apple Books Link](ibooks://assetid/632BAA19E66D73A76366CEADC1671EF8)

> AUROC is robust to class imbalance, unlike raw accuracy. AUROC is constructed using True Positive rate and False Positive rate which are not affected by the class distribution shifting, whereas raw accuracy, based on the class distribution of the test dataset would be affected if the underlying class distribution changes or is unknown.

> What are the trade-offs between the different types of classification algorithms? How do you choose the best one?

> Why is Area Under ROC Curve (AUROC) better than raw accuracy?

> AIC (Akaike Information Criterion)

> Naive Bayes assumes that all the features in a dataset are equally important and conditionally independent of each other, i.e, given a class label, Y, each feature is conditionally independent of each other. These assumptions are rarely true in the real world scenario which is why Naive Bayes is called "naive".

> What is a p-value? Why is it important?

> What do you mean by paired t-test? Where would you use it

> Define F-test. Where would you use it?

> How would you evaluate a Logistic Regression model?

> Differentiate between ROC curve and PR curve.

> What is an F1 score?

> strata

> The penalty in Ridge regression is the sum of the squares of the coefficients whereas, for the Lasso, it is the sum of the absolute values of the coefficients. The Lasso regression is used to achieve sparse solutions by making some of the coefficients zero. The Ridge regression, on the other hand, tries to smoothen the solution as it keeps all the coefficients but minimizes the sum of their squares.

> If the training set is small, the high bias/low variance models, such as Naive Bayes, tend to perform better because they are less likely to overfit. If the training set, on the other hand, is large, then, low bias/high variance models, such as Logistic Regression, tend to perform better because they can reflect more complex relationships.

> Decision Tree partitions the feature space into smaller and smaller subspaces, whereas Logistic Regression fits a single hypersurface to divide the feature space exactly into two. When the classes are not well-separated, decision trees are susceptible to overfitting whereas Logistic Regression generalizes better.

> How can you check if the regression model fits the data well?

> How is Singular Value Decomposition (SVD) mathematically related to EVD for PCA?

> Bagging works mainly by reducing the variance and Boosting works by primarily reducing the bias.

> How would you reduce the dimensionality of a dataset which has correlated features?

> What is a chi-squared test?

> For the smaller training dataset, high bias/low variance classifiers (e.g. Naive Bayes) have an advantage over low bias/high variance classifiers (e.g. k-Nearest Neighbors), as the latter will overfit the dataset. But as the size of the training dataset grows, the latter starts performing better as they have a lower asymptotic error

