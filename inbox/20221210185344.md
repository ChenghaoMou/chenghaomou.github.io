---
aliases:
  - On the Dangers of Large Generative Models
tags:
  - WIP
title: "On the Dangers of Large Generative Models"
---

# On the Dangers of Large Generative Models

## Intended Audience

People who have no conflict of interest in defending or criticizing for-profit companies behind some of the large generative models.

## Scope

This article specifically focuses on large generative models often used in content generation. This is an opinion piece on what has gone wrong instead of presenting stories from both sides.

## Background

**What are they?**

They are models that can draw shallow or deep statistics from the training data and synthesize output, usually with some level of generalizability and are done so with objectives to maximize the mimicry. Most pre-training objectives have nothing to do with truthfulness or understanding. The compelling, seemingly logically sound, and fluent appearance of the output only makes sense in our interpretation ([[Human-like programs abuse our empathy - even Google engineers aren’t immune]], [[Why We Should Not Trust Chatbots As Sources of Information]], [[Did the GPT3 Chatbot Pass the Lovelace Creativity Test-|Did the GPT3 Chatbot Pass the Lovelace Creativity Test?]])

Of course, those models can be quite useful in some use cases, but I am not counting on those "AI" products launched on Product Hunt that are essentially a wrapper of commercial APIs.

**What's the porblem?**

The problem is not just about the model. Models aside, the very decision to build one without proper user study, ethical reviews, or any willingness to be open for research discussions, and with the data collection that is obscure at best, unethical at worst, the never-ending scaling in both model size and carbon emission, the hasty public release and happily hype-based propaganda are all contributing to this mess.

The problem is that we **should have done it better**, but no one is admitting the mistake, and everyone is just doubling down on the hype train so they would look less stupid. Why? I honestly couldn't know for sure. Maybe investors, after three years of the pandemic, are just not that patient for thoughtfulness in research. Or maybe we are already too deep in this if-not-SOTA-then-you-are-irrelevant vicious loop. Something must have triggered the global AI frenzy, whether you like it or not.

But we are definitely witnessing the dangers of those decisions in real time. What I hope to achieve here is to document these consequences, and if someone cares at all, we might have a chance to correct the course or avoid them in the near future.

## Dangers

### 1. Perpetuated Stereotypes and Biases

Exacerbating the marginalization of already marginalized and harmed groups.[^1]

### 2. Large Scale Misinformation and Plagiarism

Lowering the barrier for malicious actors[^2], dissemination[^3][^1],[^4] and facilitating plagiarism.[^5]
Creating more problems than it solves: Galactica aims to help researchers distinguish what is meaningful and what is not, but ironically generates fake research content.[^6]
Unhelpful and useless generated content will drown us with a sea of information noise, and dissemination of such garbage forces us into a "human centipede" experiment that we could not opt out of[^7] [^8] [^9] [^10] [^11] [^12] [^13] .[^14] (Zero-cost [[20230115161911|Bullshit]] generator or low-quality science fiction writer[^15] [^16])
"Hallucination" has already started to impact business with nonexistent services.[^17]

### 3. Diminished Human Value and Infringed Human Rights

Diminishes the human work,[^18] human creativity[^19], human connections/communities[^20] or humanity[^21],[^22] and human responsibility behind the model[^1][^23][^24] .[^25]
Displace human workers from unscrupulous employers[^26].
Exploitation of human labor for the labeling[^23] .[^27]
infringing on people's voice[^28] and image (deepfake[^29]) even though they can be of help in movie industry;
[[Justine Bateman on AI, Labor, and the Future of Entertainment|Justine Bateman on AI, Labor, and the Future of Entertainment]]
[[20240228190625|AI-Generated Kara Swisher Biographies Flood Amazon]]
### 4. Disconnected Communities and Knowledge-seeking Journey

Losing the human connection further.[^30]
Cutting short the knowledge-seeking journey.[^31]

Information at your fingertip [[20230720210314|Connection Engine]]

### 5. Weakened Education

Cheapening our education and weakening young minds[^30] .[^32]
How is this tool going to affect our education system?[^33]

### 6. Worsened Environment

The investment behind those models are also creating an environment where companies are encouraged to release models or products before worrying about their social impacts.[^29]

## Takeaways

### 1. Call it What it is

It is not Artificial Intelligence, Currently, Large Generative Models aren't suitable for serious use.[^4]
Fundamentally, lack of understanding of what is true and what is not[^31] .[^34]
Lack of quick factual or knowledge update mechanism.[^31]

### 2. Computer Science can't be the Solution to Everything

- Hype in AI, journalism, marketing, and any other discipline that people are trying to reform with AI.[^35]
- What is a better way to spend the funding, the resources, and the emission budget on?[^36]
- What do we need to do to build AI applications/systems ethically[^1][^37] with transparency and trustworthiness?[^38] [^39]
- Business-driven model release could potentially worsen it[^40].[^41]
- Power dynamics with content producers and for-profit companies.[^41]
- Data-laundry: sponsoring a non-profit organization to collect public accessible data.[^22]
- The problematic data collection process, albeit open-source, shows little to no respect and consideration towards creators and artists.[^22]
- Releasing models before guardrails makes it so much easier to generate without them, exacerbating the truth issues and polarization of the society.[^22] Even those guardrails are only after-thoughts where people can easily circumvent them with clever prompt engineering[^42] .[^11]
- Instead of the ad-based business model, is there a better way for both content providers and the platform?[^43]
- Hold companies behind those models accountable.[^44]
### 3. Harms vs Existential Risks

- No extinction risks but present harms[^45] [^46]
[[Human Extinction- A Brief Guided Tour of the Book]]

## Questions

- What should we do with large generative models, especially from large companies or people in power?[^4] What is the innovator's dilemma?[^40]
- Is launching public beta access a form of annotation collection from free human labor? If so, how can we stop being guinea pigs for their experiments? (See [[Microsoft’s AI chatbot is going off the rails|Microsoft’s AI chatbot is going off the rails]])
- Is such trust a choice, a belief, or even a religion? Similar to how some people believe in flat earth, or taro cards.
- Humans are capable of learning rules, generalizing them, and applying them in a given context. How can we combine symbolic systems with current neural networks as efficiently as human[^47][^48][^49]?

Another question we often ask is whether a system (not just the model) can have [[20220625181723|World Knowledge]] one day. My answer would be a hopefully yes. More and more studies are going in this direction, where humans are becoming an integral part, and the large language model is only a small part of it. I believe such interactive, integrated, and grounded learning can bridge the gap between the model world with the real physical world via humans. ([[shanahan_2022|Talking About Large Language Models]], [[Why We Should Not Trust Chatbots As Sources of Information|Why We Should Not Trust Chatbots As Sources of Information]])

[^1]: [[ChatGPT, Galactica, and the Progress Trap]]
[^2]: [[Bing AI Can't Be Trusted|Bing AI Can't Be Trusted]]
[^3]: [[Ahead of AI -3- A(I)nnouncements|Ahead of AI]]
[^4]: [[Google won’t launch ChatGPT rival because of ‘reputational risk’]]
[^5]: [[CNET's AI Journalist Appears to Have Committed Extensive Plagiarism]]
[^6]: [Mystery AI Hype Theater 3000 - videos.trom.tf](https://videos.trom.tf/w/p/4gykGcMrmHHs7bG2Y6qK9W?playlistPosition=6&resume=true)
[^7]: [[The Expanding Dark Forest and Generative AI]]
[^8]: [[CNET Is Reviewing the Accuracy of All Its AI-Written Articles After Multiple Major Corrections]]
[^9]: [[AGI will not happen in your lifetime. Or will it-|AGI will not happen in your lifetime. Or will it?]]
[^10]: [[What Google Should Really Be Worried About|What Google Should Really Be Worried About]]
[^11]: [[What Google Should Really Be Worried About|What Google Should Really Be Worried About]]
[^12]: [[What Google Should Really Be Worried About|What Google Should Really Be Worried About]]
[^13]: [[AI is killing the old web, and the new web struggles to be born|AI is killing the old web, and the new web struggles to be born]]
[^14]: [[What Google Should Really Be Worried About|What Google Should Really Be Worried About]]
[^15]: [[Inside the Heart of ChatGPT’s Darkness|Inside the Heart of ChatGPT’s Darkness]]
[^16]: [[Magazine Publishes Serious Errors in First AI-Generated Health Article|Magazine Publishes Serious Errors in First AI-Generated Health Article]]
[^17]: [[4chan users embrace AI voice clone tool to generate celebrity hatespeech]]
[^18]: [[24 Seriously Embarrassing Hours for AI]]
[^19]: [[Artists are alarmed by AI - and they’re fighting back|Artists are alarmed by AI - and they’re fighting back]]
[^20]: [[Reddit Moderators Do Over $3.4 Million in Free Labor Every Year|Reddit Moderators Do Over $3.4 Million in Free Labor Every Year]]
[^21]: [[Did the GPT3 Chatbot Pass the Lovelace Creativity Test-|Did the GPT3 Chatbot Pass the Lovelace Creativity Test?]]
[^22]: [[The Alt-Right Manipulated My Comic. Then A.I. Claimed It.|The Alt-Right Manipulated My Comic. Then A.I. Claimed It.]]
[^23]: [[AI Isn’t Artificial or Intelligent|AI Isn’t Artificial or Intelligent]]
[^24]: [[What is Art Without the Human Mind-|What is Art Without the Human Mind?]]
[^25]: [[The Need for Accountability in AI-Generated Content|The Need for Accountability in AI-Generated Content]]
[^26]: [[ChatGPT took their jobs. Now they walk dogs and fix air conditioners.|ChatGPT took their jobs. Now they walk dogs and fix air conditioners.]]
[^27]: [[somepalli_2022|Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models]]
[^28]: [[Separating Fact from Fiction-|Separating Fact from Fiction?]]
[^29]: [[OPWNAI - Cybercriminals Starting to Use ChatGPT]]
[^30]: [[What Students Lose by Embracing Easy Tech Like ChatGPT]]
[^31]: [[Is ChatGPT Really a “Code Red” for Google Search-|Is ChatGPT Really a “Code Red” for Google Search?]]
[^32]: [[Why I Don't Care if Students Use GPT|Why I Don't Care if Students Use GPT]]
[^33]: [[Students Depend on ChatGPT for Final Exams]]
[^34]: [[Transcript- Ezra Klein Interviews Gary Marcus]]
[^35]: [[Why Google Missed ChatGPT]]
[^36]: [[The issue of how these models handle negation a...|The issue of how these models handle negation a...]]
[^37]: [[AI porn is easy to make now. For women, that’s a nightmare.|AI porn is easy to make now. For women]]
[^38]: [[In AI arms race, ethics may be the first casualty]]
[^39]: [[AI statement - Neil Clarke|AI statement - Neil Clarke]]
[^40]: [[Don't believe ChatGPT - we do NOT offer a -phone lookup- service]]
[^41]: [[20220925095941|Ethical Questions to Ask in AI]]
[^42]: [[We're seeing multiple folks in -NLProc who -sho...|We're seeing multiple folks in]]
[^43]: [[Transcript- Ezra Klein Interviews Gary Marcus]]
[^44]: [[Did ChatGPT Really Pass Graduate-Level Exams-]]
[^45]: [[Is Avoiding Extinction from AI Really an Urgent Priority-|Is Avoiding Extinction from AI Really an Urgent Priority?]]
[^46]: [[The DAIR Institute|The DAIR Institute]]
[^47]: [[We come to bury ChatGPT, not to praise it.|We come to bury ChatGPT]]
[^48]: [[My Strange Day With Bing’s New AI Chatbot|My Strange Day With Bing’s New AI Chatbot]]
[^49]: [[This Is Too Important to Leave to Microsoft, Google and Facebook|This Is Too Important to Leave to Microsoft, Google and Facebook]