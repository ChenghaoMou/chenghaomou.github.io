---
title: "优化器"
created: 2024-06-17
modified: 2024-06-17
---

### 优化器

#### adagrad

$$θ_{t+1}​=θ_{t}​−\frac{\eta}{\sqrt{ G_{t} + \epsilon }}​∇θ​J(θ_{t}​)$$

$G_{t}$ 就是历史梯度的平方和。

主要的原理就是通过参数的历史梯度调整学习率，梯度调整越大，学习率越低。这让每个参数都可以有不同的学习率，大大提升了模型的学习性能。

#### RMSProp

$$θ_{t+1}​=θ_{t}​−\frac{\eta}{\sqrt{ E[g^2]_{t} + \epsilon }}​∇θ​J(θ_{t}​)$$

$E[g^2]_{t}$ 就是历史梯度的平方指数移动平均。

和 adagard 类似，但是是通过历史梯度的移动平均计算学习率。这样的好处是，RMS 可以学习不同的目标函数。

#### adam

$$
\begin{align}
m_{t}​&=β_{1}​m_{t−1}​+(1−β_{1}​)∇θ​J(θ_{t})\\
𝑣_{t}&=𝛽_{2}𝑣_{𝑡−1}+(1−𝛽_{2})(∇𝜃𝐽(𝜃_{t}))^2\\
\tilde{𝑚^𝑡}&=\frac{m_{t}}{1-\beta_{1}^t}\\​​
\tilde{v^𝑡}&=\frac{v_{t}}{1-\beta_{2}^t}\\
𝜃_{t+1}&=𝜃_{t}−\frac{𝜂\tilde{𝑚^𝑡}}{\sqrt{\tilde{v^𝑡}}+ϵ}\\
\end{align}
$$

基于 SGD w/ momentum 和 RMSProp，计算两种动量。

#### sgd

最原始的优化器，计算简单。但是容易陷入局部最优解，对学习率有一定的要求。

#### Gradient Clipping

当梯度的 norm 超过一定值时，进行 clipping，避免梯度爆炸。

#### Momentum

动量的加入使得每次参数调整，优化器会考虑之前的参数更新（移动平均），从而使得模型的更新更稳定，避免震荡。

#### Sigmoid 导数

$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

#### Softmax 导数

$$
\begin{align}
\frac{\partial z(x_{i})}{\partial x_j} = z(x_i)(\delta_{ij} - z(x_j))
\end{align}
$$