---
title: "工具框架题"
created: 2024-06-17
modified: 2024-06-25
---

# 工具框架题

## 模型并行、数据并行区别优缺点

*数据并行*：切分数据到不同的设备上。在反向传播中，需要对各个设备上的梯度进行 AllReduce。适合**数据量大，模型小**的情况。

*模型并行*：包括张量并行和流水线并行。

1. 张量并行就是把模型**参数**进行切分（通常时同一层内的参数），其中的代价就是需要把数据在多个设备之间进行广播。适合大模型训练。Communication 的代价较大。常用的场景时嵌入层和 softmax 层。
2. 流水线并行就是把模型的**不同层**切分到不同的设备，每台设备需要前一台设备的输出。适合深度大模型训练。避免设备空闲的情况，可以在一批数据中再进行切分进行微批量流水处理。

## Megatron LM

### MLP 层

参数：$M \in \mathbb{R}^{a\times b}$， 输入：$x \in \mathbb{R}^{i\times a}$， 输出：$x \in \mathbb{R}^{i\times b}$

Column Splitting:

- $M \in \mathbb{R}^{a\times b} = [A_0;A_1]$, $A_0 \in \mathbb{R}^{a\times b/2}, A_1 \in \mathbb{R}^{a\times b/2}$
- $[xA_0, xA_1] \in \mathbb{R}^{i\times b}$

Row Splitting:

- $M \in \mathbb{R}^{a\times b} = [B_0, B_1]^T$, $B_0 \in \mathbb{R}^{a/2 \times b}, B_1 \in \mathbb{R}^{a/2\times b}$
- $x_0 \in \mathbb{R}^{i\times a/2}, x_1 \in \mathbb{R}^{i\times a/2}$
- $x_0B_0 + x_1B_1 \in \mathbb{R}^{i\times b}$

如果激活函数是非线性的话，row splitting 需要再激活函数之间进行同步，而 column splitting 则没有这个需要；

### 注意力层

可以多头分布

### 激活断点

重计算 + 断点

## DeepSpeed

模型状态：优化器状态（动量和方差），梯度，参数

残差状态：激活，缓存等

ZeRO 的分类如下：

1. 0 级：仅仅使用数据并行。适合大数据小模型。
2. 1 级：切分优化器状态
3. 2 级：切分优化器状态和梯度
4. 3 级：切分优化器状态，梯度和模型参数
5. Offload：把计算任务给 CPU
6. Infinity：用固态硬盘来扩展内存
7. 混合精度：混合精度优化器

优点：

1. Zero Redundancy Optimizer （ZeRO）大大减少了**内存**占用。同样的硬件条件，支持的模型大小也得到了增加。
2. 支持数据并行，模型并行（张量并行和流水线并行），和混合并行
3. 提供了高性能的定制推理内核，通信优化和异构内存技术
4. 提供了灵活的模型压缩技术（ZeroQuant）
5. 简单的 API 接口和配置文件
6. 支持混合精度训练

缺点：

1. 学习曲线

## Megatron 优缺点

优点：

1. 支持数据并行，模型并行（张量并行和流水线并行），和混合并行
2. 在很多大模型实际训练中表现出色

缺点：

1. 门槛较高
2. 资源需求高

## 混合精度训练的原理，有哪些优缺点，针对这些优缺点是如何改进的

优点：FP16 的计算速度比 FP32 快，内存占用少。从而可以支持更大的模型或者更大的批量大小，减少存储空间的使用。

缺点：FP16 的精度和数据范围都有一定的减少，可能会出现计算不准确的情况。bFP16 相对于 FP16，数据表示范围较大，数据精度有所减少。

改进：

1. 混合精度训练或者自动混合精度训练（Apex）。
2. **权重备份**：用 FP32 来进行权重更新，其余的计算用 FP16。需要保留一份 FP32 的权重。主要原因是梯度乘上学习率之后如果数值特别小，超过 FP16 的表示范围，会出现无法收敛的情况（置零），在 FP32 下进行权重更新训练更稳定。
3. 为了解决梯度过小，模型训练不收敛的情况，可以对损失进行缩放。缩放因子也可以动态调整。比如：没有梯度溢出，那么不进行缩放，不然缩放因子减半，后期模型收敛，梯度更新的幅度减小，可以尝试增加缩放因子。

## RAG 优化

可以从四个方面进行优化：数据 + 检索 + 重排 + 生成：

1. 数据上，可以进行预处理，文档分块，增加元数据，针对不同领域，可以收集数据进行适配；
2. 微调双编码器 (bi-encoder) 和交叉编码器 (cross-encoder) 来改进检索和重排性能；
3. 使用高效的向量数据库来提高检索性能；
4. 针对不同问题类型,动态调整检索文档数量、检索策略；
5. 生成上，优化提示工程,有效利用检索到的上下文；对生成的信息进行二次检测（例如事实检查）

也可以从系统上进行优化：分别评估不同组件的性能瓶颈；根据评估结果,针对性地微调嵌入模型、检索器或语言模型

## langchain 和 llama index 的区别

| 框架         | 目的和适用场景   | 功能范围                       | 性能    | 学习曲线         |
| ---------- | --------- | -------------------------- | ----- | ------------ |
| langchain  | 各种 LLM 应用 | 通用框架，包含全面的工具集（链、代理、向量数据库等） | 复杂系统  | 功能繁多，上手难     |
| llamaindex | 数据索引和 RAG | 数据连接和索引构建                  | 大数据检索 | 专注于信息检索，上手简单 |

总的来说,如果项目主要涉及高效的数据索引和检索，LlamaIndex 可能是更好的选择。如果需要构建更复杂、灵活的 LLM 应用，LangChain 可能更合适。在某些情况下,两者可以结合使用，发挥各自的优势。

## langchain AGENT、MEM、向量知识库的细节

Langchain 的新库 Langgraph 会更适合**复杂代理系统**的构建；

代理是一种可以基于大模型推理能力的决策节点，它可以通过调用历史信息，不同的函数或者工具来进行更复杂的操作。

Memory 主要的初衷就是为多轮对话系统保留足够的上下文。