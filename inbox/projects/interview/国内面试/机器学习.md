---
title: "机器学习"
created: 2024-06-17
modified: 2024-06-24
---

## 决策树

构造方法： 信息增益、信息增益率、基尼指数；

剪枝：

- **预剪枝**：是否划分取决于是否提升模型性能（深度，节点样本数量，测试机准确度）；预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但是，另一方面，因为预剪枝是基于“贪心”的，所以，虽然当前划分不能提升泛化性能，但是基于该划分的后续划分却有可能导致性能提升，因此预剪枝决策树有可能带来**欠拟合的风险**。
- **后剪枝**：先构造一颗完整的决策树，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛化性能的提升，则把该子树替换为叶结点。后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛化性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。

## 分类数据不均衡

数据上的处理：

1. 过采样：SMOTE （高维数据采样的噪音较大）
2. 欠采样：由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息

模型训练上的处理：

1. Class weight：在损失计算中，加大少数数据的权重；
2. Focal loss：简单的实例权重减小，困难的实例权重增加；
3. Boosting

## 过拟合

1. 增加训练数据量：使用更大的数据集训练模型，可以帮助模型学习到更一般化的特征
2. 数据增强：通过对现有数据进行变换 (如旋转、缩放、添加噪声等) 来人为扩充数据集
3. 降低模型复杂度：减少神经网络的层数或神经元数量，使用更简单的模型架构
4. 正则化：在损失函数中添加惩罚项 (如 L1、L2 正则化) 来限制模型参数
5. 早停法 (Early Stopping)：在验证集性能开始下降时停止训练
6. 集成方法：结合多个模型的预测结果，如随机森林、Boosting 等
7. 特征选择：选择最相关的特征，去除不重要或冗余的特征
8. Dropout：在训练过程中随机丢弃一部分神经元，防止模型过度依赖某些特征
9. 交叉验证：使用 K 折交叉验证等方法来更准确地评估模型性能
10. 添加噪声：向输入数据或模型参数添加少量噪声，提高模型的鲁棒性