---
title: "自然语言常识"
created: 2024-06-17
modified: 2024-06-17
---

### NLP 常识

#### Word2vec

W2V 通常采用两种模式进行训练:

第一种：Continuous Bag of Words （CBOW）：使用上下文词预测中心词。

第二种：Skip-gram： 使用中心词预测上下文词。

##### 负采样

负采样通常在 Skip-gram 里面使用。它将模型的训练目标从通过中心词预测正向词到分类问题。正向词就是出现在上下文中的词，而负向词就是一小部分没有出现在上下文的词，通过采样获得。

分类的具体构成为：

1. $\log\sigma(v_\text{center} * v_\text{context}) + \sum\log\sigma(-v_\text{center} * v_{\text{neg}})$
2. 中心词和上下文词的相似度越高，中心词和负向词的相似度越低，那么损失就会越小

其中负采样的概率分布是在词频的概率分布上取 3/4 次方，以此鼓励从稀有词中进行采样。

负采样使得模型的损失计算量大大减小，加快了模型训练，也使得模型的 scalability 和性能得到提升。当然负采样的质量也会影响模型的最终效果。

##### 层次 softmax

用二叉树进行 softmax 的计算，叶子结点表示每个词，每条路径有对应的概率，从根节点到叶子间的概率积就是最终拟合的 softmax 的值。它将概率计算的复杂度从 $O(n)$ 减少到了 $O(\log n)$。

 概念上，它采用了 Huffman tree，对中间节点的方向（向左还是向右）进行直接建模。每个节点向左还是向右的概率和为 1。缺点是概率不如直接 softmax 精确， 而且构建这棵树也是比较耗时的。

#### 词嵌入模型的对比

早期的词嵌入模型包括 Word2vec（skip-gram 和 CBOW），fasttext，glove 和 elmo。

###### fasttext

在 word2vec 的基础上使用了 subword （character ngram），每个词的词嵌入就是 ngram 嵌入之和。能够有效解决 OOV 的问题。通常的使用场景是文本分类。

##### glove

通过向量点乘来拟合**共现频次矩阵**，实际效果比 Word2vec 好，缺点是计算量大。通常使用场景是相似度或对比。

##### elmo

通过双向 LSTM 进行 character 建模。能够有效捕捉上下文的信息，比传统的词向量更好解决多义词。当然，计算量也增加了。

#### BPE

原本是用于数据压缩的技术，被移用到 subword 分词上了。通常的训练步骤如下：

1. 提前分词（空格）
2. 开始时，所有的单个字符为初始词典；
3. 计算任意连续两个字符的出现频次；
4. 合并最高频的一对（产生新的词），把这一对出现的地方换成新的词，更新频次；
5. 重复 2-3，直到合并的次数或者词表的大小达到预定的大小；

#### Wordpiece

需要提前分词。

合并的选择是基于下面的计算方法（需要训练语言模型进行概率计算），不再是单纯的词频：

$$\text{argmax }\frac{p(ab)}{p(a)p(b)}$$

#### Unigram

#### Sentencepiece

用特殊词替换空格，不需要提前分词。因此更适合中文。支持训练 BPE 和 wordpiece 或者 unigram。

#### 熵

熵表示一个系统的不确定性程度。信息论中用来表示信息的不确定性，也叫做信息熵。

$$
 H(P) = - \sum_{i=1}^n P(x_i)\log P(x_i)
$$

熵越高，不确定性越大。

#### KL 散度

KL 散度通常用来描述两个分布之间的非对称性差别。具体来讲，就是分布 P 用分布 Q 来解释所丢失的信息。越小的话，两个分布越接近。也叫做相对熵。非对称且不为负。P 通常用来表示目标分布。Q 用来表示模型预测的分布。

    $$
    D_{KL} (P \lVert Q) = \sum_xP(x)\log\frac{P(x)}{Q(x)}
    $$

#### 交叉熵

交叉熵用来表示同一个变量，预测分布 Q 和真实分布 P 之间的差距。

$$
H(P,Q) = -\sum_{i=1}^nP(x_i)\log Q(x_i)
$$

它和 KL 散度之间的联系：

$$
D_{KL}(P \lVert Q) = H(P, Q) - H(P)
$$

也就是说，如果我们有真实分布 P，那么 KL 散度和 CE 没有区别。如果有真实分布，那么直接计算交叉熵更直接，通常用于分类/语言模型训练。

#### 困惑度

困惑度通常用于语言模型的大致质量评估。通俗来讲，就是预测一个词有多少可能。越小越好。但是它和下游模型的能力并没有直接关联，所以只能是一种参考，具体性能还是要看下游任务的评测。一般通过参考预料的困惑度进行模型对比。

$$
\text{Perplexity}(P) = 2^{H(P)}
$$

#### 信息增益

信息增益通常用于决策树分支选择。它用来描述一个数据集的熵在选择切分特征的变化。策略一般是选择信息增益最大的特征。

$$
IG(D, A) = H(D) - \sum_{j}^mP(a_j)H(D∣a_j)
$$

#### 基尼指数

Gini Impurity 也是用来决策树分支是的特征选择。指数越低，表示数据纯度越高，切分的效果越好。

$$
G(D)=1−\sum^n_{i=1}P(x_i)^2
$$

#### softmax 和交叉熵的联系

```python
import numpy as np

def softmax(x, dim=-1):
	x = np.asarray(x)
	nom = np.exp(x - np.min(x, axis=dim, keepdims=True))
	denom = nom.sum(axis=dim, keepdims=True)
	return nom / denom

def cross_entropy(logits, labels):
	# log softmax + negative log likelihood
	sm = softmax(logits, dim=1)
	return - np.sum(np.log(sm) * labels) / len(labels)
```

#### BERT

基于 Transformer 的编码器，由谷歌研发，在 2018 年发布。采用 Wordpiece。

BERT 使用了三种 embedding：token embedding + segment embedding（与 NSP 相呼应）+ position embedding (learned)

- bert-base (12 层，768， 12 heads， 110M）
- bert-large (24 层，1024， 16 heads， 340M）

大致参数计算

```python
def calculate(vocab_size: int, d_model: int, d_ff: int, num_layer: int):
    embedding = vocab_size * d_model
    pos = d_model * 512
    seg = d_model * 2
    qkv = d_model * d_model * 3
    proj = d_model * d_model
    ffn = d_model * d_ff * 2
	encoder = num_layer * (qkv + proj + ffn)
	return embedding + pos + seg + encoder
```

其中使用了两种损失函数：

1. Masked Language Modelling：15% 的 token 被随机采样，目标就是训练模型预测被采样的词。其中 80% 在输入是被替换成 $[MASK]$，10% 被随机替换，10% 保持不变。
2. Next Sentence Prediction：输入两个句子 A 和 B，目标是学习 B 是不是 A 的下一句。
3. 激活函数：GELU （连续可导，避免神经元死亡）

#### 不同的掩码策略

- Padding 掩码
- 注意力 causal 掩码
- 随机 token 掩码：BERT 中使用的随机掩码 （静态或者动态）
- 词掩码：帮助模型学习词的含义，而不仅仅是 subword token
- 知识/实体/短语掩码：鼓励模型学习高级别的语义信息

#### LSTM 和 GRU 对比

LSTM 和 GRU 都是 RNN 模型的一种。

##### LSTM

LSTM 通过多种门控制对信息进行取舍，适合长文建模。

1. 输入门：用来控制多少新信息被加入 （sigmoid）
2. 遗忘门：用来控制多少旧信息需要被遗忘（sigmoid）
3. 输出门：用来控制多少信息需要被输出（sigmoid）
4. Cell state：用来保留信息的参数（tanh）
5. Hidden state： 用来传递数据

##### GRU

GRU 是对 LSTM 的简化。只用了两种门控制且合并了 Cell state 和 Hidden state：

1. Reset 门：控制旧信息去留
2. Update 门：控制新信息流入

GRU 对比 LSTM 更快，更简单，但是长文处理上可能不如 LSTM。但都比传统的 RNN 更复杂，效果也更好，避免了梯度消失和爆照问题。

```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class RNNModel(nn.Module):
        
        def __init__(self):
            super().__init__()
            self.embed = nn.Embedding(100, 256)
            self.rnn = nn.GRU(256, 256, batch_first=True)
            self.ffn = nn.Sequential(
                nn.Linear(256, 1024),
                nn.ReLU(),
                nn.Linear(1024, 1),
            )
        
        def forward(self, x):
            
            embeddings = self.embed(x)
            output, *_ = self.rnn(embeddings)
            return self.ffn(output)
    
    inp = torch.randint(0, 100, (2, 30))
    model = RNNModel()
    output = model(inp)
    
    x, y = output[0], output[1]
    
    def cosine_similarity(x, y):
        norm_x = (x ** 2).sum().sqrt().item()
        norm_y = (y ** 2).sum().sqrt().item()
    
        return (x.T @ y / (norm_x * norm_y)).item()
    
    cosine_similarity(x, y)
```

#### Beam Search

每一步的推理，选取 top_k 的词而不是 max。计算量比贪婪算法高，但是质量提升。

```python
    def generate_random_dist(inp, vocab):
        # ignore inp for now
        dist = torch.softmax(torch.randn(vocab), dim=-1)
        return dist
    
    def beam_search(max_length: int, k: int = 2, vocab: int = 200):
        hypothesis = [([], 0)]
        for t in range(max_length):
            new_hypo = []
            for hypo, score in hypothesis:
                dist = score - torch.log(generate_random_dist(hypo, vocab))
                for i, new_score in enumerate(dist):
                    new_hypo.append((hypo + [i], new_score))
            new_hypo = sorted(new_hypo, key=lambda x: x[1])[:k]
            hypothesis = new_hypo
    
        return hypothesis
    
    beam_search(10, 5)
```

#### padding 的优缺点

优点

- 更适合 mini batch： 长度一致的情况下，可以更好地利用硬件进行并行处理。
- 保留上下文：比起 truncation，上下文能够得到保留。
- 动态 padding：按照数据的长度进行动态 padding，减少 mini batch 中 padding token 的比列，可以最大化利用计算资源。

缺点

- 更多的计算资源（内存和时间）耗费在了无关紧要的 token 上。特别是当 mini batch 中的数据长度变化很大的时候。

解决方案包括：

- 动态 padding
- Masking
- 序列 packing （多个序列变成一个序列）

#### bert vs roberta vs albert vs deberta

##### roberta

FB 提出的 BERT 优化版本，具体的优化体现在：

1. 只用了 MLM，没有 NSP；
2. 10x 训练数据量
3. 更好的超参

##### Albert

1. factorized embedding parameterization: 大大减小了嵌入层的参数量
	1. V x E → E x H → V x H
2. 参数共享
3. 没有 dropout
4. MLM + Sentence Oder Prediction

##### Deberta

1. disentangled attention：文本嵌入和位置嵌入分开计算注意力得分
	1. Q_content @ K_content
	2. Q_content @ K_pos
	3. Q_pos @ K_content
2. 增强 mask 解码器
3. MLM
4. 参数共享

##### BigBird

稀疏化的注意力机制

##### Transformer-XL

TODO

##### XLNet

TODO

##### BART

TODO

##### ELECTRA

TODO

##### ERNIE

TODO

#### 常见大模型对比

| 模型         | 结构        | 位置编码    | 激活函数        | 模型大小      | 主要贡献        | 目标函数                                           |
| ---------- | --------- | ------- | ----------- | --------- | ----------- | ---------------------------------------------- |
| GPT1       | 解码器       | Learned | ReLU        | 117M      |             | LM                                             |
| GPT2       | 解码器       | Learned | ReLU        | 1.5B      |             | LM                                             |
| GPT3       | 解码器       | Learned | GeLU        | 175B      | ICL         | LM                                             |
| ChatGPT    | 解码器       |         |             |           |             | RLHF                                           |
| GPT4       | 解码器（MOE）  |         |             |           |             |                                                |
| T5         | 编码器 - 解码器 | 相对位置    | ReLU        | 60M-11B   |             | Text-to-text                                   |
| OPT        | 解码器       | Learned | ReLU        | 125M-175B | 开源          | LM                                             |
| Llama      | 解码器       | Learned | GeLU、SwiGLU | 7B-70B    | 开源 小模型加大数据  | LM                                             |
| GLM        | 解码器       | 2D      | GeLU        | 10B-130B  | 开源 中英双语     | 自回归的空白填充，并通过 GLM 通过添加 2D 位置编码和打乱片段顺序来改进空白填充预训练 |
| QWen       | 解码器       | Learned | GeLU        | 0.5-72B   |             | LM                                             |
| Phi        | 解码器       | Learned | GeLU        |           | 合成数据        | LM                                             |
| BLOOM      | 解码器       | ALiBi   | GeLU        | 560M-176B | 多语言 + 开源    | LM                                             |
| Chinchilla | 解码器       |         |             |           | Scaling Law |                                                |
| ERNIE      | 解码器       | 相对位置    | GeLU        |           |             | 短语和实体 MASK + 持续多任务学习                            |
| mamba      | 解码器       |         | SwiGLU      |           |             |                                                |

#### mamba，xlstm

[[20240217115122|Mamba]]

xLSTM：

一种结合 sLSTM 和 mLSTM 的网络模型。

- sLSTM：加入了 normalizer state 用来计算 hidden state。输入和输出门用了指数门控制。
- mLSTM：cell state 变成了矩阵，采用了和注意力机制类似的 query，key 和 value projection。
- 使用残差网络结构，让更深层的网络架构成为可能。

#### focal loss

Focal Loss 基本思想是分类自信度高的例子会降权，自信度低的例子会增权。一般使用的场景就是有类别分布不均的情况。鼓励模型学习困难的（通常情况下占比较少的类别）例子：

$$
CE(p_t) = -y\log(p_t)\\FL(p_t)=−α(1−p_t)^γ\log(p_t)
$$

Gamma 越高，简单的例子（$p_t$ 越高）的权重就会越小。

Alpha 一般来说是 inverse class frequency。

#### Dropout

```python 
import torch  
import torch.nn as nn  
  
class Dropout(nn.Module):  
      
    def __init__(self, prob: float = 0.0):  
        super().__init__()  
        self.prob = prob  
      
    def forward(self, x):  
        if self.training:  
            dist = torch.distributions.binomial.Binomial(probs=1-self.prob)  
            return x * dist.sample(x) * (1 / (1 - self.p))  
        return x
```

`(1 / (1 - self.p))` 确保输出的期望不变。