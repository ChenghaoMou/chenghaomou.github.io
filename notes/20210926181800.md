---
aliases:
  - Mixture of Experts
  - Mixture of Experts
created: 2021-09-26
tags: []
title: "Mixture of Experts"
modified: 2024-04-21
---

# Mixture of Experts

Mixture of Experts (MoE) is special case of [[20210926153300|Adaptive Computation]] where multiple experts (usually subnets) are used to solve a problem by dividing the problem space into regions. In neural networks, a MoE model can use a gating/routing network to decide which expert/sub-model to use. But it has some limitations like **training stability**, **complexity**, and **communication cost**.