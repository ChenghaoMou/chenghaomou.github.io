---
aliases:
  - Mixture of Experts
  - Mixture of Experts
tags:
  - machine_learning
  - artificial_intelligence/deep_learning/optimization
title: "Mixture of Experts"
---

# Mixture of Experts

Mixture of Experts (MoE) is special case of [[20210926153300|Adaptive Computation]] where multiple experts (usually subnets) are used to solve a problem by dividing the problem space into regions. In neural networks, a MoE model can use a gating/routing network to decide which expert/sub-model to use. But it has some limitations like **training stability**, **complexity**, and **communication cost**.