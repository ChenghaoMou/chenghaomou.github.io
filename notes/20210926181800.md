---
aliases:
  - Mixture of Experts
created: 2021-09-26
date: 2021-09-26
modified: 2024-04-27
tags: []
title: "Mixture of Experts"
---

# Mixture of Experts

Mixture of Experts (MoE) is special case of [[20210926153300|Adaptive Computation]] where multiple experts (usually subnets) are used to solve a problem by dividing the problem space into regions. In neural networks, a MoE model can use a **gating/routing network** to decide which expert/sub-model to use. But it has some limitations like **training stability**, **complexity**, and **communication cost**, in addition to the hardware requirements like memory for the entire network.

In practice, FFNs are often used as the experts, but they can also be replaced with other networks or more MoEs[^1].

The routing network plays an important role in training stability, namely loading balancing as tokens are sent to different experts[^1]:
1. To make better use of the sparsity, usually some kind of **auxiliary loss** is added to the training objective to assure equal training among experts. One of them is z-loss, a loss designed to penalise large logits from the router output, preventing the network from *playing favorates*.
2. From the expert perspective, we can assign a **capacity threshold** to control how many tokens one expert could handle, where overflow tokens can be discarded or sent to the next layer. This is often defined as: $$
C = \frac{\text{number of tokens in this batch}}{\text{number of experst}} \times \text{scale factor}
$$where a large scale factor means one expert can handle more tokens at the cost of communication and risk of overfitting.

Finetuning a MoE model can also be challenging. Some ideas mentioned in [^1]:
1. Different drop out rate for MoE layers to prevent overfitting;
2. Freeze MoE parameters;
3. Use a smaller batch size and larger learning rate;
4. Larger dataset with the auxiliary loss;

[^1]: [[20240426084544|Mixture of Experts Explained]]