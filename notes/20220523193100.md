---
aliases:
- Transfer Learning Approaches
created: '2022-05-23'
title: Transfer Learning Approaches
---

# Transfer Learning Approaches

**Multitask Training**: Train the model on all tasks/datasets all at once;

**Sequential Multitask Training**: Train the model on some tasks/datasets first before continuing to train it with the target tasks/datasets;

**Multitask Training and Fine-tuning**: Train the model on some tasks first and fine-tuning it with target tasks/datasets;

Findings from [[lourie_2021|UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark]] that
1. Sequential Multitask Training almost always outperform other methods.
2. Multitask training helps the most when target data is scarce.
3. Larger models see more gains from transfer learning.