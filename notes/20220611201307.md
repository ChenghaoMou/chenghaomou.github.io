---
aliases:
- Current Issues with Natural Language Processing Evaluation
created: '2022-06-11'
tags: []
title: Current Issues with Natural Language Processing Evaluation
---

# Current Issues with Natural Language Processing Evaluation

Evaluation datasets or benchmarks are often targeted at narrow and specific tasks. Such that large pre-trained models are known to solve them. Thus it begs questions like what these models dont't know and what they are capable of that we don't foresee.

[[Srivastava et al_2022_Beyond the Imitation Game]] proposed BIG-bench as result to measure such capabilities with ever changing suite of tasks. It primarily targets at zero or few-shot learning evaluation so that it can reflect models' capability instead of its learning skill.

---
References:
- [@srivastava_2022](zotero://select/items/@srivastava_2022)