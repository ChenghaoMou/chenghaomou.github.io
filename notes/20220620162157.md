---
aliases:
- Machine Learning Optimization
created: '2022-06-20'
date: '2022-06-20'
modified: '2022-06-20'
references:
- https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html
tags: []
title: Machine Learning Optimization
---

# Machine Learning Optimization

In general, you can optimize your model locally or globally. Locally, it is when you improve operators, blocks, or layers of the model, while globally, it is when you optimize your model end to end.

Specifically, you can optimize your model in the following areas:
- **From loops to vectorization**: The most obvious optimization we are doing nowadays is putting several examples into a small batch to improve the efficiency. Some other libraries ([Jax](https://github.com/google/jax)) can automatically batch processing with a function designed for one single input.
- **Parallelization**: One example of this is using CNNs to approximate RNNs to utilize GPUs powerful parallel computation. Recurrence is often helpful, but sadly slow.
- **Hardware-specific optimization**: Depending on the hardware you are using, you can optimize your model, or data to be accessed/computed in a way that's easy for the hardware such as loop tiling.
- **Framework-specific or model-specific optimization**: e.g., operator fusion. `torch.nn.CrossEntropyLoss` is known to be better than separated `LogSoftmax` and `NLLLoss`.

Outside the scope of the model itself, you can rely on [[20210926153100|Machine Learning Compilers and Optimizers]] to further improve the performance.