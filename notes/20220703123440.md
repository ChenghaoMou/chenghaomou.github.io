---
aliases:
- Data Filtering
created: '2022-07-03'
references:
- https://openai.com/blog/dall-e-2-pre-training-mitigations/
tags: []
title: Data Filtering
updated: 2022-07-04 14:07
---

# Data Filtering

Source: [[DALL·E 2 Pre-Training Mitigations]]

When designing datasets and models, it is often a common practice to filter out low-quality or duplicate data so that trained models don't generate toxic output.

## Methods We Can Take to Improve Data Quality

- Active learning with human-in-the-loop annotation process. However, it might also introduce unintentional biases.
- Recall over precision:
  - You can always add new data to teach the model new things, but it is almost impossible to force the model to forget them.
- Avoid amplifying bias:
  - Up-sample data points that are affected during filtering.

## Deduplication

Duplicates incurs generating training data verbatim. This could lead to some legal and privacy issues.

Consider clustering before deduplication:

- Instead of all-pair calculation, you can reduce the computation significantly by focusing on intra-cluster deduplication.
- Try multiple different runs/seeds;
- The results are very robust against different $k$ values based on their experiments;

We often say [Garbage in, garbage out - Wikipedia](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out) in this [[20210928180400|Data-centric AI]] era, but it is worth-noting that we humans do not have such systematic filters to remove harmful content. Nor do we live in a censored environment where only positive things exist. Instead, we make intentional efforts to avoid being influenced by it, and more importantly, we also make sure we have proper sources, systems, and parenting to teach us what is wrong and what is right in a given social context. In this case, the models are still far away from "learning". But the question remains — how can we model these collective social guardrails besides just feeding data to the model?