---
aliases:
  - Confident Adaptive Language Modeling
  - Confident Adaptive Language Modeling
tags: []
title: "Confident Adaptive Language Modeling"
---

# Confident Adaptive Language Modeling

An [[20220723150024|Early Exiting]] example that tries to address these issues:
1. Confidence measure
2. Sequence level constraints at token-level exit decisions;
3. Attending previous tokens when decoding;

## Confidence Measure and Sequence-level Constraints

This framework uses a decaying threshold for each time step for exit decisions.

![[CleanShot 2022-07-23 at 15.45.45.png]]
The model becomes more relaxed as the generation reaches the end. This comes from the observation that early perturbations degrade the performance more than late perturbations.

Deciding which $\lambda$ to use come from a rigorous testing with a calibration dataset.

![[CleanShot 2022-07-23 at 15.51.14.png]]

Here, $L$ can be either
1. Dissimilarity measure that measures the distance for textual consistency between the generation from an early exiting model and a full model. The smaller, the better.
2. Risk consistency measure that measures the risk between two generations. The lower, the better.
This is where the sentence level constraints are incorporated into the decisions.

This framework uses $1 - \textit{metric}$ to calculate the two measures. The goal is to keep it small (smaller than the tolerance parameter $\delta$) while looking for a minimum $\lambda$ value.

## Attending Previous Tokens When Decoding

![[CleanShot 2022-07-23 at 16.13.38.png]]

At step 6, layer 5, previous steps' early layers' hidden states are copied directly as the previous layer's output. #todo Why is this needed if all inputs are concatenated for next step?