---
aliases:
  - Layer Normalization
  - "Layer Normalization"
linter-yaml-title-alias: Layer Normalization
order: -20220731103941
tags: [artificial_intelligence/deep_learning/transformers]
title: Layer Normalization
---

# Layer Normalization

Highlights: [[Natural Language Processing with Transformers]]

Definition: $\sigma(0, 1)$ normalization over a layer's activations. It is commonly used in architectures like [[20220905125225|Transformers]], but it is hard to train since it reduces the individual information about their relative norms.

![[CleanShot 2022-07-31 at 10.42.47.png]]
Source: [^1]

## Post-layer Normalization

Layer Normalization after skip connections. Gradients can diverge in this configuration, therefore, learning rate warm-up is often used. (WHY?) ^a66698

## Pre-layer Normalization

More stable than [[#Post-layer Normalization]] and generally does not require warm-up.

[^1]: [@xiong_2020](zotero://select/items/@xiong_2020)