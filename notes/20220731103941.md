---
aliases:
- Layer Normalization
created: '2022-07-31'
date: '2022-07-31'
modified: '2022-07-31'
tags: []
title: Layer Normalization
---

# Layer Normalization

Highlights: [[Natural Language Processing with Transformers]]

Definition: $\sigma(0, 1)$ normalization over a layer's activations. It is commonly used in architectures like Transformers[^1], but it is hard to train since it reduces the individual information about their relative norms.

![[CleanShot 2022-07-31 at 10.42.47.png]]

<center>Source: Xiong et al. 2020</center>

## Post-layer Normalization

Layer Normalization after skip connections. The underlying reason why we need a warm-up for Post-LN is that the scale of gradients can be very large as the depth increases initially. We have to control the step size first. But this is not the case for Pre-LN as it normalizes the gradients with respect to the depth. Gradients can diverge in this configuration, therefore, learning rate warm-up is often used.

## Pre-layer Normalization

More stable than [[#Post-layer Normalization]] and generally does not require warm-up according to [[xiong_2020|On Layer Normalization in the Transformer Architecture]].

[^1]: [[20220905125225|Layer Normalization in Transformers]]