---
aliases:
  - Layer Normalization
created: 2022-07-31
date: '2022-07-31'
modified: 2024-07-20
tags: []
title: "Layer Normalization"
---

# Layer Normalization

Highlights: [[Natural Language Processing with Transformers]]

Definition: $\sigma(0, 1)$ normalization over a layer's activations. It is commonly used in architectures like Transformers[^1], but it is hard to train since it reduces the individual information about their relative norms.

![[CleanShot 2022-07-31 at 10.42.47.png]]

<center>Source: Xiong et al. 2020</center>

## Post-layer Normalization

$$
\begin{align}
Y &= \text{Norm}(X + \text{Attention}(X))\\
O &= \text{Norm}(Y + \text{FFN}(Y))
\end{align}
$$

Layer Normalization after skip connections. The underlying reason why we need a warm-up for Post-LN is that the scale of **gradients** can be very large as the depth increases initially.

Gradients can diverge in this configuration, therefore, learning rate warm-up is often required.

## Pre-layer Normalization

More stable and faster to convergence than [[#Post-layer Normalization]] and generally does not require warm-up according to [[xiong_2020|On Layer Normalization in the Transformer Architecture]].

However, it might suffer from representation collapse meaning the performance might not be as good as Post Norm.

[^1]: [[20220905125225|Layer Normalization in Transformers]]