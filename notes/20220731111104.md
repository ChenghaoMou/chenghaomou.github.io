---
aliases:
  - Tokenization
tags: []
title: "Tokenization"
updated: 2022-11-22 19:57
---

# Tokenization

## Background

Common English tokenization algorithms are

1. splitting by characters or words;
2. [byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding), which joins frequently occurring bytes/characters together as a new token;
3. unigram that starts with all words and potential subwords and gradually remove them to reach ideal vocabulary size.

Comparison between popular implementations:

| Feature                                 |       SentencePiece      | [subword-nmt](https://github.com/rsennrich/subword-nmt) | [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) |
| :-------------------------------------- | :----------------------: | :-----------------------------------------------------: | :-----------------------------------------------: |
| Supported algorithm                     | BPE, unigram, char, word |                           BPE                           |                        BPE*                       |
| OSS?                                    |            Yes           |                           Yes                           |                  Google internal                  |
| Subword regularization                  |            Yes           |                            No                           |                         No                        |
| Pre-segmentation required?              |            No            |                           Yes                           |                        Yes                        |
| Customizable normalization (e.g., NFKC) |            Yes           |                            No                           |                        N/A                        |
| Direct id generation                    |            Yes           |                            No                           |                        N/A                        |

(Source: [google/sentencepiece](https://github.com/google/sentencepiece#comparisons-with-other-implementations))

## Tokenization Metrics

1. Coverage
   1. Percentage of unknown words in the corpus
   2. Percentage of "rarely used tokens" in a corpus
2. Fragmentation
   1. Percentage of words that are split
3. Subword Fertility
   1. Average number of subwords per word
