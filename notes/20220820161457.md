---
aliases:
- Expert Language Model Forest
created: '2022-08-20'
tags: []
title: Expert Language Model Forest
---

# Expert Language Model Forest

Source [^1] [^2]

## Expert Language Model

Language models trained on specific domain corpus. They are considered domain experts as a result.

## Expert Language Model Forest

By ways of averaging (normal, $argmax$, weighted) or ensembling a group of [[#Expert Language Model]]s, a ELM forest can:
1. outperform a baseline Transformer LM;
2. be trained in parallel and incrementally with less cost;
3. good options to cost-sensitive (weighted average) applications and as well as performance-sensitive (ensemble with domain posterior) applications.

However, the question remains, do we have such compartmentalization in our brain as well? According to [@piantadosi_2022](zotero://select/items/@piantadosi_2022), ![[20220814211819#^9abb87]]
shouldn't such interrelation, instead of isolation, help models' "understanding"?

One piece of evidence I can see in the paper is that, an ensemble forest at 1B parameter scale (125M * 8 ELMs) actually performs worse than a 1B dense model, even though they are comparing them at per GPU level (125M per GPU vs. 125M model).

[^1]: [@li_2022c](zotero://select/items/@li_2022c)
[^2]: [[text.markdown]]