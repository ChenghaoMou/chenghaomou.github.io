---
aliases:
- Branch-Train-Merge
created: '2022-08-20'
date: '2022-08-20'
modified: '2022-08-20'
tags: []
title: Branch-Train-Merge
---

# Branch-Train-Merge

Source [^1][^2]

A strategy to train a [[20220820161457|Expert Language Model Forest]] asynchronously. It started with one LM that is trained on seed data. Then, a new LM is introduced by ensembling relevant LM(s) in the forest and is trained on a new domain dataset. After the training, the new [[20220820161457#Expert Language Model]] is added to the forest.

It is shown that ensembling domain-specific models is better than models trained on random splits. The resulting forest, similar to [[20220724140856|Model Soups]], outperforms baseline models while using comparable compute. Though, the compute cost can vary a lot depending on the ensemble strategy.

[^1]: [[text.markdown]]
[^2]: [@li_2022c](zotero://select/items/@li_2022c)