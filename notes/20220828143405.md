---
aliases:
  - ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION
  - '"TRAIN SHORT'
  - 'TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION"'
linter-yaml-title-alias: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION
order: -20220828143405
tags: 
title: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION
---

# ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION

References: [ALiBi enables transformer language models to handle longer inputs](https://www.youtube.com/watch?v=Pp61ShI9VGc)

Most pre-trained language models use certain context length during training, and unfortunately most of them do not extrapolate to longer text during inference:

1. It is simply not possible to extrapolate using trained embeddings because you don't have the corresponding learned weights;
2. Fixed positional embeddings (sinusoidal) do not work as well.
3. Rotary positional embeddings perform better than sinusoidal, but still not very good.
4. [T5 Bias](TODO) seems to extrapolate better than aforementioned embeddings, to some extent.

> Hypothesis: models are overfitting to the positional embeddings and larger models are more easily overfitting (reverse scaling law?)

## Implementation

1. Using the last work in the sequence as a query;
2. Using the rest of the sequence as keys;
3. Calculate the dot products and subtract some biases manually set;

$$
\begin{align}
word_{L-1}^{Q} \cdot word_{i}^{K} - (L-i) \times m\\
m = 2^{\frac{-8}{n}}
\end{align}
$$

![[CleanShot 2022-08-28 at 15.00.36.png]]

## Takeaways

- Alibi consumes a longer context, but still uses a shorter context internally.
- Using [[20220828154238|Sliding Window Evaluation]], it shows a flat perplexity curve rather than an exploding curve from baseline models.