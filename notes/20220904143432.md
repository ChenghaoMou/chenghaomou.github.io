---
aliases:
  - Form, Meaning, and Communicative Intents
  - Form
  - Meaning
  - and Communicative Intents
title: "Form, Meaning, and Communicative Intents"
---

# Form, Meaning, and Communicative Intents

Highlights: [[Climbing towards NLU On Meaning Form and Understanding in the Age of Data]]

Understanding is, proposed in this paper, the reconstruction of communicative intents from forms (observable realization of language), while meaning is the relation between the two.

Communicative intents can take many forms:
1. to convey information;
2. to convey instructions;
3. to socialize;

```
                                                                              
◀────────────────────────────────────Express──────────────────────────────────
                                                                              
┌─────────┐   ┌──────────────────────────────┐  ┌────────────────────────────┐
│  Form   │   │     Conventional Meaning     │  │  Communicative Intentions  │
└─────────┘   └──────────────────────────────┘  └────────────────────────────┘
                                                                              
 ────────────────────────────────Understand──────────────────────────────────▶
                                                                              
```

Additionally, there is Conventional Meaning in the middle, which creates a context-independent view of the form, and when interpreted by the listener, the real communicative intentions are reconstructed. However, we often attribute such communicative intentions to objects that don't even have them in the first place, such as language models.

One argument of this paper is that language or text needs to be grounded with [[20220814212300|Referents]] and was challenged in [[20220814211819|Meaning and Reference]] from [[Piantadosi_Hill_2022_Meaning without reference in large language models]].

However, I don't really see these two opinions being mutually exclusive. As mentioned in this paper, tasks such as reading comprehension require, as well as include, weak information beyond the text that models might seem to be able to pick up. But that's beyond the scope of this paper.

For language models, the ability to model interrelation between [[20220814211819#^9abb87|Conceptual Roles]] does not negate the fact such conceptual roles or representational states can be ungrounded. (**And no one can really say for sure that modern LM training corpus are free of any kind of training data)**

Yes, the interrelation with forms (which can be seen as a tiny fraction of the world) can be seen as some level of meaning. But such meaning would be a castle in the air, ungrounded, no matter how fluent or plausible the construction is.

![[What Kind of Mind Does ChatGPT Have-|What Kind of Mind Does ChatGPT Have?]]

To learn meaning in a more grounded way or to move forward in general, some suggestions are proposed:
1. Training with multi-modality; ^423031
2. Asking top-down questions about our directions — choosing the right hill to climb;
3. Being skeptical, which is different from being pessimistic;
4. Saying something as it is — a model captures some **reflection of meaning** from the form which can be very useful in applications.

References:
[[Thought experiment in the National Library of Thailand|Thought experiment in the National Library of Thailand]]