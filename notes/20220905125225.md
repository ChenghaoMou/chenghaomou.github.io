---
aliases:
  - Layer Normalization in Transformers
tags: []
title: "Layer Normalization in Transformers"
---

# Layer Normalization in Transformers

Highlights: [[xiong_2020|On Layer Normalization in the Transformer Architecture]]

Layer normalization mainly has two ways of implementation in Transformers:
1. [[20220731103941#Pre-layer Normalization]]: adding layer normalization before self-attention and FNN;
2. [[20220731103941#Post-layer Normalization]]: adding layer normalization after residual connections;

It is shown that Post-LN suffers from additional hyperparameters (learning rate warm-up) and, as a result, slower training. Experiments confirm the hypothesis that training with Pre-LN and without warm-up can achieve comparable results faster.