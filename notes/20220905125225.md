---
aliases:
  - Layer Normalization in Transformers
  - "Layer Normalization in Transformers"
linter-yaml-title-alias: Layer Normalization in Transformers
order: -20220905125225
tags: 
title: Layer Normalization in Transformers
---

# Layer Normalization in Transformers

Highlights: [[On Layer Normalization in the Transformer Architecture]]

Layer normalization mainly have two ways of implementations in Transformers:
1. [[20220731103941#Pre-layer Normalization]]: adding layer normalization before self-attention and FNN;
2. [[20220731103941#Post-layer Normalization]]: adding layer normalization after residual connections;

It is shown that Post-LN suffers from additional hyperparameters (learning rate warm-up) and, as a result, slower training. Experiments confirms the hypothesis that training with Pre-LN and without warm-up can achieve comparable results faster.

The underlying reason why we need a warm-up for Post-LN is that the scale of gradients can be very large as the depth increases initially. We have to control the step size first. But this is not the case for Pre-LN as it normalizes the gradients with respect to the depth. (This answers the question in [[20220731103941#^a66698]])

This is also adopted in [[4archives/highlights/Xiong et al. - 2022 - Simple Local Attentions Remain Competitive for Lon.textbundle/text]].