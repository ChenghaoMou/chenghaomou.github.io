---
aliases:
  - "Goodhart's Law"
created: 2022-11-13
date: '2022-11-13'
modified: 2024-08-11
tags: []
title: "Goodhart's Law"
---

# Goodhart's Law

> When a measure becomes a target, it ceases to be a good measure.

An example of Goodhart's law in Machine Learning is that we often use a differentiable metric for training but a different objective for evaluation. In the case of overfitting, the training metric continues to improve while the *evaluation metric* stagnates or even deteriorates. Other examples can be found in saturated evaluation benchmarks in recent years, such as the GLUE and SuperGLUE challenges, and new benchmarks (e.g. ARC-AGI[^1]) with targeted solutions. The pace of those benchmarks been solved is astonishing, yet we still haven't seen the intelligence they set out to evaluate.

Such deterioration is described as the **strong version of Goodhart's law**[^2]. Compounded with the metric black hole from [[31554997|Deep Work]] in modern office work settings, it would negatively impact our productivity even more.

Another example of this in academia is the number of paper published. The unintended consequences of this is that people start to yield papers with inconsequential contributions and sometimes fake results. Other metrics like h-index are also imperfect in a sense that there exists citation bias[^3].

[^1]: [[On the “ARC-AGI” $1 Million Reasoning Challenge|On the “ARC-AGI” $1 Million Reasoning Challenge]]
[^2]: [[Too much efficiency makes everything worse- overfitting and the strong version of Goodhart’s law|Too much efficiency makes everything worse: overfitting and the strong version of Goodhart’s law]]
[^3]: [[Goodhart’s Law and Scientific Innovation in Academia|Goodhart’s Law and Scientific Innovation in Academia]]