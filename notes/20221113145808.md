---
aliases:
  - "Goodhart's Law"
tags:
title: "Goodhart's Law"
---

# Goodhart's Law

References:
- [Goodhart's law - Wikipedia](https://en.wikipedia.org/wiki/Goodhart%27s_law)

> When a measure becomes a target, it ceases to be a good measure.

An example of Goodhart's law in Machine Learning is that we often use a differentiable metric for training but a different objective for evaluation. In the case of overfitting, the training metric continues to improve while the *evaluation metric* stagnates or even worsens.

Such deterioration is described as the **strong version of Goodhart's law**[^1]. Compounded with the metric black hole from [[31554997|Deep Work]], it would negatively impact our productivity even more.

Another example of this in academia is the number of paper published. The unintended consequences of this is that people start to yield papers with inconsequential contributions and sometimes fake results. Other metrics like h-index are also imperfect in a sense that there exists citation bias[^2].

[^1]: [[Too much efficiency makes everything worse- overfitting and the strong version of Goodhart’s law|Too much efficiency makes everything worse: overfitting and the strong version of Goodhart’s law]]
[^2]: [[Goodhart’s Law and Scientific Innovation in Academia|Goodhart’s Law and Scientific Innovation in Academia]]