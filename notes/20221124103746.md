---
aliases:
- Research Like an Engineer
created: '2022-11-24'
date: '2022-11-24'
modified: '2022-11-24'
tags: []
title: Research Like an Engineer
---

# Research Like an Engineer

What exactly does it take to become a good machine learning engineer?

This is mainly my thoughts on reading the paper [[taylor_2022]] [^1] and the drama of it on Twitter.

The paper starts off with the information overload issue, which is valid, in my opinion. But it bothers me because it didn't go into detail on what frictions or pain points researchers have when doing research. I fail to see the assumption they implicitly have that what the model is capable of is what people need (case in point: the related work section has nothing to do with researching itself or information overload).

There is a saying that I couldn't find a source and I think is appropriate: Ask not what a model can do, instead, ask what the issues are and how a model can help. The issue here, I think, is the lack of a clear definition of the problem, and it reminds me of the work experience I have as an engineer.

High-level decisions from leaders or product managers are ambiguous and often require many back-and-forth discussions to figure out what they mean. It could be a user complaint, or our competitor is doing so, or they don't even know. Typically, they don't work with a product manager, which means little to no systematic access to stakeholders' opinions. Imagine this, if all they can do is from a research lab without any meaningful connection to the real user, how can I trust their results that they hope to have an impact on the real world?

That's basically the point I want to make here. Building something user-facing frequently has more stake than a niche research project because you get direct user feedback, whether a slash back or a warm welcome. You have to test your idea with tons of user research because it is no longer a thought experiment based on individual experience.

![[taylor_2022#^5ce274]]

You can't expect to have trust in your model and yet completely avoid the responsibility of building a trustworthy system.

In order to be trustworthy, we need to be more thoughtful (like asking [[20220925095941|Ethical Questions to Ask in AI]]) when doing research and to have higher standards, not just models, but systems, and to ensure the integrity of each component.

[^1]: [@taylor_2022](zotero://select/items/@taylor_2022)