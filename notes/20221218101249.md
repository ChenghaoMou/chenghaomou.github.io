---
aliases: [Constitutional AI, "Constitutional AI"]
tags: []
title: "Constitutional AI"
---

# Constitutional AI

Reference: [[bai_]]

**Main motivation**: model outputs are often evasive when it comes to sensitive topics. This is "harmless" but not "helpful". To engage in a better way, models should provide better responses.

The idea of constitutional AI is that such generative models are "governed" with a set of rules/principles. In the paper, a set of 16 principles were used for both helpfulness and harmlessness.

**Method**: This extends the idea of RLHF (reinforcement learning with human feedback) by replacing human feedback with AI critics and revisions.

It starts with a helpful but toxic model, trained with RLHF and helpfulness feedback. It is then tasked to critic its helpful but harmful responses with random sampled principles embedded in the prompt. At this point, you have a "red team" query and a few revisions that reduce the harmfulness while preserving the helpfulness.

You can either fine-tune a model with such data (SL-CAI) or further train a model with reinforcement learning with such data (RL-CAI).

## Comments

### Future Work

![[bai_#^3857c9]]

![[bai_#^aada6a]]

There is too much valuable work to be done for future work or hidden away in Appendix. How can we start to work together beyond our research silos when the so-called AI is inevitably taking a more interdisciplinary direction? Do we hold researchers accountable for doing research experiments without proper user study, social study, and ethical study[^1]?

### Chain-of-thought

 ![[bai_#^81dd1a]]

There is no real "decision-making", period. To me, the success of chain-of-thought prompting is the perfect evidence that those models don't have the capability of "reasoning". The fact we have to work our input around the model overlord shows how much human work is still required, but often understated.

[^1]: [[20221210185344|On the Dangers of Large Generative Models]]