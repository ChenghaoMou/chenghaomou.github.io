---
aliases:
- Random Ideas
- Random Ideas
created: '2022-12-21'
date: '2022-12-21'
modified: '2022-12-21'
tags: []
title: Random Ideas
---

# Random Ideas

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">#4: Accepting edits from your PI without understanding them<br><br>üñçÔ∏è Critical feedback from your supervisor can feel discouraging but it really is an opportunity to learn. So, ask for the reasons of edits and rewrites so you can refine your writing skills one iterations at a time.</p>&mdash; Anna Clemens, PhD (@scientistswrite) <a href="https://twitter.com/scientistswrite/status/1604869672725352448?ref_src=twsrc%5Etfw">December 19, 2022</a></blockquote>

How can I incorporate the motivation/why into model training instead of just how (gradient update)?

Can I train a model to predict answers for previous batches?

Multi-modality (document images, text, locations), multipage, and multiscale (sections, paragraphs, sentences, words, characters) in one system.

Model optimization compilation (different providers in ONNX, deepsparse, triton, Intel etc.)

An NER dataset for books, authors, links, relations

If there is anything pre-trained large language models can teach me, I think it is the importance of reading, a lot of reading.

A visual and interactive regex builder by highlighting text you want to extract and the level of granularity you intend to achieve.