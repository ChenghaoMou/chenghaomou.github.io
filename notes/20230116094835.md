---
aliases:
- Modeling Beliefs and Communicative Intents
created: '2023-01-16'
tags: []
title: Modeling Beliefs and Communicative Intents
---

# Modeling Beliefs and Communicative Intents

Tu understood how models are modeling beliefs and [[20220904143432|Communicative Intents]], we need to define those terms first with a framework proposed by Bratman in 1987[^1].
1. An agent, usually an intellectual being, has a set of beliefs $B$ regarding the world â€” a [[20221120094433|Cognitive Model]].
2. Such agent also has some desires $D$.
3. Combined, desires and beliefs form communicative intents $I \sim P_{intention}(\cdot|B, D)$, and as a result, an utterance $U \sim P_{utterance}(\cdot | I)$is produced and observed, interacting with the environment.
4. The interaction, in return, informs and updates the mental world model.

This is kind of a rebuttal to [[20220904143432|Form, Meaning, and Communicative Intents]] where it is argued that no communicative intents can be learned because they are not present in the datasets, but here it is saying that it is possible, even without explicitly trained to do so.

The main argument of this stance is that: Multiple studies have found that without explicitly training for intents or beliefs, including the sketch experiment in the paper, neural networks do capture some narrow aspects of them[^2] [^3] [^4] [^5]. Because for an **effective** LM, it must learn to maintain the constraints based on the abovementioned framework to generate coherent text, both in terms of beliefs and communicative intents.

However, such modeling is very narrow and restricted in its applicability and reliability. To achieve a better "understanding", we need a paradigm shift to overcome the current limitation of model architectures, and to understand those models better[^6].

Question: is it sufficient for a model to "learn" all things about "This is different from That" without understanding truth and falsity?

[^1]: [[andreas_2022#^edfcf1|Language Models as Agent Models]]
[^2]: [[andreas_2022#^346bc2|Language Models as Agent Models]]
[^3]: [[andreas_2022#^020a41|Language Models as Agent Models]]
[^4]: [[Google AI Blog- Google Research, 2022 & Beyond- Language, Vision and Generative Models|Google AI Blog: Google Research]]
[^5]: [[Large Language Model- world models or surface statistics-|Large Language Model: world models or surface statistics?]]
[^6]: [[andreas_2022#^b62890|Language Models as Agent Models]]