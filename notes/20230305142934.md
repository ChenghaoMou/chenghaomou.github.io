---
aliases:
  - Anthropomorphizing AI
linter-yaml-title-alias: Anthropomorphizing AI
order: -20230305142934
tags: []
title: Anthropomorphizing AI
---

# Anthropomorphizing AI

Autoregressive generative models are getting better at producing fluent text every day. This might be helpful in some use-cases, but their emotions are no more real.

Making the generated text more human-like has nothing to do with the search tasks. But we also don't know if this is caused by careful design, or lack thereof.

> Work on synthetic human behavior is a bright line in ethical AI development, where downstream effects need to be understood and modeled in order to block foreseeable harm to society and different social groups.
> -- [@bender_2021](https://doi.org/10.1145/3442188.3445922)

I always wonder why most voice assistants haven't really picked up any "personality" over the decade. Maybe there are some lessons learned in that community that haven't been propagated to other areas, like how security is finding its way into LLM demos just now ([[20230226150428|Prompt Engineering]]).

We have yet to see the full impact of such systems on people, and on the society at large[^1].

[^1]: [[People keep anthropomorphizing AI Here’s why|People keep anthropomorphizing AI. Here’s why]]