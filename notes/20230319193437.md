---
aliases:
- Truth and Falsity
created: '2023-03-19'
tags: []
title: Truth and Falsity
---

# Truth and Falsity

We approach things with a set of beliefs and perspectives that may or may not be shared by others. However, large pre-trained models treat all the information with equal facility[^1], and there is no inherent mechanism for discriminating truth from falsity[^2]. Even with reinforcement from human feedback, the generation is no less a probabilistic sampling, and there shouldn't be anything probabilistic regarding beliefs.

Would it be possible to ask a model to dynamically weight its training documents once it reaches some stable stage during pretraining? Maybe [[20221218101249|Constitutional AI]] can be directly incorporated into the training by weighting documents discriminatively. Is this a form of self-reinforcement?

Another issue with the inability to distinguish truth from misinformation, compounded with the wide release of generative APIs, is that the fake or low-quality information gets recycled into the internet and potentially amplified through other clueless models[^3]. It is bad enough that we have to ingest such information firsthand; now we must suffer repeatedly[^4].

[^1]: [[Noam Chomsky- The False Promise of ChatGPT|Noam Chomsky: The False Promise of ChatGPT]]
[^2]: [[shanahan_2022|Talking About Large Language Models]]
[^3]: [[ChatGPT sometimes makes up facts. For one law prof, it went too far.|ChatGPT sometimes makes up facts. For one law prof]]
[^4]: [[The Expanding Dark Forest and Generative AI|The Expanding Dark Forest and Generative AI]]