---
aliases:
  - Construct Validity
linter-yaml-title-alias: Construct Validity
order: -20230321205856
tags: []
title: Construct Validity
---

# Construct Validity

Construct validity reflects to which extent the indicators/metrics/scores can represent a concept or an ability that is not directly measurable[^1] [^2].

In the realm of machine learning, we often create benchmarks under the assumption that having a high score, sometimes higher than humans, means having some level of intelligence like humans, but this is rarely true at all. When we look at how OpenAI boasting about GPT-4's performance on some standard tests, data leakage issues aside, the construct validity was never thoroughly tested or at least stated[^3]. This reminds me of the metric black hole mentioned in [[Deep Work]] â€” we still don't know much about how to test intelligence, and this is just creating distractions and hypes.

[^1]: [Construct validity - Wikipedia](https://en.wikipedia.org/wiki/Construct_validity?useskin=vector)
[^2]:[[AI Causes Real Harm. Let&rsquo;s Focus on That over the End-of-Humanity Hype|AI Causes Real Harm. Let&rsquo;s Focus on That over the End-of-Humanity Hype]]
[^3]: [[GPT-4 and professional benchmarks the wrong answer to the wrong question|GPT-4 and professional benchmarks: the wrong answer to the wrong question]]