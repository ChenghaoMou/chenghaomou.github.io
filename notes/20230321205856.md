---
aliases:
- Construct Validity
created: '2023-03-21'
date: '2023-03-21'
modified: '2023-03-21'
tags: []
title: Construct Validity
---

# Construct Validity

Construct validity reflects to which extent the indicators/metrics/scores can represent a concept or an ability that is not directly measurable[^1] [^2].

In machine learning, we often create benchmarks under the assumption that having a high score, sometimes higher than humans, means having some level of intelligence like humans, but this is rarely true. When we look at how OpenAI boasts about GPT-4's performance on some standard tests, data leakage issues aside, the construct validity was never thoroughly tested or at least stated[^3]. This reminds me of the metric black hole mentioned in [[31554997|Deep Work]] â€” we still don't know much about how to test intelligence. This is just creating distractions and hype.

[^1]: [Construct validity - Wikipedia](https://en.wikipedia.org/wiki/Construct_validity?useskin=vector)
[^2]:[[AI Causes Real Harm. Let&rsquo;s Focus on That over the End-of-Humanity Hype|AI Causes Real Harm. Let&rsquo;s Focus on That over the End-of-Humanity Hype]]
[^3]: [[GPT-4 and professional benchmarks- the wrong answer to the wrong question|GPT-4 and professional benchmarks: the wrong answer to the wrong question]]