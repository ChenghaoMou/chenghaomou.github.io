---
aliases:
- AI Alignment
created: '2023-07-30'
tags: []
title: AI Alignment
---

# AI Alignment

![[Eugenics and Longtermist Effective Altruism Share the Same Original Sin#^34c39d|Eugenics and Longtermist Effective Altruism Share the Same Original Sin]]

![[‘A certain danger lurks there’- how the inventor of the first chatbot turned against AI#^87423f]]

Alignment is problematic in the sense that even if it is successful, people will attribute more power and more decision-making judgments to a model and therefore **avoid responsibility**. And the reality is that it is not even remotely perfect, and people are already shifting all the burden and responsibility to the end recipient.