---
aliases:
- Speculative Decoding
created: '2023-12-03'
date: '2023-12-03'
modified: '2023-12-03'
title: Speculative Decoding
---

# Speculative Decoding

Source: [[leviathan_2023|Fast Inference from Transformers via Speculative Decoding]]

Decoding easy tokens should not require a full forward pass of a dense model, as this can be achieved using a relatively small one at a lower cost. Consider a large model, denoted as $p$, and a small model, denoted as $q$.

The process is as follows:
1. Generate $\gamma$ tokens using $q$;
2. Evaluate those tokens using $p$ with token probabilities and distributions. Additionally, an new token distribution after $\gamma$ tokens is also available;
3. Accept the prediction $x_t$ of $q$ if $p_q(x_t) \le p_p(x_t)$ or accept with a probability $\frac{p_p(x_t)}{p_q(x_t)}$.
4. If rejected, then sample $x_t$ again from the $(P_p(x) - P_q(x))_+$. This is proven to be equivalent to sampling directly from $p$;

This process is guaranteed to generate at least one token from $p$ and at most $\gamma + 1$ tokens, all in one go.

When adopting this method, there are a few parameters that require attention:
1. How good is the small model at approximating the large one's distribution $\alpha$?
2. How many tokens are outsourced to the small model $\gamma$?
3. How cost-effective is the small model compared to the large model $c$?