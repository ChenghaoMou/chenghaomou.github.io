---
aliases:
  - Hallucination is Here to Stay
title: "Hallucination is Here to Stay"
---

# Hallucination is Here to Stay

On the surface, the word itself is too anthropomorphic — ascribing too much psychological affordance to a neural network. It also euphemises the underlying unreliable synthesis — essentially [[20230115161911|Bullshit]] — as if it is something we can get rid of easily with a panacea.

Unfortunately, it seems that the name and the underlying phenomena are here to stay.

All model generations are unreliable, but some are more useful than others. People also said that hallucination, to the model itself,  is the feature, not the bug, but a bug indeed at the system/assistant level, and they deem it fixable[^1]. Others share little of such optimism[^2] [^3] [^4].

[^1]: [[35423015|# On the "hallucination problem"]]
[^2]: [[32882444|Muddles About Models]]
[^3]: [[ChatGPT Is a Blurry JPEG of the Web|ChatGPT Is a Blurry JPEG of the Web]]
[^4]: [[“If It Sounds Like Sci-Fi, It Probably Is”|“If It Sounds Like Sci-Fi, It Probably Is”]]