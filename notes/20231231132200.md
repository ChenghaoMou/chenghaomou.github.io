---
aliases:
- Sliding Window Attention
created: '2023-12-31'
date: '2023-12-31'
modified: '2023-12-31'
title: Sliding Window Attention
---

# Sliding Window Attention

Source: [[jiang_2023|Mistral 7B]]

![[highlights/Archive/Xiong et al. - 2022 - Simple Local Attentions Remain Competitive for Lon.textbundle/text#^0125cb|text]]

At timestamp $t$, position $i$ at layer $l$ only attends to positions from $i - W$ to $i$ from the same layer, where $W$ is the window size. Recursively, with $k$ layers, one position can theoretically attend to $k \times W$ positions at the end.