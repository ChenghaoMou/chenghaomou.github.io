---
aliases:
- Self-Extended Attention
created: '2024-01-13'
title: Self-Extended Attention
---

# Self-Extended Attention

Source: [[jin_2024|LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning]]

Key insights:
- Humans can naturally generalise to long contexts by learning from short ones;
- Precise positional information might not be crucial for most tasks where relative ordering can be sufficient;

Key contributions:
- Using floor operation to map a long context window to a short one within the limit of learned positions. This is equivalent to bucketing the position embeddings where neighbouring positions might use the same encodings. This is controlled by a parameter called group size, typically from 2 to 8.
- To further maintain the model's performance impacted by the introduction of the floor operation, the authors proposed using the original attention values for tokens within a neighbour window because such tokens play an important role in the generation of future tokens in the sense of locality. This is parameterised with a neighbour window size.

![[jin_2024#^8555ea|LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning]]

These two methods are combined and softmaxed to derive the final attention weights.  However, based on the pseudo-code, it looks like there are two sets of attention matrix multiplications  â€” one normal and one floored. Further optimisations such as FlashAttention can be used.