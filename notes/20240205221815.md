---
aliases:
- Text Closeness Is Use Case Dependent
created: '2024-02-05'
date: '2024-02-05'
modified: '2024-02-05'
title: Text Closeness Is Use Case Dependent
---

# Text Closeness Is Use Case Dependent

Unfortunately, we seem to abuse the word similarity a lot even in the context of NLP applications. We have seen models using contrastive learning objectives with either symmetric or asymmetric data for embedding learning tasks. A simple example from [[nussbaum_2024|Nomic Embed: Training a Reproducible Long Context Text Embedder]] clarifies the difference:

Q: *What is the capital of France?*

Candidate 1: *What is the name of the capital city of France?*
Candidate 2: *Paris is the capital city of France*

In retrieval or QA tasks, candidate 2 would be considered **similar** to the query while in clustering or semantic similarity tasks, candidate 1 is closer to the query.