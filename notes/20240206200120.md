---
aliases:
- Proxy Training
- Title
created: '2024-02-06'
title: Proxy Training
---

# Proxy Training

Source: [[20240203150054|Research Papers in January 2024 - by Sebastian Raschka, PhD]]

For generative neural networks, the knowledge/learning can be approximated by the distribution difference during sampling between a trained model and an untrained one.

 It is much easier to measure the difference and transfer such difference to similar models (with the exact same vocabulary) than transferring the information trained and encoded in the parameters. Additionally, the learning/training can be done with a small model and the hot transfer can be applied to a large one.

Base Model $M_1$: a small model that is untrained, e.g. Llama 2 7B.
Trained Model $M_2$: a small model that is trained with desire behaviours, e.g. Llama 2 7B Chat.
Target Model $M_3$: a large model such as Llama 2 70B.

The learning can be represented as $M_2(x) - M_1(x)$, and the final logits can be calculated as $M_3^*(x) = M_s(x) + M_2(x) - M_1(x)$ before the final softmax and sampling.