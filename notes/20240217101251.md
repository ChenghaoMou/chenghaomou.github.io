---
aliases:
- State Space Models
created: '2024-02-17'
title: State Space Models
---

# State Space Models

Sources:
- [Piped](https://piped.video/watch?v=8Q_tqwpTpVU)
- [Piped](https://piped.video/watch?feature=youtu.be&v=vrF3MtGwD0Y)

SSM is often used to model continuous signals with respect to time. A typical fomula for SSM can be expressed as:
$$
\begin{align}
h'(t) &= Ah(t) + Bx(t) \\
y(t) &= Ch(t) + Dx(t)
\end{align}
$$
where $h(t)$ is the state representation,  $x(t)$ is the input and $y(t)$ is the output.

- $A$: how previous state impacts current state (in continuity);
- $B$: how input impacts state;
- $C$: how state impacts output;
- $D$: how input impacts output; Often skipped as this can be independently calculated with a skip connection.

Based on the fomulation, the relationship between input and output is linear and time-invariant because $A$, $B$, $C$, and $D$ are fixed.

However, in reality, we work with sampled and discret signals instead, so, our requirements for finding $h(t)$ becomes finding an approximation of discretised $h(k\Delta)$ where $\Delta$ is the step size, which can be learned through BP;

## Discretisation and Recurrent Formulation

Given that the derivative can be calculated by limitation:
$$
\underset{\Delta \rightarrow 0}{limit} \frac{h(t + \Delta) - h(t)}{\Delta} = h'(t)
$$
we now have this if we choose a really small $\Delta$:

$$
\begin{align}
h(t + \Delta) &\cong \Delta h'(t) + h(t) \\
&= \Delta (Ah(t) + Bx(t)) + h(t) \\
&= (I + \Delta A)h(t) + \Delta Bx(t) \\
&= \bar{A}h(t) + \bar{B}x(t) \\
\end{align}
$$
which makes $h(t + \Delta)$ recurrent:

$$
\begin{align}
h_t &= \bar{A}h_{t - 1} + Bx_t\\
y_t &= Ch_t
\end{align}
$$

- Recurrent formulation makes inference faster because it takes constant time to produce a token at any time unlike transformers where you have to calculate the attention weights with growing length.
- However, such recurrency also makes it harder to train the network because you can't parallelise computation between time steps.

## Convolutional Formulation

From above equation, one can derive $y_t$ by:

$$
\begin{align}
y_k &= C\bar{A}^k\bar{B}x_0 + C\bar{A}^{k-1}\bar{B}x_{1} + ... + C\bar{B}x_k \\
y &= x \cdot \bar{K}
\end{align}
$$
where $\bar{K} = (C\bar{A}^k\bar{B}, C\bar{A}^{k-1}\bar{B}, ..., C\bar{B})$ can be seen as a convolutional kernel and the calculation of $y_k$ can be calculated in parallel. This makes it ideal for training.