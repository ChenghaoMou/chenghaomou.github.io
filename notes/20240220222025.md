---
aliases:
- robots.txt
created: '2024-02-20'
date: '2024-02-20'
modified: '2024-02-20'
title: robots.txt
---

# robots.txt

Is this our only defence to the crawlers and consequently the AI overlords?

Why website owners have to shield themselves when corporate content stealers are roaming free? Do we have to choose among paywalls, captchas, and spiders?

Some companies are knowingly violating the handshake rule defined in `robots.txt`,  or have done so before under public scrutiny[^1]. Should we hold them accountable? Especially when they are profiting in the same market as the original content producers? If so, how?

[^1]: [[20240215093449|With the rise of AI, web crawlers are suddenly controversial - The Verge]]