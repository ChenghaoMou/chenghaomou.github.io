---
aliases:
  - Fact-checking in the LLM Era
created: 2024-03-31
date: '2024-03-31'
modified: 2024-04-30
title: "Fact-checking in the LLM Era"
---

# Fact-checking in the LLM Era

Generative LLMs suffer from hallucinations[^1] [^2] and even when specifically trained to synthesis citations, they have little to no correlation with factual precision[^3].

To automate the fact-checking process, [[wei_2024|Long-form factuality in large language models]] proposed a system called [Search-Augmented Factuality Evaluator (**SAFE**)](https://github.com/google-deepmind/long-form-factuality) that:
1. breaks down a response/generation into individual statements, revised to be self-contained if necessary;
2. checks each statement relevancy to the query with a model;
3. uses a search enigne (in this case Google) to determine if a statement is supported;
4. aggregates the results for final decision and reasoning.

This underlying assumption that a search engine can and will provide factual data in the first place can be undermined in light of the information ecosystem pollution with GenAI artefacts and rampant misinformation and disinformation[^4].

[^1]: [[xu_2024|Hallucination is Inevitable: An Innate Limitation of Large Language Models]]
[^2]: [[20231214212913|Hallucination is Here to Stay]]
[^3]: [[min_|FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation]]
[^4]: [[20221210185344|On the Dangers of Large Generative Models]]