---
aliases:
  - LLMentalist Effect
title: "LLMentalist Effect"
created: 2024-04-21
updated: 2024-04-21
modified: 2024-04-21
---

# LLMentalist Effect

Source: [[20240414162554|LLMs arenâ€™t very bright. Why are so many people fooled?]]

A similar phenomena to feedback bias where people who gives feedback for a product are often the ones who believe in the products and are thus biased towards giving extreme reviews.

This effect comes down to six steps:

## 1. Self Selection

The audience are filtered: active users are early adopters, enthusiasts, and firm believers. People who are sceptical about the product are cautious and therefore less eager to use it.

## 2. Scene Setting

Media coverage and hype prime the users. Chat interface and prevalence of [[20230305142934|Anthropomorphizing]] set the scene. Warnings of [[20231214212913|Hallucination]] are easily ignored just like any other digital agreements we have skipped for ages.

## 3. Context Establishing

Gamified prompting on one hand reels us in for an intellectual competition with a machine, and on the other hand creates a sense of control and responsibility. If it fails, it's' your prompt. If it succeeds, it's because of its intelligence.

## 4. Selection Bias

For the targeted audience, they find evidence to convince themselves to believe instead of the other way around. The authorative sound and seemingly plausible and specific output pull the audience a little more closer.

## 5. Validation Loop

A positive loop for both the audience and the model where more and more confident and probable text convince the user that the chatbot is intelligent. (But does this creates a skewed engagement data?)