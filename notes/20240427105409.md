---
aliases:
  - Infini-attention
title: "Infini-attention"
created: 2024-04-27
updated: 2024-04-27
modified: 2024-04-27
---

# Infini-attention

The idea of infini-attention builds on top serveral techniques such as compressive memory, linear attention and delta rules. It's also support plug and play with the existing [[20240427105437|Multi-head Attention]].

For a segment $\underset{N \times d_{\text{model}}}{X_{s}}$ of the input $\underset{L\times d_{\text{model}}}{X}$ where $N < L$, the following operations are carried out to retrieve information from the memory $\underset{d_{\text{key}} \times d_{\text{value}}}{M_{s-1}}$ as well as updating the memory for future segments:

## Memory Update

The simple version is as follows:
$$
\begin{align}
\underset{N\times d_{key}}{K_{s}} &= X_{s}W_{K} \\
\underset{N\times d_{value}}{V_{s}} &= X_{s}W_{V} \\
\underset{d_{\text{key}} \times d_{\text{value}}}{M_{s}} &= M_{s-1} + \sigma(K_{s})^TV_{s}
\end{align}
$$
Let's unroll the second equation with an example of three segments:

$$
\underset{d_{\text{key}} \times d_{\text{value}}}{M_{3}} = \sigma(K_{0})^TV_{0} + \sigma(K_{1})^TV_{1} + \sigma(K_{2})^TV_{2}
$$
If we encounter a future $K_t = K_1$, a query can be made with the following equation:

$$
\begin{align}
\underset{N \times d_{\text{key}}}{\sigma(K_{t})}\underset{d_{\text{key}} \times d_{\text{value}}}{M_{3}} &= \sigma(K_{1})\sigma(K_{0})^TV_{0} + \sigma(K_{1})\sigma(K_{1})^TV_{1} + \sigma(K_{1})\sigma(K_{2})^TV_{2} \\
&= \sigma(K_{1})\sigma(K_{0})^TV_{0} + \sigma(K_{1})\sigma(K_{1})^TV_{1} + \sigma(K_{1})\sigma(K_{2})^TV_{2} \\
\end{align}
$$

Assuming keys are **genearlly orthogonal to each other in the high dimensional space** and *normalised*, the above equation becomes:

$$
\begin{align}
\underset{N \times d_{\text{key}}}{\sigma(K_{t})}\underset{d_{\text{key}} \times d_{\text{value}}}{M_{3}} &= \sigma(K_{1})\sigma(K_{1})^TV_{1} \\
&= V_{1} \\
\end{align}
$$
Thus, the corresponding values are retrieved.

## Memory Retrival

To use such a memory with a given $\underset{L\times d_{query}}{Q_{s}}$:
$$
\begin{align}
\underset{S\times d_{\text{value}}}{W_{s}^{\text{mem}}} &= \frac{\sigma(Q_{s})M_{s-1}}{\sigma(Q_{s})Z_{s-1}} \\
&= \frac{\sigma(Q_{s})(\dots + \sigma(K_{s-1})^TV_{s-1})}{\sigma(Q_{s})Z_{s-1}} \\ \\
&= \frac{\sigma(Q_{s})\sigma(K_{0})^TV_{0} + \dots + \sigma(Q_{s})\sigma(K_{s-1})^TV_{s-1}}{\sigma(Q_{s})Z_{s-1}} \\
&= \frac{\sigma(Q_{s})\sigma(K_{0})^T}{\sigma(Q_{s})Z_{s-1}}V_{0} + \dots + \frac{\sigma(Q_{s})\sigma(K_{s-1})^T}{\sigma(Q_{s})Z_{s-1}}V_{s-1}
\end{align}
$$

where $\underset{d_{key}}{Z_{s}} = Z_{s-1} + \sum_{t=1}^N\sigma(K_{t})$. It essentially becomes a weighted sum of all past values based on past keys and current queries. (This looks a lot like the unfolded normal attention calculation because it assumes that the original softmax can be decomposed to matrix multiplication with something like a kernel trick)

The above uses the most straightforwad update, but it can modified to factor in the delta change for information that already exists in the memory related to the key and value before an update:

$$
\underset{d_{\text{key}} \times d_{\text{value}}}{M_{s}} = M_{s-1} + \sigma(K_{s})^T(V_{s} - \frac{\sigma(K_{s})M_{s-1}}{\sigma(K_{s})Z_{s-1}})
$$

## Final Output

Final attention output for a segment from one attention head:
$$
A_{s} = \text{sigmoid}(\beta) \odot \text{sigmoid}(\frac{Q_{s}K_{s}^T}{\sqrt{d_{\text{model}}}})V_{s} + (1 - \text{sigmoid}(\beta)) \odot W_{s}^{\text{mem}}
$$
with a learned weight parameter $\beta$.

Final output from $H$ attention heads:

$$
\underset{L\times d_{\text{model}}}{O} = [A_{s}^0, A_{s}^1, \dots]\underset{H\times d_{\text{value}} \times d_{\text{model}}}{W}
$$

The advantage of this memory mechanism is that it uses **bounded memory space** for theoretically unlimited segments while maintaining **subquadratic complexity** when dealing with long context.