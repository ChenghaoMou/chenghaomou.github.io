---
aliases:
  - Multi-head Attention
  - Title
title: "Multi-head Attention"
created: 2024-04-27
updated: 2024-04-27
modified: 2024-06-30
---

# Multi-head Attention

Standard multi-head attention formulation:

In one attention head, given input $\underset{L\times d_{\text{model}}}{X}$ and parameters $\underset{d_{\text{model}} \times d_{\text{query}}}{W_{Q}}$, $\underset{d_{\text{model}} \times d_{\text{key}}}{W_{K}}$, and $\underset{d_{\text{model}} \times d_{\text{value}}}{W_{V}}$:

1. $\underset{L\times d_{query}}{Q} = XW_{Q}$
2. $\underset{L\times d_{value}}{V} = XW_{V}$
3. $\underset{L\times d_{key}}{K} = XW_{K}$

Attention weights can be compuated as (if $d_{\text{key}} = d_{\text{query}}$):

$$
\underset{L\times d_{\text{value}}}{A} = \text{softmax}(\frac{QK^T}{\sqrt{ d_{\text{model}}}})V
$$

Then the final output from $H$ attention heads becomes:

$$
\underset{L\times d_{\text{model}}}{O} = [A_{0}, A_{1}, \dots]\underset{H\times d_{\text{value}} \times d_{\text{model}}}{W}
$$