---
aliases:
  - Selective Language Modeling
title: "Selective Language Modeling"
created: 2024-04-27
updated: 2024-04-27
modified: 2024-06-30
---

# Selective Language Modeling

Source: [[lin_2024a|Rho-1: Not All Tokens Are What You Need]]

Almost like a different version of [[20210926153300|Adaptive Computation]], but instead allocating different compute for different input, it is restricting learning from only certain tokens.

The first step of SLM is to build a reference model that can recognize high-quality tokens from in the input. This can be achieved by curating a high-quality dataset and training a model on top of it. From such a model, a loss associated with a token $x_i$ in a sequence can be formalised as:

$$
\mathcal{L}(x_{i};\theta_{ref}) = -\log P(x_{i}|x<i;\theta_{ref})
$$

For the regular pretraining of a LLM with SLM incorporated on a larger pretraining dataset, an excess loss can be expressed as:

$$
\begin{align}
\mathcal{L}_{\Delta}(x_{i}) &= \mathcal{L}(x_{i};\theta) - \mathcal{L}(x_{i};\theta_{ref}) \\
\mathcal{L}_{SLM}(\theta) &= \frac{1}{N * k\%}\sum_{i=1}^NI_{k\%}(x_{i})\cdot \mathcal{L}(x_{i};\theta)
\end{align}
$$

where $I_{k\%}(x_i)$ is an indicator function:

$$
I_{k\%}(x_i) = \begin{cases}
1 & \text{if } x_i \text{ is in the top k\% of } \mathcal{L}_{\Delta} \\
0 & \text{otherwise}
\end{cases}
$$

What this does is basically masking out unimportant tokens during the training based on the difference between the reference model and the regular model.

To the reference model, a high-quality token means small loss (since the model is expecting such tokens) and therefore a **high excess loss** (surprising to the regular model but not the reference model) indicates tokens that the regular model should better focus on.