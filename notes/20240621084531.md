---
aliases:
  - Natural Language Gradient
title: "Natural Language Gradient"
created: 2024-06-21
updated: 2024-06-21
modified: 2024-06-21
---

# Natural Language Gradient

Reference: [[yuksekgonul_2024|TextGrad Automatic Differentiation via Text]]

By treating **models or complex systems** as black boxes, taking "gradients" on textual variables (input or output) can be an effective way to improve overall system performance.

## Gradient Computation

Given an input $\text{Prompt}$, a model $M$, and its output $\text{output}$, one can derive the gradients with respect to the output and the prompt by:

$$
\begin{align}
\frac{\partial L}{\partial \text{output}} &= \text{LLM}(\text{output}, \text{evaluation})\\
\frac{\partial L}{\partial \text{input}} &= \text{LLM}(\text{input}, \text{output}, \frac{\partial L}{\partial \text{output}})
\end{align}
$$

Where the first natural language gradient could be something like: `The output does not consider factors like x...`.

## Gradient Update

The gradient can be directly applied to the ouput:

$$
\text{output}' = \text{LLM}(output, \frac{\partial L}{\partial \text{output}})
$$

such that it could be considered as self-reflection/self-evaluation at test time.

Similarly, the gradients can be used to improve the input/prompt:

$$
\text{input}' = \text{LLM}(input, \frac{\partial L}{\partial \text{input}})
$$

Other challenges:

1. Batch training: gradients are concatenated mimicing traditional "addition";
2. Momentum: previous iterations can be seen when making gradient update;