---
aliases:
  - The Scaling in Scaled Dot Product Attention
title: "The Scaling in Scaled Dot Product Attention"
created: 2024-07-04
updated: 2024-07-04
modified: 2024-07-04
---

# The Scaling in Scaled Dot Product Attention

Why does the attention computation uses $\sqrt{d_k}$ in the denominator?

The paper mentions a rough idea on why it is needed:

> While for small values of  the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$. We suspect that for large values of, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.

In the footnote, the author also mentions why that particular value is used.

> To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean $0$ and variance $1$. Then their dot product, $q \cdot k = \sum_{i=1}^{d_k}q_ik_i$, has mean $0$ and variance $d_k$.

Essentially, to use such a scale factor is to normalise the data.

To further explain why such variance would lead to small gradients, we need to understand the behavior of softmax with one simple example:

```python
import torch
import torch.nn.functional as F

x = torch.normal(0, 1, (5, 3))
y = torch.normal(0, 768, (5, 3))
```

In this case, we can see the softmax output of the two variables have very different distributions:

```python
>>> F.softmax(x, dim=-1)
tensor([[0.2483, 0.4607, 0.2910],
        [0.6518, 0.1930, 0.1553],
        [0.6572, 0.3159, 0.0270],
        [0.3380, 0.1171, 0.5449],
        [0.4289, 0.0851, 0.4860]])
>>> F.softmax(y, dim=-1)
tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00],
        [1.0000e+00, 0.0000e+00, 0.0000e+00],
        [4.5905e-09, 0.0000e+00, 1.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0000e+00],
        [0.0000e+00, 7.2139e-42, 1.0000e+00]])
```

The softmax gradient can be calculated as:

$$
\frac{\partial{y_i}}{\partial{x_k}} = y_i(\delta_{ik} - y_k)
$$

where

$$
\delta_{ik} = \begin{cases}
1,& \text{if } i = k\\
0,& \text{if } i \neq k
\end{cases}
$$

When the variance is large, softmax essentially becomes argmax and most values become zeros. Thus no gradients will be ever propagated.

References:

1. [从梯度最大化看Attention的Scale操作 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9812)
2. [NLP面试官：“Attention为什么要除以根号d” 算法女生这么回答当场想发 offer](https://mp.weixin.qq.com/s/3o0NgpFPKS1RNICNuMuygg)