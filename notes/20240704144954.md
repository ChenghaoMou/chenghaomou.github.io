---
aliases:
  - FFN in Transformers
title: "FFN in Transformers"
created: 2024-07-04
updated: 2024-07-04
modified: 2024-07-04
---

# FFN in Transformers

Source: [NLP面试官：“Transformers 中 FFN 的作用” 算法女生这么回答下午就想安排入职](https://mp.weixin.qq.com/s/dLTHD05wezA5pAM2c4j07Q)

Even though there is a softmax involved in Attention computation, it is still linear transformation of the values. This makes the nonlinearity in FFN attractive.

Probing experiments also show that such FFN layers actually store all the information/knowledge the model has learned from the training data[^1] [^2] [^3].

[^1]: Transformer Feed-Forward Layers Are Key-Value Memories
[^2]: Knowledge Neurons in Pretrained Transformers
[^3]: MoEfication: Transformer Feed-forward Layers are Mixtures of Experts