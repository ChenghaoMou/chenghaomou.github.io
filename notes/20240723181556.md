---
aliases:
  - DPO vs. PPO
title: "DPO vs. PPO"
created: 2024-07-23
updated: 2024-07-23
modified: 2024-07-23
---

# DPO vs. PPO

Source: [An Update on DPO vs PPO for LLM Alignment](https://www.youtube.com/watch?v=rDF7eFPeVto)

DPO stands for Direct Preference Optimisation, which is a simpler technique than Proximal Policy Optimisation. Both are versions of RLHF to align models with human preferences; It has been shown that PPO generally shows better results than DPO. However, DPO is more straightforward to use and implement without a reward model[^1].

- A bigger reward model does not necessarily improve the performance;
- Training a good reward model is not easy as well;
- The underlying boost for PPO over DPO is unclear;

Possible improvement:

1. Scaling RLHF pipelines and datasets;
2. Building baselines with techniques like Rejection Sampling;
3. Functional tools and collaboration;
4. More meaningful benchmarks;
5. More time and attention on things other than IFT/SFT;

[^1]: [[20240512123317|How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?]]