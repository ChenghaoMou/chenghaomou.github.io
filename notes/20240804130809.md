---
aliases:
  - AFM
  - Apple Intelligence Foundation Models
title: "AFM"
created: 2024-08-04
updated: 2024-08-04
modified: 2024-08-04
---

# AFM

Source: [[_af|Apple Intelligence Foundation Language Models]]

Apple Intelligence Foundation Language Models (AFM for short) refers to a family of models designed for Apple's ecosystem.

**Salient architecture choices**:

1. Shared input and output embedding weights;
2. Pre-Normalisation ([[20220905125225|Layer Normalization in Transformers]]) with *RMSNrom*;
3. Query/Key Normalisation;
4. Grouped Query Attention with 8 Key-Value Heads;
5. SwiGLU activation;
6. RoPE;

**Data pipeline includes**:

1. Body extraction with Safari's reader mode and Boilerpipe;
2. Safety and profanity filtering with heuristics and models;
3. Fuzzy deduplication with LSH;
4. Quality filtering with models;
5. Decontamination with n-gram collisions;

**Post-training includes**:

1. Supervised fine-tuning (Treat the weights of data components as parameters and optimise them with model performance on a set of benchmarks.);
2. Reinforcement learning form human feedback (RLHF) (1. rejection sampling with model committee with models trained with different techniques to provide data for human annotation; 2. Mirror Descent Policy Optimization (MDPO) and Leave-One-Out (LOO) estimator to estimate the advantage of a prompt-response pair, or referred as MDLOO as a whole);

**Quality Recovery Adaption**:

During LoRA training, the adapters are initialised with weights from accuracy-recovery adapters, while the rest of the model is kept frozen.

**Quantisation**:

Based on the intuition that not all layers are of the same importance, some layers are quantised with 2-bit quantisation instead of the usual 4-bit, leading to the overall 3.7 bits per weight for production.