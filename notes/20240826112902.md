---
aliases:
  - Jamba
title: "Jamba"
created: 2024-08-26
updated: 2024-08-26
modified: 2024-08-26
---

# Jamba

Source: [[team_2024b|Jamba-1.5: Hybrid Transformer-Mamba Models at Scale]]

A hybrid transformer-[[20240217115122|Mamba]] model at scale.

Interesting observations:
- The improvement from **mamba 1** to **mamba 2** does not transfer to a hybrid architecture where both traditional attention and mamba attention are used. The hypothesis is that causal attention achieves similar results that brought by the larger state size in mamba 2;
- Optimised de-quantisation (INT8 storage + de-quantisation on load + BF16 calculation) in a fused kernel might be faster than direct BF16 (storage + calculation) by cutting down the data transfer cost in GPU;
- Overly large activation magnitude can be remedied by adding an auxiliary loss to minimise the magnitude without affecting the performance;