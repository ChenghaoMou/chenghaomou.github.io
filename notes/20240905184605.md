---
aliases:
  - The Dimensions of Reasoning
title: "The Dimensions of Reasoning"
created: 2024-09-05
updated: 2024-09-05
modified: 2024-09-05
---

# The Dimensions of Reasoning

Source: [[Do LLMs REALLY reason, understand, think, summarise...-|Do LLMs REALLY reason, understand, think, summarise...?]]

When we talk about reasoning with LLMs, we can use different dimensions to better understand the model behaviour:

- **Task**: the specific problem we try to solve. e.g. to extract information when reading an article;
- **Means**: how the task is completed. e.g. to go letter by letter or word by word when reading;
- **Attainment or affordance**: how is the ability achieved. e.g. to read at certain level;

Achieving good results in one dimension does not generalise to other dimensions: a model can get good results on a benchmark by focusing on superficial correlations.

[[20230305142934|Anthropomorphizing AI]] aside, how should we approach reasoning in LLMs? There are arguments made in various domains about LLMs' capabilities: [[20230321205856|Construct Validity]], [[20240905150515|Deductive and Inductive Reasoning in LLMs]], and [[20220904143432|Form, Meaning, and Communicative Intents]]. But one point made in [[Do LLMs REALLY reason, understand, think, summarise...-|Do LLMs REALLY reason, understand, think, summarise...?]] I think makes sense: despite the fact reasoning is an underspecified term in humans, behavioural experiments are a critical part of figuring it out; we can't wait until someone figures it out and then do all the construct validity setup.