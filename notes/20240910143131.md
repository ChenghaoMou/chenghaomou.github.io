---
aliases:
  - AdEMAMix
title: "AdEMAMix"
created: 2024-09-10
updated: 2024-09-10
modified: 2024-09-10
---

# AdEMAMix

[[20240910143203|AdamW]] uses two hyperparameters $\beta_1$ and $\beta_2$ to control how much influence the old gradients have over the current the step — large values emphasis more on old values and thus more insensitive to current gradients and small values allow the models to update to current batch more quickly but forget old updates more quickly. This observation prompts a new modification of AdamW: [[pagliardini_2024a|The AdEMAMix Optimizer: Better, Faster, Older]]:

$$
\begin{align}
m_1^{(t)} &= \beta_1m_1^{(t-1)} + (1 - \beta_1)g^{(t)}\\
m_2^{(t)} &= \beta_3m_2^{(t-1)} + (1 - \beta_3)g^{(t)}\\
v^{(t)} &= \beta_2v^{(t-1)} + (1 - \beta_2)g^{(t)^2}\\
\hat{m}^{(t)} &= \frac{m^{(t)}}{1 - \beta_1^t}\\
\hat{v}^{(t)} &= \frac{v^{(t)}}{1 - \beta_2^t}\\
\theta^{(t)} &= \theta^{(t-1)} - \eta(\frac{\hat{m}^{(t)} + \alpha m_2^{(t)}}{\sqrt{\hat{v}^{(t)}} + \epsilon} + \lambda \theta^{(t-1)})
\end{align}
$$

with a new addition of $m_2$, allowing $\beta_3$ to learn more from old updates while keeping the original hyperparameters $\beta_1, \beta_2$ the same. However, this also necessitates a new scheduler for $\beta_3$ and $\alpha$:

$$
\begin{align}
\alpha^{(t)} &= \text{min}(\frac{t}{T_\alpha}\alpha, \alpha)\\
\beta_3^{(t)} &= \text{min}(\text{exp}(\frac{\ln\beta_{start}\ln\beta_3}{(1 - \frac{t}{T_{\beta_3}})\ln\beta_3 + \frac{t}{T_{\beta_3}}\ln\beta_{start}}), \beta_3)\\

\end{align}
$$

$\beta_3 = 0.9999, \alpha \in [4, 10], \beta_{start} = \beta_1$ and $T_\alpha = T_{\beta_3} = T$ works well in practice.

Based on the experiments, AdEMAMix shows a more **smooth** and **faster convergence** and retains data influence/knowledge better during the training.