---
aliases:
  - AdamW
title: "AdamW"
created: 2024-09-10
updated: 2024-09-10
modified: 2024-09-10
---

# AdamW

$$
\begin{align}
m_t &= \beta_1m_{t-1} + (1 - \beta_1)g_t\\
v_t &= \beta_2v_{t-1} + (1 - \beta_2)g_t^2\\
\hat{m_t} &= \frac{m_t}{1 - \beta_1^t}\\
\hat{v_t} &= \frac{v_t}{1 - \beta_2^t}\\
\theta_t &= \theta_{t-1} - \eta(\frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon} + \lambda \theta_{t-1})
\end{align}
$$

- $g_t$ is the gradients at $t$;
- $m_t$ is the first momentum at $t$;
- $v_t$ is the second momentum at $t$;
- $\eta$ is the learning rate;
- $\lambda$ is the weight decay;