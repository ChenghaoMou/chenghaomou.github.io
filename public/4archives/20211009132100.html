<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><style>:where(img){height:auto}</style><title>Open Vocabulary</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=IBM Plex Sans Condensed:wght@400;700&amp;family=IBM Plex Sans Condensed:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"><meta name="viewport" content="width=device-width,initial-scale=1"><meta property="og:title" content="Open Vocabulary"><meta property="og:description" content="Open Vocabulary Problems Word based models suffer from the Out-of-Vocabulary (OOV) problem. Character-level models can be useful if the sequence length is manageable."><meta property="og:image" content="https://sleeplessindebugging.blog/static/og-image.png"><meta property="og:width" content="1200"><meta property="og:height" content="675"><link rel="icon" href="../static/icon.png"><meta name="description" content="Open Vocabulary Problems Word based models suffer from the Out-of-Vocabulary (OOV) problem. Character-level models can be useful if the sequence length is manageable."><meta name="generator" content="Quartz"><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve=""><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve=""><script src="../prescript.js" type="application/javascript" spa-preserve=""></script><script type="application/javascript" spa-preserve="">const fetchData=fetch("../static/contentIndex.json").then(t=>t.json());
</script></head><body data-slug="4archives/20211009132100"><div id="quartz-root" class="page"><div id="quartz-body"><div class="sidebar left"><h1 class="page-title"><a href="..">Sleepless in Debugging</a></h1><div class="mobile-only spacer"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../4archives/">4archives</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="">Open Vocabulary</a></div></nav><h1 class="article-title">Open Vocabulary</h1><p show-comma="true" class="content-meta"><span>Apr 13, 2024</span><span>2 min read</span></p><ul class="tags"><li><a href="../tags/natural_language_processing/open_vocabulary" class="internal tag-link">natural_language_processing/open_vocabulary</a></li><li><a href="../tags/natural_language_processing/representation_learning" class="internal tag-link">natural_language_processing/representation_learning</a></li><li><a href="../tags/natural_language_processing/translation" class="internal tag-link">natural_language_processing/translation</a></li><li><a href="../tags/natural_language_processing" class="internal tag-link">natural_language_processing</a></li><li><a href="../tags/todo" class="internal tag-link">todo</a></li></ul></div></div><article class="popover-hint"><h1 id="open-vocabulary">Open Vocabulary<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#open-vocabulary" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="problems">Problems<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#problems" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Word based models suffer from the Out-of-Vocabulary (OOV) problem. Character-level models can be useful if the sequence length is manageable. Recent sub-word models using <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label" class="internal alias">1</a></sup>, <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label" class="internal alias">2</a></sup>, and <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label" class="internal alias">3</a></sup> are a good compromise. To some extend, they can be considered as open vocabulary since you can degenerate a word to complete characters (all Unicode characters theoretically, but ASCII characters in the English dominating world). But the problem of a vocabulary remains:</p>
<ul>
<li>You have to store and maintain a physical copy of the vocabulary along side your model</li>
<li>You have to store the token embeddings, which can be a lot of parameters, in your model</li>
</ul>
<h2 id="open-low-and-no-vocabulary">Open, Low, and No Vocabulary<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#open-low-and-no-vocabulary" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>If we define open vocabulary as a criterion that there would be no OOV problem, then you can either have a finite vocabulary or no vocabulary at all(hash, VTR). If you do, the vocabulary can be small (bytes, ASCII characters), moderate or big(BPE w/ fixed size).</p>
<h3 id="no-vocabulary">No Vocabulary<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#no-vocabulary" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p></p><blockquote class="transclude" data-url="Paper-Robust-Open-Vocabulary-Translation-from-Visual-Text-Representations" data-block=""><h1>Paper Robust Open-Vocabulary Translation from Visual Text Representations</h1><p>We have seen similar problems using subwords for languages like English in paper <a href="../4archives/Literature-Notes/Readings/Paper-Robust-Open-Vocabulary-Translation-from-Visual-Text-Representations/../../../../4archives/Literature-Notes/Readings/Paper-CANINE-Pre-training-an-Efficient-Tokenization-Free-Encoder-for-Language-Representation" class="internal alias" data-slug="4archives/Literature-Notes/Readings/Paper-CANINE-Pre-training-an-Efficient-Tokenization-Free-Encoder-for-Language-Representation">Paper CANINE Pre-training an Efficient Tokenization-Free Encoder for Language Representation</a>, and in this paper, the authors takes on the same problem with a refreshing perspective - learning text embeddings from visually rendered text.</p>
<p>Text segmentation techniques like BPE and SentencePiece are subject to a lot of noise from the following scenarios:</p>


















































<div class="table-container"><table><thead><tr><th>Phenomena</th><th>Word</th><th>BPE</th></tr></thead><tbody><tr><td>Vowelization</td><td>كتاب</td><td>كتاب</td></tr><tr><td></td><td></td><td>الك·ِ·ت·اب·</td></tr><tr><td>Misspelling</td><td>lang<strong>ua</strong>ge</td><td>language</td></tr><tr><td></td><td>lang<strong>au</strong>ge</td><td>la ng au ge</td></tr><tr><td>Confusables</td><td>rea<strong>ll</strong>y</td><td>really</td></tr><tr><td></td><td>rea<strong>11</strong>y</td><td>re a 1 1 y</td></tr><tr><td>Shared Character Components</td><td>확인<strong>한</strong>다</td><td>확인·한·다</td></tr><tr><td></td><td>확인<strong>했</strong>다</td><td>확인·했다</td></tr></tbody></table></div>
<p><img src="../4archives/Literature-Notes/Readings/Paper-Robust-Open-Vocabulary-Translation-from-Visual-Text-Representations/../../../../vtr.png" width="auto" height="auto" alt="Architecture" fetchpriority="high" decoding="async"></p>
<p>Here, the rendered text is segmented into blocks/slices using a sliding window and each slice goes through a series of transformations — Conv2D, BatchNorm, ReLU and a linear transformation — and eventually goes to a standard transformer for further processing.</p>
<p>Some interesting observations from the paper:</p>
<ul>
<li>Given a fixed window size, increasing the stride degrades the performance</li>
<li>Increasing the convolution channel size does not necessarily translate into performance gains</li>
<li>Smaller strides increase the training time as the sequence are getting longer</li>
<li>Consistent improvement over baseline models on noisy data (confusables, permutations, natural noise like misspelling)</li>
</ul><a href="../4archives/Literature-Notes/Readings/Paper-Robust-Open-Vocabulary-Translation-from-Visual-Text-Representations" class="internal transclude-src">Link to original</a></blockquote><p></p>
<ul>
<li><sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label" class="internal alias">4</a></sup></li>
<li><sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label" class="internal alias">5</a></sup></li>
<li><sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="" aria-describedby="footnote-label" class="internal alias">6</a></sup></li>
</ul>
<h3 id="low-vocabulary">Low Vocabulary<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#low-vocabulary" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p></p><blockquote class="transclude" data-url="Paper-CANINE-Pre-training-an-Efficient-Tokenization-Free-Encoder-for-Language-Representation" data-block=""><h1>Paper CANINE Pre-training an Efficient Tokenization-Free Encoder for Language Representation</h1><p>We’ve seen some ideas on remedy the vocabulary burden from modern large models with hashing tricks like <a href="https://www.aclweb.org/anthology/D19-1506.pdf" class="external">PARDO<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>/<a href="https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html" class="external">PQRNN<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. But all of them still require some level of tokenization up-front where text is broken into hashable &amp; somehow meaningful chunks/subwords.</p>
<h2 id="model">Model<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#model" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>In this paper, CANINE makes it more generic by applying character/code-point level hash embeddings and block-wise self-attention – focusing on locality of characters since they don’t really have long dependency – and strided convolutions to down-size the sequence length (<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.345em;height:1.1901em"></span><span class="mord"><span class="nulldelimiter mopen"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight"><span class="mord mtight">2048</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="nulldelimiter mclose"></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord"><span class="mord">512</span></span></span></span></span>).</p>
<p>Above-mentioned architecture is sufficient for classification, but for sequence generation tasks, they concatenate the attended character embeddings (word/subword-level information) with down-sampled hidden embeddings (contextual information) and apply another round of convolution and transformer decoder layer to generate characters sequentially.</p>
<h2 id="loss">Loss<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#loss" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Whitespace-bounded Span masking and prediction: they mask several spans of characters and ask the model to predict characters within each span.</li>
<li>Subword span masking and prediction: instead of using whitespace to find spans, they can fall back to subword (presumably generated by something else like Sentencepiece)</li>
</ul>
<h2 id="thoughts">Thoughts<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#thoughts" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>It is a neat extension to what people are already doing with hash embeddings but at the end of the day, if we were ever to build an ultimate multilingual model, CANINE probably still need more parameters at convolution layers and longer sequence length, and potentially more data to compensate learning from scratch.</p><a href="../4archives/Literature-Notes/Readings/Paper-CANINE-Pre-training-an-Efficient-Tokenization-Free-Encoder-for-Language-Representation" class="internal transclude-src">Link to original</a></blockquote>
<blockquote class="transclude" data-url="Paper-Charformer-Fast-Character-Transformers-via-Gradient-based-Subword-Tokenization" data-block=""><a href="../Paper-Charformer-Fast-Character-Transformers-via-Gradient-based-Subword-Tokenization" class="internal alias transclude-inner" data-slug="Paper-Charformer-Fast-Character-Transformers-via-Gradient-based-Subword-Tokenization">Transclude of Paper-Charformer-Fast-Character-Transformers-via-Gradient-based-Subword-Tokenization</a></blockquote><p></p>
<ul>
<li><sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label" class="internal alias">7</a></sup></li>
</ul>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#footnote-label" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ol>
<li id="user-content-fn-1">
<p><a href="https://arxiv.org/abs/1609.08144v2" class="external">[1609.08144v2] Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="internal alias data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://arxiv.org/abs/1808.06226" class="external">[1808.06226] SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="internal alias data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3">
<p><a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" class="external">Byte pair encoding - Wikipedia<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="internal alias data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p><a href="https://www.aclweb.org/anthology/D19-1506.pdf" class="external">https://aclanthology.org/D19-1506.pdf<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="internal alias data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5">
<p><a href="https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html" class="external">Google AI Blog: Advancing NLP with Efficient Projection-Based Model Architectures<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5" class="internal alias data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6">
<p><a href="https://arxiv.org/abs/2106.12672" class="external">[2106.12672] Charformer: Fast Character Transformers via Gradient-based Subword Tokenization<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> <a href="#user-content-fnref-6" data-footnote-backref="" aria-label="Back to reference 6" class="internal alias data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7">
<p><a href="https://arxiv.org/abs/2105.13626" class="external">[2105.13626] ByT5: Towards a token-free future with pre-trained byte-to-byte models<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> <a href="#user-content-fnref-7" data-footnote-backref="" aria-label="Back to reference 7" class="internal alias data-footnote-backref">↩</a></p>
</li>
</ol>
</section></article></div><div class="sidebar right"><div class="desktop-only toc"><button type="button" id="toc" class=""><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#open-vocabulary" data-for="open-vocabulary">Open Vocabulary</a></li><li class="depth-1"><a href="#problems" data-for="problems">Problems</a></li><li class="depth-1"><a href="#open-low-and-no-vocabulary" data-for="open-low-and-no-vocabulary">Open, Low, and No Vocabulary</a></li><li class="depth-2"><a href="#no-vocabulary" data-for="no-vocabulary">No Vocabulary</a></li><li class="depth-2"><a href="#low-vocabulary" data-for="low-vocabulary">Low Vocabulary</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer class=""><hr><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> © 2024</p><ul><li><a href="https://codeberg.org/Chenghao2023/blog">CodeBerg</a></li><li><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></li></ul></footer></div><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">let mermaidImport;document.addEventListener("nav",async()=>{if(document.querySelector("code.mermaid")){mermaidImport||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs");const e=mermaidImport.default,t=document.documentElement.getAttribute("saved-theme")==="dark";e.initialize({startOnLoad:!1,securityLevel:"loose",theme:t?"dark":"default"}),await e.run({querySelector:".mermaid"})}});
</script><script src="../postscript.js" type="module"></script></body></html>