<!DOCTYPE html>
<html lang="en"><head><title>Techniques for Training Large Neural Networks</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=IBM Plex Sans:wght@400;700&amp;family=IBM Plex Sans:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Techniques for Training Large Neural Networks"/><meta property="og:description" content="Techniques for Training Large Neural Networks Metadata Author: openai.com Title: Techniques for Training Large Neural Networks Reference: openai.com/blog/techniques-for-training-large-neural-networks/⤴ ..."/><meta property="og:image" content="https://sleeplessindebugging.blog/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../../static/icon.png"/><meta name="description" content="Techniques for Training Large Neural Networks Metadata Author: openai.com Title: Techniques for Training Large Neural Networks Reference: openai.com/blog/techniques-for-training-large-neural-networks/⤴ ..."/><meta name="generator" content="Quartz"/><link href="../../index.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="4archives/Hypothesis/Techniques-for-Training-Large-Neural-Networks"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h1 class="page-title"><a href="../..">Sleepless in Debugging</a></h1><div class="spacer mobile-only"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../4archives/">4archives</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../4archives/Hypothesis/">Hypothesis</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Techniques for Training Large Neural Networks</a></div></nav><h1 class="article-title">Techniques for Training Large Neural Networks</h1><p show-comma="true" class="content-meta"><span>Mar 02, 2024</span><span>4 min read</span></p><ul class="tags"><li><a href="../../tags/todo" class="internal tag-link">todo</a></li><li><a href="../../tags/article" class="internal tag-link">article</a></li><li><a href="../../tags/Personal" class="internal tag-link">Personal</a></li></ul></div></div><article class="popover-hint"><h1 id="techniques-for-training-large-neural-networks">Techniques for Training Large Neural Networks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#techniques-for-training-large-neural-networks" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="metadata">Metadata<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#metadata" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>Author: <a href="../.." class="internal alias" data-slug="index">openai.com</a></li>
<li>Title: Techniques for Training Large Neural Networks</li>
<li>Reference: <a href="https://openai.com/blog/techniques-for-training-large-neural-networks/" class="external">https://openai.com/blog/techniques-for-training-large-neural-networks/<span class="external-icon">⤴</span></a></li>
<li>Category:<a href="../.././../tags/article" class="tag-link internal alias" data-slug="tags/article">article</a></li>
</ul>
<h2 id="page-notes">Page Notes<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-notes" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h2 id="highlights">Highlights<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#highlights" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>
<p>An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. — <a href="https://hyp.is/CDl3Bu_5Eey9Dmcdy4GYAQ/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:00:52<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
<ul>
<li>Annotation: <img src="https://i.imgur.com/DzCqrBF.png" alt/></li>
</ul>
</li>
<li>
<p>Data parallelism—run different subsets of the batch on different GPUs; Pipeline parallelism—run different layers of the model on different GPUs; Tensor parallelism—break up the math for a single operation such as a matrix multiplication to be split across GPUs; Mixture-of-Experts—process each example by only a fraction of each layer. — <a href="https://hyp.is/52KOSu_5Eeyx_iPWxexFtQ/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:01:54<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>Data Parallel training means copying the same parameters to multiple GPUs (often called “workers”) and assigning different examples to each to be processed simultaneously. Data parallelism alone still requires that your model fits into a single GPU’s memory, but lets you utilize the compute of many GPUs at the cost of storing many duplicate copies of your parameters. That being said, there are strategies to increase the effective RAM available to your GPU, such as temporarily offloading parameters to CPU memory between usages. — <a href="https://hyp.is/_byIqO_5EeyXgxfY5ZLZOA/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:02:32<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>With Pipeline Parallel training, we partition sequential chunks of the model across GPUs. Each GPU holds only a fraction of parameters, and thus the same model consumes proportionally less memory per GPU. — <a href="https://hyp.is/FdFIju_6EeyXDpdx_rlijw/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:03:12<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>GPipe has each worker process forward and backward passes consecutively and then aggregates gradients from multiple microbatches synchronously at the end. PipeDream instead schedules each worker to alternatively process forward and backward passes. — <a href="https://hyp.is/TYRBtO_6Eey6a2faTPfcpg/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:05:13<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
<ul>
<li>Annotation: <img src="https://i.imgur.com/Thvh3Fp.png" alt/></li>
</ul>
</li>
<li>
<p>Pipeline parallelism splits a model “vertically” by layer. It’s also possible to “horizontally” split certain operations within a layer, which is usually called Tensor Parallel training. — <a href="https://hyp.is/nYsKdu_6EeymeJPAWVBMjg/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:07:00<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>With either strategy, we can slice the weight matrix into even-sized “shards”, host each shard on a different GPU, and use that shard to compute the relevant part of the overall matrix product before later communicating to combine the results. — <a href="https://hyp.is/oPoVlO_6EeyrSL_Etmz77g/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:07:06<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>With the Mixture-of-Experts (MoE) approach, only a fraction of the network is used to compute the output for any one input. One example approach is to have many sets of weights and the network can choose which set to use via a gating mechanism at inference time. — <a href="https://hyp.is/sx2MLu_6EeyiLbOjpm-x7g/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:07:36<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>Different experts can be hosted on different GPUs, providing a clear way to scale up the number of GPUs used for a model. — <a href="https://hyp.is/tvhSju_6EeyuYhOejHQOLA/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:07:43<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>Checkpointing (also known as activation recomputation) stores any subset of activations, and recomputes the intermediate ones just-in-time during the backward pass. This saves a lot of memory at the computational cost of at most one additional full forward pass. — <a href="https://hyp.is/z7f2HO_6EeyyAReKX6ic8Q/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:08:24<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>Mixed Precision Training is to train models using lower-precision numbers (most commonly FP16). Modern accelerators can reach much higher FLOP counts with lower-precision numbers, and you also save on device RAM. — <a href="https://hyp.is/1g65zu_6EeyyAvP2uURbvA/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:08:35<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>Offloading is to temporarily offload unused data to the CPU or amongst different devices and later read it back when needed. — <a href="https://hyp.is/26sPIu_6Eey9F1uWUTVuLQ/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:08:44<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>Memory Efficient Optimizers have been proposed to reduce the memory footprint of the running state maintained by the optimizer, such as Adafactor. — <a href="https://hyp.is/5BRpnO_6Eey7CrNEUwHmRA/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:08:58<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
<li>
<p>Compression also can be used for storing intermediate results in the network. For example, Gist compresses activations that are saved for the backward pass; DALL·E compresses the gradients before synchronizing them. — <a href="https://hyp.is/6q66ZO_6EeykgyeRU8ywFw/openai.com/blog/techniques-for-training-large-neural-networks/" class="external">Updated on 2022-06-19 11:09:09<span class="external-icon">⤴</span></a> — Group:<a href="../.././../tags/Personal" class="tag-link internal alias" data-slug="tags/Personal">Personal</a></p>
</li>
</ul>
<hr/>
<h1 id="techniques-for-training-large-neural-networks-1">Techniques for Training Large Neural Networks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#techniques-for-training-large-neural-networks-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<hr/>
<h1 id="techniques-for-training-large-neural-networks-2">Techniques for Training Large Neural Networks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#techniques-for-training-large-neural-networks-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h1 id="techniques-for-training-large-neural-networks-3">Techniques for Training Large Neural Networks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#techniques-for-training-large-neural-networks-3" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h1 id="techniques-for-training-large-neural-networks-4">Techniques for Training Large Neural Networks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#techniques-for-training-large-neural-networks-4" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1></article></div><div class="right sidebar"><div class="toc desktop-only"><button type="button" id="toc" class><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#techniques-for-training-large-neural-networks" data-for="techniques-for-training-large-neural-networks">Techniques for Training Large Neural Networks</a></li><li class="depth-1"><a href="#metadata" data-for="metadata">Metadata</a></li><li class="depth-1"><a href="#page-notes" data-for="page-notes">Page Notes</a></li><li class="depth-1"><a href="#highlights" data-for="highlights">Highlights</a></li><li class="depth-0"><a href="#techniques-for-training-large-neural-networks-1" data-for="techniques-for-training-large-neural-networks-1">Techniques for Training Large Neural Networks</a></li><li class="depth-0"><a href="#techniques-for-training-large-neural-networks-2" data-for="techniques-for-training-large-neural-networks-2">Techniques for Training Large Neural Networks</a></li><li class="depth-0"><a href="#techniques-for-training-large-neural-networks-3" data-for="techniques-for-training-large-neural-networks-3">Techniques for Training Large Neural Networks</a></li><li class="depth-0"><a href="#techniques-for-training-large-neural-networks-4" data-for="techniques-for-training-large-neural-networks-4">Techniques for Training Large Neural Networks</a></li></ul></div></div><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><svg version="1.1" id="global-graph-icon" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
	s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
	c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
	C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
	c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
	v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
	s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
	C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
	S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
	s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
	s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer class><hr/><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> © 2024</p><ul><li><a href="https://codeberg.org/Chenghao2023/blog">CodeBerg</a></li><li><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="../../postscript.js" type="module"></script></html>