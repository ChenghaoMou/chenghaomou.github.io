<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><style>:where(img){height:auto}</style><title>Deduplication in Modern Large-scale LM Datasets</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=IBM Plex Sans Condensed:wght@400;700&amp;family=IBM Plex Sans Condensed:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"><meta name="viewport" content="width=device-width,initial-scale=1"><meta property="og:title" content="Deduplication in Modern Large-scale LM Datasets"><meta property="og:description" content="Deduplication in Modern Large-scale LM Datasets Motivations Behind Deduplication Like the old saying, garbage in, garbage out, it is important to take care of our data before feeding ..."><meta property="og:image" content="https://sleeplessindebugging.blog/static/og-image.png"><meta property="og:width" content="1200"><meta property="og:height" content="675"><link rel="icon" href="../../../static/icon.png"><meta name="description" content="Deduplication in Modern Large-scale LM Datasets Motivations Behind Deduplication Like the old saying, garbage in, garbage out, it is important to take care of our data before feeding ..."><meta name="generator" content="Quartz"><link href="../../../index.css" rel="stylesheet" type="text/css" spa-preserve=""><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve=""><script src="../../../prescript.js" type="application/javascript" spa-preserve=""></script><script type="application/javascript" spa-preserve="">const fetchData=fetch("../../../static/contentIndex.json").then(t=>t.json());
</script></head><body data-slug="4archives/Literature-Notes/Readings/Research-Deduplication-in-Modern-Large-scale-LM-Datasets"><div id="quartz-root" class="page"><div id="quartz-body"><div class="sidebar left"><h1 class="page-title"><a href="../../..">Sleepless in Debugging</a></h1><div class="mobile-only spacer"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../4archives/">4archives</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../4archives/Literature-Notes/">Literature Notes</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../4archives/Literature-Notes/Readings/">Readings</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="">Deduplication in Modern Large-scale LM Datasets</a></div></nav><h1 class="article-title">Deduplication in Modern Large-scale LM Datasets</h1><p show-comma="true" class="content-meta"><span>Apr 13, 2024</span><span>9 min read</span></p></div></div><article class="popover-hint"><h1 id="deduplication-in-modern-large-scale-lm-datasets">Deduplication in Modern Large-scale LM Datasets<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deduplication-in-modern-large-scale-lm-datasets" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="toc" data-theme="github-light github-dark"><code data-language="toc" data-theme="github-light github-dark" style="display:grid"><span data-line=""> </span></code></pre></figure>
<h2 id="motivations-behind-deduplication">Motivations Behind Deduplication<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#motivations-behind-deduplication" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Like the old saying, garbage in, garbage out, it is important to take care of our data before feeding it to the model. One of the problems we face, especially considering the rapid scaling of modern language modeling datasets, is duplication. It has been shown that models tend to output training data verbatim when there are many duplicates <a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> and it potentially makes the model vulnerable to privacy attacks <a href="zotero://select/items/@kandpal_2022" class="external">@kandpal_2022<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. Typically, the advantage of deduplication includes:</p>
<ol>
<li>Efficient training. Depending on how you look at it, you can have either the same amount of learnable knowledge with smaller data size or more with the same data size, which means you can achieve the same performance with less training steps or better performance with same number of steps. <a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>Better understanding. In <a href="zotero://select/items/@bowman_2021a" class="external">@bowman_2021a<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, authors have discussed various problems regarding the benchmarks. Thought language modeling datasets often precedes the use of benchmarking datasets, similar problems they presented, in my opinion, extend to this stage as well, especially the statistical power the dataset represents. Without deduplicates, the metrics give a clearer view on the performance without being skewed by the duplicates. Given the fact that pre-training with large dataset is still a dominant schema in deep learning, we should pay at least equal attention to LM datasets, if not more.</li>
<li>Another related issue is the duplication between splits or between datasets or between LM datasets and benchmarks. Any amount of duplicates discredit our metrics and potentially make so-called improvement a false promise.</li>
<li>More accessibility. Socially speaking, most of us cannot afford to download or transfer thousands of giga bytes of text, not to mention training a model with it. Deduplication, for a fix-sized dataset, makes it more easier to transfer and collaborate on.</li>
</ol>
<h2 id="pipeline">Pipeline<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#pipeline" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>The following sections cover a typical workflow of data deduplication. Admittedly, the workflow is intended mostly for English only. I will try my best to include my thoughts for multilingual datasets.</p>
<h3 id="preprocessing">Preprocessing<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#preprocessing" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<h4 id="from-web-pages-to-text">From Web Pages to Text<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#from-web-pages-to-text" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>It is easy to find templated web pages on the internet, but how do we transform the webpages into textual data and maybe remove the bulk of the deduplicates/templates in the very first step?</p>
<p>It does not look like Common Crawl is doing anything special in this regarding. I couldn’t find obvious WET extraction code on the internet or any mention about special text extraction being used, so I am assuming that the WET format text data is directly extracted, without any filtering, from the WARC data, which means the templates in those web pages are most likely reserved in the final text output.</p>
<p>I understand that from their perspective, they are keeping the data as raw as possible to enable any further research. Unfortunately, re-processing the WARC data so that one can remove templated content with some tools like <a href="https://github.com/adbar/trafilatura" class="external">adbar/trafilatura<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> seems like an endeavor only a handful researchers have done so far. In Pile-CC <a href="zotero://select/items/@gao_2020" class="external">@gao_2020<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, the authors used <a href="https://github.com/miso-belica/jusText" class="external">miso-belica/jusText<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> that removes the boilerplate from the WARC files and it yields better quality in the end. (They also have a nice discussion on their decision with jusText.)</p>
<h4 id="from-text-to-data">From Text to Data<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#from-text-to-data" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>Common procedures I have seen in papers like <a href="zotero://select/items/@wenzek_2020" class="external">@wenzek_2020<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> and <a href="zotero://select/items/@lin_2021a" class="external">@lin_2021a<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>:</p>
<ol>
<li>Lowercase everything.</li>
<li>Replace numbers with placeholders. I would argue that this might lead to the lack of numerical or mathematical astuteness of the model.</li>
<li>Remove all punctuation. Downsides of this is that your model won’t be able to understand punctuations at all and some context might be lost as well. In languages like Spanish, “Tú eres estudiante.” (You are a student) means totally different from “¿Tú eres estudiante?” (Are you a student?).</li>
<li>Remove accents. That would be understandably a No for many other languages.</li>
</ol>
<p>Ideally, you might want a recoverable deduplication where you find as many as possible duplicates with relaxed matching (e.g w/o accents) and compare them with the raw text (w/ accents) afterwards to decide if they are duplicates or not. This would help weeding out the false positives from the relaxed matching.</p>
<h3 id="exact-deduplication">Exact Deduplication<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#exact-deduplication" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>For starters, one can remove duplicated webpages by just looking at the URL. A simple hash set of urls would suffice and it is usually not elaborated in papers since the size of URLs is significantly smaller than the actual text.</p>
<p>Exact deduplication on the text can be more involved. Depending on what level of granularity you want to achieve, people usually do one or more of:</p>
<ol>
<li>document level</li>
<li>paragraph level</li>
<li>sentence level</li>
<li>sub-string level</li>
</ol>
<p>deduplications.</p>
<p>Exact deduplication usually takes on the form of hashing. Each piece of text is hashed into a bucket (e.g. the first 64 bit of SHA-1 in <a href="zotero://select/items/@wenzek_2020" class="external">@wenzek_2020<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>) or directly into a hash set. Sub-string level deduplication is more complicated and it goes beyond any boundaries so we will talk about it in the next section.</p>
<h3 id="exact-sub-string-deduplication">Exact Sub-string Deduplication<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#exact-sub-string-deduplication" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> builds on the idea of <a href="https://en.wikipedia.org/wiki/Suffix_array" class="external">Suffix array - Wikipedia<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> to find duplicate sub-strings in a corpus. If a string is repeated in two locations in the corpus, their suffices will appear as neighbors in the sorted suffix array. The authors also shared their code in <a href="https://github.com/google-research/deduplicate-text-datasets" class="external">GitHub - google-research/deduplicate-text-datasets<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> so I encourage you to give it a try.</p>
<h3 id="near-deduplication">Near Deduplication<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#near-deduplication" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Aka. the friendly competition between MinHash + LSH and SimHash. SimHash was proposed in <a href="zotero://select/items/@charikar_" class="external">@charikar_<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> in 2002 and widely used by Google crawlers <a href="zotero://select/items/@manku_2007" class="external">@manku_2007<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, while MinHash debuted in <a href="zotero://select/items/@broder_1998" class="external">@broder_1998<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> in 1998. Both algorithms require some form of hashing, permutation and grouping. Without going too much into details here, based on my personal experience and some account on <a href="https://stackoverflow.com/a/46415603" class="external">Choosing between SimHash and MinHash for a production system - Stack Overflow<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, the main difference is that:</p>
<ol>
<li>SimHash is faster and more cpu-friendly than MinHash;</li>
<li>SimHash is more strict on what are near duplicates than MinHash;</li>
<li>MinHash + LSH seems like a more popular choice (I also only learned about MinHash + LSH in school)</li>
</ol>
<h3 id="semantic-deduplication">Semantic Deduplication<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#semantic-deduplication" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>TODO</p>
<h3 id="repetition-vs-deduplication">Repetition VS. Deduplication<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#repetition-vs-deduplication" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>One kind of deduplication or quality filtering is removing documents that are self-repetitive, meaning a document could contain:</p>
<ol type="I">
  <li>small pieces of text that appear many times</li>
  <li>large chunk of text that appear more than once</li>
</ol>
<p>In this regard, <a href="zotero://select/items/@rae_2021" class="external">@rae_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> developed some useful heuristics for finding those English documents that are infested with (i) repeated lines (2) repeated paragraphs or (3) repeated n-grams;</p>
<p>Terminologies they used in the paper:</p>
<ul>
<li><strong>Duplicate line/paragraph fraction:</strong> Given a line/paragraph duplicate, does it appear more than <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.0556em;height:.8056em"></span><span class="mord mathnormal">x</span><span class="mord">%</span></span></span></span> of the document? (case I);</li>
<li><strong>Duplicate line/paragraph character fraction:</strong> Given a line/paragraph duplicate, does it have more than <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.0556em;height:.8056em"></span><span class="mord mathnormal">x</span><span class="mord">%</span></span></span></span> characters of the document? (case II);</li>
<li><strong>Top n-gram character fraction:</strong> Given a document, if the top n-gram occurrences contains more than <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.0556em;height:.8056em"></span><span class="mord mathnormal">x</span><span class="mord">%</span></span></span></span> of the characters, remove the document;</li>
<li><strong>Duplicate n-gram character fraction:</strong> Given a document, if the top duplicated n-gram occurrences contains more than <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.0556em;height:.8056em"></span><span class="mord mathnormal">x</span><span class="mord">%</span></span></span></span> of the characters, remove the document;</li>
</ul>
<p>They also postulated the thresholds <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span> in each scenario, please the original paper if you’re interested.</p>
<h3 id="removal">Removal<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#removal" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>It often goes unmentioned on how to deal with the duplicates.</p>
<p>Exact duplicates that have boundaries (sentences, paragraphs, documents) are easy, you can keep the first instance and ditch the rest. It is unclear what is the best practice for something like sub-string duplicates. Removing one sub-string can easily break the context for the text.</p>
<p>Depending on the algorithms you choose, near deduplication might give you pairs of duplicates or duplicates for each query documents. For pair of duplicates, <a href="zotero://select/items/@rae_2021" class="external">@rae_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> only removes a random one from that pair. If given <code>(A, B)</code>, <code>(B, C)</code> and <code>(C, A)</code> then it is possible you remove <code>[B, C, A]</code> for each pair and lead to non of the docs are kept for that cluster. But building a cluster from those pairs might lead you to a cluster that are not as compact as you might expect – a few dissimilarity here and there might group completely unrelated items together in the end.</p>
<p>TODO What would be the best action then?</p>
<h2 id="applications">Applications<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#applications" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="validation-or-test-leakage">Validation or Test Leakage<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#validation-or-test-leakage" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> found a large duplicates exist in dataset splits and stressed the problem of metric being unreliable due to such duplication. <a href="zotero://select/items/@gao_2020" class="external">@gao_2020<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> states that they only make sure the Pile itself, along with its splits are deduplicated and they won’t proactively deduplicating for any downstream benchmarks and leave that decision to readers; <a href="zotero://select/items/@rae_2021" class="external">@rae_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> applied n-gram Jaccard similarity to filter training documents that resemble test documents and also pointed out that some datasets do have curated test-set and such filtering is not necessary for them.</p>
<h2 id="cost-and-implementation">Cost and Implementation<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cost-and-implementation" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>When you have an academic budget, what is your best defense?
Time.</p>
<ul>
<li>It took several days to process Pile-CC with an in-memory LSH according to <a href="zotero://select/items/@gao_2020" class="external">@gao_2020<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.</li>
<li>It 96 hours on 100 48-core machines on MareNostrum 4 for <a href="zotero://select/items/@gutierrez-fandino_2021" class="external">@gutierrez-fandino_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>’s deduplication.</li>
<li>It took me ~12 hours on a 80 core machine to calculate SimHash for 1TB Oscar English data (python) and few days on my M1 Max laptop to cluster them (c++).</li>
<li>Running any code or algorithms with a 1TB input would requires measurement in days.</li>
</ul>
<h3 id="libraries">Libraries<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#libraries" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li><a href="http://corpus.tools/wiki/Onion" class="external">Onion deduplication tool<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li><a href="https://github.com/google-research/deduplicate-text-datasets" class="external">GitHub - google-research/deduplicate-text-datasets<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>
<ul>
<li>You can run the code with just a text file;</li>
<li>If you want to respect the document boundaries, you have to manually recover from their output;</li>
<li>The number of jobs should not exceed the number of openable files or number of arguments if you do not wish to modify their code;</li>
<li>The <code>collect_similar</code> command does not like a relative file path;</li>
<li>It will take 2 hours for a 4GB file in a single job, and each job takes about <strong>10x</strong> memory;</li>
<li>Running it with Oscar English would requires at least <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.25em;height:1em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord">8</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span> of storage and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.25em;height:1em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span> of memory;</li>
</ul>
</li>
<li><a href="https://github.com/ekzhu/datasketch" class="external">GitHub - ekzhu/datasketch: MinHash, LSH, LSH Forest, Weighted MinHash, HyperLogLog, HyperLogLog++, LSH Ensemble<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li><a href="https://github.com/ChenghaoMou/simhash" class="external">GitHub - ChenghaoMou/simhash: Simhash in C++<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li><a href="https://github.com/ChenghaoMou/data_tooling/tree/dedup-improved/ac_dc/deduplicate" class="external">data_tooling/ac_dc/deduplicate at dedup-improved · ChenghaoMou/data_tooling · GitHub<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
<h2 id="final-thoughts">Final Thoughts<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#final-thoughts" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Here is a table that covers most techniques mentioned in this article.</p>

























































































<div class="table-container"><table><thead><tr><th>Dataset</th><th>OpenWebText2 <a href="zotero://select/items/@gao_2020" class="external">@gao_2020<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th>Pile-CC <a href="zotero://select/items/@gao_2020" class="external">@gao_2020<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th><a href="zotero://select/items/@gutierrez-fandino_2021" class="external">@gutierrez-fandino_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th>MassiveText <a href="zotero://select/items/@rae_2021" class="external">@rae_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th><a href="zotero://select/items/@lin_2021a" class="external">@lin_2021a<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th>C4 <a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th>Real News <a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th>LM1B <a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th><th>WIKI40B <a href="zotero://select/items/@lee_2021" class="external">@lee_2021<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></th></tr></thead><tbody><tr><td>Input Size</td><td>After URL deduplication: 193.89GB (69M docs)</td><td>~306GB</td><td>Between 2TB and 59TB</td><td></td><td></td><td>806.92GB (364M docs)</td><td>~120GiB</td><td>~4.40GiB (30M)</td><td>~2.9M</td></tr><tr><td>Output Size or Deduction</td><td>After MinHashLSH: 65.86GB (17M docs)</td><td>227.12GiB (~55M)</td><td>2TB after document deduplication<br>570GB after substring deduplication</td><td>0.001TB~2.1TB</td><td>0.01GiB~3324.45GiB</td><td>3.04%~7.18% <strong>↓</strong> (train)</td><td>13.63%~19.4% <strong>↓</strong> (train)</td><td>0.76%~4.86% <strong>↓</strong> (train)</td><td>0.39%~2.76% <strong>↓</strong> (train)</td></tr><tr><td>Level</td><td>1. URL<br>2. Documents</td><td>Documents</td><td>1. Sentences<br>2. Substrings</td><td>Documents</td><td>1. URL<br>2. Paragraphs</td><td>1. Substrings<br>2. Documents</td><td>Same as <strong>C4</strong></td><td>Same as <strong>C4</strong></td><td>Same as <strong>C4</strong></td></tr><tr><td>Method</td><td>1. URL(Exact)<br>2. Documents(MinHash LSH)</td><td>Documents(jusText + MinHash LSH)</td><td>Sentences(Exact) Substrings(Exact)</td><td>Documents(Exact) Documents(MinHash)</td><td>1. URL(Exact)<br>2. Paragraphs(Exact)</td><td>1. Substrings(Suffix Array)<br>2. Documents(MinHash)</td><td>Same as <strong>C4</strong></td><td>Same as <strong>C4</strong></td><td>Same as <strong>C4</strong></td></tr><tr><td>Parameters</td><td><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.25em;height:1em"></span><span class="mopen">(</span><span class="mord">10</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">0.5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mclose">?</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6887em"><span style="margin-right:.05em;top:-3.063em"><span class="pstrut" style="height:2.7em"></span><span class="mtight reset-size6 size3 sizing"><span class="mtight mbin">∗</span></span></span></span></span></span></span></span></span></span></span></td><td><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.25em;height:1em"></span><span class="mopen">(</span><span class="mord">10</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">0.5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mclose">?</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6887em"><span style="margin-right:.05em;top:-3.063em"><span class="pstrut" style="height:2.7em"></span><span class="mtight reset-size6 size3 sizing"><span class="mtight mbin">∗</span></span></span></span></span></span></span></span></span></span></span></td><td>N/A</td><td><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.25em;height:1em"></span><span class="mopen">(</span><span class="mclose">?</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">0.8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">13</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6887em"><span style="margin-right:.05em;top:-3.063em"><span class="pstrut" style="height:2.7em"></span><span class="mtight reset-size6 size3 sizing"><span class="mtight mbin">∗</span></span></span></span></span></span></span></span></span></span></span></td><td>SHA-1</td><td>1. Suffix Array: minimum 50-token window<br>MinHash: <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.25em;height:1em"></span><span class="mopen">(</span><span class="mord">9000</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">0.8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">20</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">450</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6887em"><span style="margin-right:.05em;top:-3.063em"><span class="pstrut" style="height:2.7em"></span><span class="mtight reset-size6 size3 sizing"><span class="mtight mbin">∗</span></span></span></span></span></span></span></span></span></span></span></td><td>Same as <strong>C4</strong></td><td>Same as <strong>C4</strong></td><td>Same as <strong>C4</strong></td></tr><tr><td>Language</td><td>English</td><td>English</td><td>Spanish</td><td>English</td><td>Multilingual</td><td>English</td><td>English</td><td>English</td><td>English</td></tr></tbody></table></div>
<p>* MinHash + LSH parameters <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="vertical-align:-.25em;height:1em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mclose">)</span></span></span></span>:</p>
<ol>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span></span></span></span> number of permutations/hashes</li>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.09618em">J</span></span></span></span> Jaccard similarity threshold</li>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span> n-gram size</li>
<li>* number of bands</li>
<li>* number of rows</li>
</ol></article></div><div class="sidebar right"><div class="desktop-only toc"><button type="button" id="toc" class=""><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#deduplication-in-modern-large-scale-lm-datasets" data-for="deduplication-in-modern-large-scale-lm-datasets">Deduplication in Modern Large-scale LM Datasets</a></li><li class="depth-1"><a href="#motivations-behind-deduplication" data-for="motivations-behind-deduplication">Motivations Behind Deduplication</a></li><li class="depth-1"><a href="#pipeline" data-for="pipeline">Pipeline</a></li><li class="depth-2"><a href="#preprocessing" data-for="preprocessing">Preprocessing</a></li><li class="depth-2"><a href="#exact-deduplication" data-for="exact-deduplication">Exact Deduplication</a></li><li class="depth-2"><a href="#exact-sub-string-deduplication" data-for="exact-sub-string-deduplication">Exact Sub-string Deduplication</a></li><li class="depth-2"><a href="#near-deduplication" data-for="near-deduplication">Near Deduplication</a></li><li class="depth-2"><a href="#semantic-deduplication" data-for="semantic-deduplication">Semantic Deduplication</a></li><li class="depth-2"><a href="#repetition-vs-deduplication" data-for="repetition-vs-deduplication">Repetition VS. Deduplication</a></li><li class="depth-2"><a href="#removal" data-for="removal">Removal</a></li><li class="depth-1"><a href="#applications" data-for="applications">Applications</a></li><li class="depth-2"><a href="#validation-or-test-leakage" data-for="validation-or-test-leakage">Validation or Test Leakage</a></li><li class="depth-1"><a href="#cost-and-implementation" data-for="cost-and-implementation">Cost and Implementation</a></li><li class="depth-2"><a href="#libraries" data-for="libraries">Libraries</a></li><li class="depth-1"><a href="#final-thoughts" data-for="final-thoughts">Final Thoughts</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="../../../4archives/BigScience/Goals" class="internal">Goals</a></li></ul></div></div></div><footer class=""><hr><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> © 2024</p><ul><li><a href="https://codeberg.org/Chenghao2023/blog">CodeBerg</a></li><li><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></li></ul></footer></div><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">let mermaidImport;document.addEventListener("nav",async()=>{if(document.querySelector("code.mermaid")){mermaidImport||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs");const e=mermaidImport.default,t=document.documentElement.getAttribute("saved-theme")==="dark";e.initialize({startOnLoad:!1,securityLevel:"loose",theme:t?"dark":"default"}),await e.run({querySelector:".mermaid"})}});
</script><script src="../../../postscript.js" type="module"></script></body></html>