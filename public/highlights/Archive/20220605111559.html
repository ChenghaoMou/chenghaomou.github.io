<!DOCTYPE html>
<html lang="en"><head><title>UniDoc Unified Pretraining Framework for Document Understanding</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=IBM Plex Sans:wght@400;700&amp;family=IBM Plex Sans:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="UniDoc Unified Pretraining Framework for Document Understanding"/><meta property="og:description" content="UniDoc Unified Pretraining Framework for Document Understanding (6/5/2022, 11:15:59 AM) “UDoc is designed to support most document understanding tasks, extending the Transformer ..."/><meta property="og:image" content="https://sleeplessindebugging.blog/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../../static/icon.png"/><meta name="description" content="UniDoc Unified Pretraining Framework for Document Understanding (6/5/2022, 11:15:59 AM) “UDoc is designed to support most document understanding tasks, extending the Transformer ..."/><meta name="generator" content="Quartz"/><link href="../../index.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="highlights/Archive/20220605111559"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../..">Sleepless in Debugging</a></h2><div class="spacer mobile-only"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../highlights/">highlights</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../highlights/Archive/">Archive</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>UniDoc Unified Pretraining Framework for Document Understanding</a></div></nav><h1 class="article-title">UniDoc Unified Pretraining Framework for Document Understanding</h1><div show-comma="true" class="content-meta"><p><span class="tag-field">UPDATED</span> <span class="tag-value">Apr 13, 2024</span>  <span class="tag-field">READING TIME</span> <span class="tag-value">8 min(s)</span></p></div></div></div><article class="popover-hint"><h1 id="unidoc-unified-pretraining-framework-for-document-understanding">UniDoc Unified Pretraining Framework for Document Understanding<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unidoc-unified-pretraining-framework-for-document-understanding" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>(6/5/2022, 11:15:59 AM)</p>
<p>“UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=FNEX9F4H" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=PXDBPDJZ" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“An important feature of UDoc is that it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=7ESCVDZZ" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=SEMMQBKW" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>(<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) No open source code available.</p>
<p>“(1) documents are composed of semantic regions.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=TJUM4U4T" class="external">pdf<span class="external-icon">⤴</span></a>) The semantic region is left undefined. The difference between OCR block and semantic region is also unexplanied.</p>
<p>“semantic regions” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=B6QXNMHZ" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“However, unlike the sequence-to-sequence learning in NLP, documents have a hierarchical structure (words form sentences, sentences form a semantic region, and semantic regions form a document).” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=JAY9KED3" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“documents have a hierarchical structure (words form sentences, sentences form a semantic region, and semantic regions form a document).” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=TUSP6D4Z" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“Moreover, current transformer-based document pretraining models suffer from input length constraints.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 1<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=1&amp;annotation=IUJVVGIV" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“(2) documents are more than words. The semantic structure of the document is not only determined by the text within it but also the visual features such as table, font size and style, and figure, etc.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=NYRN37CJ" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“(3) documents have spatial layout. Visual and layout information is critical for document understanding.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=MX54QLPT" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“However, for semi-structured documents, such as forms and receipts, words are more related to their local surroundings. This corresponds strongly with human intuition when we look at magazines or newspapers, the receptive fields are modulated by our reading order and attention.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=AUJ8PVLH" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“To handle textual information, we encode sentences using a hierarchical transformer encoder. The first level of the hierarchical encoder models the formation of the sentences from words. The second level models the formation of the document from sentences.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=TCR8W36N" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“To handle textual information, we encode sentences using a hierarchical transformer encoder.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=UTXBDQX5" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“The first level of the hierarchical encoder models the formation of the sentences from words. The second level models the formation of the document from sentences.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=J5LV6U73" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“Meanwhile, it reduces model computation complexity exponentially and increases the number of input words.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=WXLAERNG" class="external">pdf<span class="external-icon">⤴</span></a>) 512 * 512 = 262144 tokens for each document</p>
<p>“Meanwhile, it reduces model computation complexity exponentially and increases the number of input words.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=L9SV3D7V" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“we combine convolution with self-attention to form a mixed attention mechanism that combines the advantages of the two operations.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=FEV6A5IP" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“A visually-rich region (figure, chart, etc) may have stronger visual information than textual information. Instead of treating outputs from both modalities identically, we design a gating mechanism that can dynamically control the influence of textual and visual features. This approach enables cross-modal connections and allows for variable highlight the relevant information in visual and textual modality and enables cross-modal connections.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=QQYATLPI" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“Instead of treating outputs from both modalities identically, we design a gating mechanism that can dynamically control the influence of textual and visual features.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 2<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=2&amp;annotation=QXCWZCV6" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“UDoc, which consists of four components: feature extraction, feature embedding, multi-layer gated cross-attention encoder, and pretraining tasks.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 3<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=3&amp;annotation=QBZF74VW" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“In the feature extraction step, we first employ an off-the-shelf OCR tool [17] to extract text from a document image I, where the words are grouped into sentences S = {s1,…,sN } whose corresponding bounding boxes are P = {p1,…,pN }. For each sentence bounding box pi, we use a ConvNet-based backbone fImEnc and RoI Align [18] f” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=CEXI2Z76" class="external">pdf<span class="external-icon">⤴</span></a>) Sentencization in a document can be problamatic especially when the sections and headers do not follow the grammar rule of a sentence. OCR tools in my experience rarely share the same understanding of the textual information in the documents.</p>
<p>“For each sentence bounding box pi, we use a ConvNet-based backbone fImEnc and RoI Align [18] fRoIAlign to extract the pooled RoI features vi.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=TRMPJ27B" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“To obtain a feature embedding, we extract the sentence embedding si for each sentence si via a pretrained sentence encoder fSentEnc.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=Z8M7YV7L" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“Each region’s RoI feature vi is discretized into a finite set of visual representations vQ i 2 VQ via product quantization [19].” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=VCU557SK" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“Formally, a document image I 2 RW ⇥H consists of N regions, where each region’s bounding box is characterized by a 6-d vector, as pi = { xLT W , yLT H , xRB W , yRB H,w W, h H }, where w and h are of the width and height the region, W and H are the width and height of I, wh” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=2LUBWWP5" class="external">pdf<span class="external-icon">⤴</span></a>) I think they refer each sentence bounding box as region, but what if the sentence stretches two lines, or what about tables, TOC, lists? A rigorously definition of region should help here.</p>
<p>“We also have different types of segments to distinguish different modalities. The input sequence to the transformer-based encoder starts with a special start element ([CLS] and full visual features), then it is followed by multimodal elements, and it ends with a special ending element ([SEP]+full visual features).” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=6UZDVVU7" class="external">pdf<span class="external-icon">⤴</span></a>) visual features + [SEP] or [SEP] + visual features?</p>
<p>“Unlike the fixed image encoder in [6], we jointly learn the image encoder in an end-to-end fashion alongside the multimodal model.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=6EVWJDUY" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“To constrain the representation space of the visual features and facilitate the end-to-end learning of image encoder (see Task #2 in Sec. 3.2), we follow [20, 21] and use vector quantization to discretize the visual features V = {v1, … , vN } into a finite set of representations VQ = {vQ 1 , … , vQ N }.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 4<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=4&amp;annotation=YW4X9SRC" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“The goal is to predict the masked sentence embeddings based on the contextual information from the surrounding sentences and image regions, by minimizing the smooth L1 loss [16]:” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 5<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=5&amp;annotation=4H3QMC9T" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“The goal is to predict the masked sentence embeddings based on the contextual information from the surrounding sentences and image regions” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 5<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=5&amp;annotation=UZVY3DBC" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“random masking” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 5<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=5&amp;annotation=8WTQ5CY7" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“The goal is to minimize the differences” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 5<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=5&amp;annotation=HIU8QXM9" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“between the pairwise similarities of sentence embeddings and the pairwise similarities of image region features:” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 6<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=6&amp;annotation=E3EXHSX8" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“between the pairwise similarities of sentence embeddings and the pairwise similarities of image region features:” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 6<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=6&amp;annotation=I2UDHMG8" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“The paragraph mode groups the non-paragraph results into text regions.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 6<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=6&amp;annotation=YIFYWNHU" class="external">pdf<span class="external-icon">⤴</span></a>) This is different from what is previously defined.</p>
<p>“Hence, we adopt the paragraph-level outputs as the basic input elements since textual regions provide semantically more meaningful information than independent words.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 6<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=6&amp;annotation=46T9BM8I" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“We set the hidden size to 768 and the number of heads to 12, the maximum number of regions N to 64, and the maximum input sequence length for fSentEnc to 512.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 6<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=6&amp;annotation=679UY787" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“80% among the masked sentences are replaced by special sentence [CLS, MASK, SEP]” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 7<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=7&amp;annotation=4SY2D7MF" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“We find that OCR plays a key role in document classification performance.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 9<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=9&amp;annotation=FHFF5XFW" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“(2) Although impressive performance has been achieved in document entity recognition tasks such as form and receipt understanding, the classification accuracy on semi-structured documents such as forms is still inferior to that of rich-text documents. It is possible to devise a better method to model the spatial relationship among words.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 10<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=10&amp;annotation=3AAKR9K2" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<p>“Lastly, the use of different OCR tools is one of the major sources of inconsistency among the existing document pretraining works. It is worthwhile and essential to build standardized pretraining document image datasets with preprovided OCR results. In addition to scanned documents, using digital PDF as part of the pretraining data is a direction worth exploring since it provides rich metadata which could be beneficial for multimodal learning.” (<a href="zotero://select/library/items/YZ4B369T" class="external">Gu et al., 2021, p. 10<span class="external-icon">⤴</span></a>) (<a href="zotero://open-pdf/library/items/JM2W7NYK?page=10&amp;annotation=23JAQZ54" class="external">pdf<span class="external-icon">⤴</span></a>)</p>
<hr/>
<ul>
<li>@gu_2021b</li>
</ul></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><svg version="1.1" id="global-graph-icon" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
	s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
	c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
	C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
	c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
	v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
	s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
	C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
	S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
	s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
	s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.3.0</a> © 2024</p><ul><li><a href="https://codeberg.org/Chenghao2023/blog">CodeBerg</a></li><li><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script type="application/javascript">
        const socket = new WebSocket('ws://localhost:3001')
        // reload(true) ensures resources like images and scripts are fetched again in firefox
        socket.addEventListener('message', () => document.location.reload(true))
      </script><script src="../../postscript.js" type="module"></script></html>