<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><style>:where(img){height:auto}</style><title>Ruder et al_2022_Square One Bias in NLP</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=IBM Plex Sans Condensed:wght@400;700&amp;family=IBM Plex Sans Condensed:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"><meta name="viewport" content="width=device-width,initial-scale=1"><meta property="og:title" content="Ruder et al_2022_Square One Bias in NLP"><meta property="og:description" content="Page 1 Likewise, if the first 10 NLP experiments we see or conduct are in sentiment analysis, this will likely also bias how we think of NLP experiments in the future ..."><meta property="og:image" content="https://sleeplessindebugging.blog/static/og-image.png"><meta property="og:width" content="1200"><meta property="og:height" content="675"><link rel="icon" href="../../static/icon.png"><meta name="description" content="Page 1 Likewise, if the first 10 NLP experiments we see or conduct are in sentiment analysis, this will likely also bias how we think of NLP experiments in the future ..."><meta name="generator" content="Quartz"><link href="../../index.css" rel="stylesheet" type="text/css" spa-preserve=""><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve=""><script src="../../prescript.js" type="application/javascript" spa-preserve=""></script><script type="application/javascript" spa-preserve="">const fetchData=fetch("../../static/contentIndex.json").then(t=>t.json());
</script></head><body data-slug="highlights/Archive/Ruder-et-al_2022_Square-One-Bias-in-NLP"><div id="quartz-root" class="page"><div id="quartz-body"><div class="sidebar left"><h1 class="page-title"><a href="../..">Sleepless in Debugging</a></h1><div class="mobile-only spacer"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../highlights/">highlights</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../highlights/Archive/">Archive</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="">Ruder et al_2022_Square One Bias in NLP</a></div></nav><h1 class="article-title">Ruder et al_2022_Square One Bias in NLP</h1><p show-comma="true" class="content-meta"><span>Apr 13, 2024</span><span>7 min read</span></p></div></div><article class="popover-hint"><h4 id="page-1"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=1" class="external">Page 1<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>Likewise, if the first 10 NLP experiments we see or conduct are
in sentiment analysis, this will likely also bias how we think
of NLP experiments in the future.</p>
</blockquote>
<hr>
<blockquote>
<p>the existence of such an exper- imental prototype steers and
biases the research dynamics in our community.</p>
</blockquote>
<p>benchmark lottery</p>
<hr>
<h4 id="page-2"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=2" class="external">Page 2<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>We argue that the SQUARE ONE BIAS has sev- eral negative
effects, most of which amount to the study of one of the above
dimensions being biased by ignoring the others.</p>
</blockquote>
<hr>
<blockquote>
<p>multilinguality, fairness and bias, ef-</p>
</blockquote>
<hr>
<blockquote>
<p>ficiency, and interpretability.</p>
</blockquote>
<hr>
<blockquote>
<p>Overall, almost 70% of papers evaluate only on English, clearly
highlighting a lack of language diversity in NLP (Bender, 2011;
Joshi et al., 2020). Almost 40% of papers only evaluate using
accuracy and/or F1, foregoing metrics that may shed light on
other aspects of model behavior. 56.6% of pa- pers do not study
any of the four major dimensions that we investigated. We refer
to this standard ex- perimental setup—evaluating only on English
and optimizing for accuracy or another performance metric
without considering other dimensions—as the SQUARE ONE.</p>
</blockquote>
<hr>
<blockquote>
<p>Regarding work that moves from the SQUARE ONE, most papers make
a contribution in terms of efficiency, followed by
multilinguality. However, most papers that evaluate on multiple
languages are part of the corresponding MT and Multilinguality
track. Despite being an area receiving increasing at- tention
(Blodgett et al., 2020), only 6.3% of papers evaluate the bias
or fairness of a method. Overall, only 6.1% of papers make a
contribution along two or more of these dimensions.</p>
</blockquote>
<hr>
<h4 id="page-4"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=4" class="external">Page 4<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-4" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>Architectural Biases. One pervasive bias in our models regards
morphology. Many of our mod- els were not designed with
morphology in mind, arguably because of the poor/limited
morphology of English.</p>
</blockquote>
<hr>
<blockquote>
<p>However, word embeddings are not useful for tasks that require
access to mor- phemes, e.g., semantic tasks in morphologically
rich languages (Avraham and Goldberg, 2017).</p>
</blockquote>
<hr>
<blockquote>
<p>it remains unclear whether they capture the information needed
for processing morphologi- cally rich languages (Tsarfaty et
al., 2020).</p>
</blockquote>
<hr>
<blockquote>
<p>Subword tokenization performs poorly on languages with
reduplication (Vania and Lopez, 2017), while byte pair encoding
does not align well with morphol- ogy (Bostrom and Durrett,
2020). Consequently, languages with productive morphological
systems also are disadvantaged when shared ‘language- universal’
tokenizers are used in current large-scale multilingual language
models (Ács, 2019; Rust et al., 2021) without any further
vocabulary adapta- tion (Wang et al., 2020; Pfeiffer et al.,
2021).</p>
</blockquote>
<hr>
<blockquote>
<p>While the recent generation of self-attention based
architectures can be seen as inherently order-agnostic, recent
methods focus- ing on making attention more efficient (Tay et
al., 2020) introduce new biases into the models. Specif- ically,
models that reduce the global attention to a local sliding
window around the token (Liu et al., 2018; Child et al., 2019;
Zaheer et al., 2020) may incur similar limitations as their
n-gram and word embedding-based predecessors, performing worse
on languages with relatively free word order.6</p>
</blockquote>
<hr>
<blockquote>
<p>Studies proposing more interpretable methods typically build on
state-of-the-art meth- ods (Weiss et al., 2018) and much work
focuses on leveraging components such as attention for in-
terpretability, which have not been designed with that goal in
mind (Serrano and Smith, 2019; Wiegr- effe and Pinter, 2019).</p>
</blockquote>
<hr>
<h4 id="page-5"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=5" class="external">Page 5<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-5" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>Similarly, the standard pretrain- fine-tune paradigm (Ruder et
al., 2019) requires separate model copies to be stored for each
task, and thus restricts work on multi-domain, multi- task,
multi-lingual, multi-subpopulation methods that is enabled by
more efficient and less resource- intensive (Schwartz et al.,
2020) fine-tuning meth- ods (Houlsby et al., 2019; Pfeiffer et
al., 2020)</p>
</blockquote>
<hr>
<blockquote>
<p>In sum, (what we typically consider as) standard baselines and
state-of-the-art architectures favor languages with some
characteristics over others and are optimized only for
performance, which in turn propagates the SQUARE ONE BIAS: If
researchers study aspects such as multilinguality, efficiency,
fairness or interpretability, they are likely to do so with and
for commonly used architectures (i.e., often termed ‘standard
architectures’), in order to reduce (too) many degrees of
freedom in their em- pirical research.</p>
</blockquote>
<hr>
<blockquote>
<p>n interpretability, we can use feature attribution methods and
word- level annotations to evaluate interpretability meth- ods
applied to sequence classifiers (Rei and Sø- gaard, 2018), but
we cannot directly use feature at- tribution methods to obtain
rationales for sequence labelers.</p>
</blockquote>
<hr>
<blockquote>
<p>Put sim- ply, the choice of the data creation protocol, e.g.,
translation-based versus data collection directly in the target
language (Clark et al., 2020) can yield profound differences in
model performance for some groups, or may have serious impact on
the interpretability or computational efficiency (e.g., sample
efficiency) of our models.</p>
</blockquote>
<hr>
<h4 id="page-6"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=6" class="external">Page 6<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-6" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>Note how such a bias may interact in non-linear ways with
efficiency, i.e., efficient meth- ods for shorter documents need
not be efficient for longer ones, or fairness, i.e., what
mitigates gender biases in news articles need not mitigate
gender biases in product reviews.</p>
</blockquote>
<hr>
<blockquote>
<p>If our go-to architectures, resources, and experimental setups
are tailored to some languages over others, some objectives over
others, and some research paradigms over others, it is
considerably more work to explore new sets of languages, new
objectives, or new protocols.</p>
</blockquote>
<hr>
<blockquote>
<p>Character-based language models are often reported to perform
well for mor- phologically rich languages or on non-canonical
text (Ma et al., 2020), but little is known about their fairness
properties, and attribution-based in- terpretability methods
have not been developed for such models.</p>
</blockquote>
<hr>
<h4 id="page-7"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=7" class="external">Page 7<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-7" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>While recent work has begun to study the trade-off between
efficiency and fairness, this interaction remains largely
unexplored, especially outside of the empirical risk
minimization regime; (ii) fair- ness and interpretability
interact in potentially many ways, i.e., interpretability
techniques may af- fect the fairness of the underlying models
(Agarwal, 2021), but rationales may also, for example, be bi-
ased toward certain demographics in how they are presented (Feng
and Boyd-Graber, 2018; González et al., 2021); (iii) finally,
multilinguality and in- terpretability seem heavily
underexplored. While there exists resources for English for
evaluating in- terpretability methods against gold-standard
human annotations, there are, to the best of our knowledge, no
such resources for other languages.11</p>
</blockquote>
<hr>
<h4 id="page-8"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=8" class="external">Page 8<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-8" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>Always returning to the SQUARE ONE is a way to control for all
other factors and relating new findings to known territory. The
reason why this is only seemingly a good idea, however, is that
the factors we study in NLP research, may be non- linearly
related. The fact that t makes for a positive net contribution
under one set of circumstances, does not imply that it would do
so under different circumstances. This is illustrated most
clearly by the research surveyed in §3. Ideally, we thus want to
study the impact of t under as many circum- stances as possible,
but in the absence of resources to do so, it is a better
(collective) search strategy to apply t to a random set of
circumstances (within the space of relevant circumstances, of
course).</p>
</blockquote>
<hr>
<h4 id="page-9"><a href="highlights://Ruder%20et%20al_2022_Square%20One%20Bias%20in%20NLP#page=9" class="external">Page 9<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#page-9" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<blockquote>
<p>i) Currently, most NLP models are eval- uated by one or two
performance metrics, but we believe dimensions such as fairness,
efficiency, and interpretability need to become integral
criteria for model evaluation, in line with recent proposals of
more user-centric leaderboards (Ethayarajh and Ju- rafsky, 2020;
Ma et al., 2021). This requires new tools, e.g., to evaluate
environmental impact (Hen- derson et al., 2020), as well as new
benchmarks, e.g., to evaluate fairness (Koh et al., 2021) or ef-
ficiency (Liu et al., 2021b).</p>
</blockquote>
<hr>
<blockquote>
<p>ii) We believe sepa- rate conference tracks (areas) lead to
unfortunate silo effects and inhibit multi-dimensional research.
Rather, we imagine conference submissions could provide a
checklist with dimensions along which they make contributions,
similar to reproducibil- ity checklist. Reviewers can be
assigned based on their expertise corresponding to different
dimen- sions.</p>
</blockquote>
<hr>
<blockquote>
<p>iii) Finally, we recommend awareness of research prototypes and
encourage reviewers and chairs to prioritize research that
departs from pro- totypes in multiple dimensions, in order to
explore new areas of the research manifold.</p>
</blockquote>
<hr></article></div><div class="sidebar right"><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer class=""><hr><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> © 2024</p><ul><li><a href="https://codeberg.org/Chenghao2023/blog">CodeBerg</a></li><li><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></li></ul></footer></div><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">let mermaidImport;document.addEventListener("nav",async()=>{if(document.querySelector("code.mermaid")){mermaidImport||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs");const e=mermaidImport.default,t=document.documentElement.getAttribute("saved-theme")==="dark";e.initialize({startOnLoad:!1,securityLevel:"loose",theme:t?"dark":"default"}),await e.run({querySelector:".mermaid"})}});
</script><script src="../../postscript.js" type="module"></script></body></html>