title,chapter,date,url,chapter_url,annotation_url,highlight,note
"Natural Language Processing with Transformers, Revised Edition",11. Future Directions,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html#be2564fc-cd80-4d63-961d-cce4678708a6,"Modality
Language models have no way to connect to other modalities that could address the previous points, such as audio or visual signals or tabular data.",
"Natural Language Processing with Transformers, Revised Edition",11. Future Directions,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html#80ad90a1-fba4-4c7e-8912-77edf4867be0,"Facts
A probabilistic language model cannot store facts in a reliable way and can produce text that is factually wrong. Similarly, such models can detect named entities, but have no direct way to access information about them.",
"Natural Language Processing with Transformers, Revised Edition",11. Future Directions,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html#d18788ca-d3b7-4bec-b9a3-a0e070ce34fc,"Common sense
Common sense is a fundamental quality of human reasoning, but is rarely written down. As such, language models trained on text might know many facts about the world, but lack basic common-sense reasoning.",
"Natural Language Processing with Transformers, Revised Edition",11. Future Directions,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html,https://learning.oreilly.com/library/view/-/9781098136789/ch11.html#19e1a225-6683-4d03-b998-b87e647256c2,"Human reporting bias
The frequencies of events in text may not represent their true frequencies.9 A model solely trained on text from the internet might have a very distorted image of the world.",
"Natural Language Processing with Transformers, Revised Edition",10. Training Transformers from Scratch,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html#e275e7d6-e1a2-416c-a062-fc779ce8d49c,"Note that we can compute the perplexity by exponentiating the cross-entropy loss which we get from the model’s output. Especially at the start of training when the loss is still high, it is possible to get a numerical overflow when calculating the perplexity. We catch this error and set the perplexity to infinity in these instances.",
"Natural Language Processing with Transformers, Revised Edition",10. Training Transformers from Scratch,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html#5b981420-b1b3-41c5-9ea9-3d454ac273d4,Coverage metrics like the proportion of unknown words or rarely used tokens in a tokenized corpus,
"Natural Language Processing with Transformers, Revised Edition",10. Training Transformers from Scratch,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html#7bc89d8c-74be-4326-99ef-5a2f46680e5f,"Proportion of continued words, which refers to the proportion of tokenized words in a corpus that are split into at least two subtokens",
"Natural Language Processing with Transformers, Revised Edition",10. Training Transformers from Scratch,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html#a688f830-776b-4c2f-b143-129d3333bfe3,"Subword fertility, which calculates the average number of subwords produced per tokenized word",
"Natural Language Processing with Transformers, Revised Edition",10. Training Transformers from Scratch,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html#1455746d-2120-47fd-860f-5aad671988e1,"Unigram starts from the other end, by initializing its base vocabulary with all the words in the corpus, and potential subwords. Then it progressively removes or splits the less useful tokens to obtain a smaller and smaller vocabulary, until the target vocabulary size is reached. WordPiece is a predecessor of Unigram, and its official implementation was never open-sourced by Google.",
"Natural Language Processing with Transformers, Revised Edition",10. Training Transformers from Scratch,2022-07-31,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html#627c4f9b-f446-4ab3-8a39-5c3c092ebd8b,BPE starts from a list of basic units (single characters) and creates a vocabulary by a process of progressively creating new tokens formed by merging the most frequently co-occurring basic units and adding them to the vocabulary. This process is reiterated until a predefined vocabulary size is reached.,
"Natural Language Processing with Transformers, Revised Edition",10. Training Transformers from Scratch,2022-07-30,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html,https://learning.oreilly.com/library/view/-/9781098136789/ch10.html#95e0b3ab-8e94-4f23-b63b-b6ad8ffc4116,The decision to train from scratch rather than fine-tune an existing model is mostly dictated by the size of your fine-tuning corpus and the domain differences between the available pretrained models and the corpus.,
"Natural Language Processing with Transformers, Revised Edition",9. Dealing with Few to No Labels,2022-07-27,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html#4a8cfe36-cd87-4bbd-9a8c-5bfcb16b1710,"Another promising method to leverage unlabeled data is uncertainty-aware self-training (UST). The idea here is to train a teacher model on the labeled data and then use that model to create pseudo-labels on the unlabeled data. Then a student is trained on the pseudo-labeled data, and after training it becomes the teacher for the next iteration.",
"Natural Language Processing with Transformers, Revised Edition",9. Dealing with Few to No Labels,2022-07-27,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html#8a3b6e89-0a21-442c-85d5-e2b14b1e8b52,"The performance of this approach is quite impressive: with a handful of labeled examples, BERT models trained with UDA get similar performance to models trained on thousands of examples. The downside is that you need a data augmentation pipeline, and training takes much longer since you need multiple forward passes to generate the predicted distributions on the unlabeled and augmented examples.",
"Natural Language Processing with Transformers, Revised Edition",9. Dealing with Few to No Labels,2022-07-27,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html#c4eee892-86e3-4b23-ba56-331eeb74ba09,The key idea behind unsupervised data augmentation (UDA) is that a model’s predictions should be consistent for an unlabeled example and a slightly distorted one. Such distortions are introduced with standard data augmentation strategies such as token replacement and back translation. Consistency is then enforced by minimizing the KL divergence between the predictions of the original and distorted examples.,
"Natural Language Processing with Transformers, Revised Edition",9. Dealing with Few to No Labels,2022-07-27,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html#539ca442-299f-4487-8cb3-285e805f0bb3,"Back translation
Take a text in the source language, translate it into one or more target languages using machine translation, and then translate it back to the source language. Back translation tends to works best for high-resource languages or corpora that don’t contain too many domain-specific words.

Token perturbations
Given a text from the training set, randomly choose and perform simple transformations like random synonym replacement, word insertion, swap, or deletion.⁠4",
"Natural Language Processing with Transformers, Revised Edition",9. Dealing with Few to No Labels,2022-07-25,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html#209003c8-9f2a-4d7e-9861-45c15b29330e,"Another thing you can improve is the form of the hypothesis. By default it is hypothesis=""This is example is about {}"", but you can pass any other text to the pipeline. Depending on the use case, this might improve the performance.",
"Natural Language Processing with Transformers, Revised Edition",9. Dealing with Few to No Labels,2022-07-25,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html,https://learning.oreilly.com/library/view/-/9781098136789/ch09.html#2d35ff19-7cac-4612-919e-939098f5e8d2,"The way the pipeline works makes it very sensitive to the names of the labels. If the names don’t make much sense or are not easily connected to the texts, the pipeline will likely perform poorly. Either try using different names or use several names in parallel and aggregate them in an extra step.",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-24,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#8a350acb-1286-4ef5-bf09-fc6847cc2093,"Although pruning is an effective strategy for reducing the storage size of transformer models, current hardware is not optimized for sparse matrix operations, which limits the usefulness of this technique.",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-24,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#56f97df1-726b-41d6-8f00-4a60c2a319f7,"One problem with magnitude pruning is that it is really designed for pure supervised learning, where the importance of each weight is directly related to the task at hand. By contrast, in transfer learning the importance of the weights is primarily determined by the pretraining phase, so magnitude pruning can remove connections that are important for the fine-tuning task.",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-24,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#5308466a-a912-4fb5-8b81-bea36d69d15e,"at the heart of each pruning method are a set of questions that need to be considered:

Which weights should be eliminated?

How should the remaining weights be adjusted for best performance?

How can such network pruning be done in a computationally efficient way?",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-24,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#fc1dbced-4a06-4a67-a595-81a0d07c2c29,"To use this function, we first need to set some OpenMP environment variables for ONNX:

import os
from psutil import cpu_count

os.environ[""OMP_NUM_THREADS""] = f""{cpu_count()}""
os.environ[""OMP_WAIT_POLICY""] = ""ACTIVE""
OpenMP is an API designed for developing highly parallelized applications. The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel computations in the ONNX Runtime, while OMP_WAIT_POLICY=ACTIVE specifies that waiting threads should be active (i.e., using CPU processor cycles).",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-24,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#d20a5665-e352-4c8d-abd3-ceddb5240363,"Quantization-aware training
The effect of quantization can be effectively simulated during training by “fake” quantization of the FP32 values. Instead of using INT8 values during training, the FP32 values are rounded to mimic the effect of quantization. This is done during both the forward and the backward pass and improves performance in terms of model metrics over static and dynamic quantization.",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-24,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#98032904-6f55-49f5-bdbb-c667b9b6dd1b,"Static quantization
Instead of computing the quantization of the activations on the fly, we can avoid the conversion to floating point by precomputing the quantization scheme. Static quantization achieves this by observing the activation patterns on a representative sample of the data ahead of inference time. The ideal quantization scheme is calculated and then saved. This enables us to skip the conversion between INT8 and FP32 values and speeds up the computations. However, it requires access to a good data sample and introduces an additional step in the pipeline, since we now need to train and determine the quantization scheme before we can perform inference. There is also one aspect that static quantization does not address: the discrepancy between the precision during training and inference, which leads to a performance drop in the model’s metrics (e.g., accuracy).",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-24,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#f964083f-87a0-4ac1-a2d3-a20deb61f3a4,"Dynamic quantization
When using dynamic quantization nothing is changed during training and the adaptations are only performed during inference. Like with all the quantization methods we will discuss, the weights of the model are converted to INT8 ahead of inference time. In addition to the weights, the model’s activations are also quantized. This approach is dynamic because the quantization happens on the fly. This means that all the matrix multiplications can be calculated with highly optimized INT8 functions. Of all the quantization methods discussed here, dynamic quantization is the simplest one. However, with dynamic quantization the activations are written and read to memory in floating-point format. This conversion between integer and floating point can be a performance bottleneck.",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-19,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#043a3135-b529-47eb-b9d1-d6ba78eb26ae,"Changing the precision for all computations in our model introduces small disturbances at each point in the model’s computational graph, which can compound and affect the model’s performance.",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-19,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#cf3819e0-c751-4555-baf7-d52715229301,"The basic idea behind quantization is that we can “discretize” the floating-point values f in each tensor by mapping their range [
f
max
,
f
min
] into a smaller one [
q
max
,
q
min
] of fixed-point numbers 
q
, and linearly distributing all values in between",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-19,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#f576fbbe-537d-41f1-9357-b12df730c0e3,"When 
T
=
1
 we recover the original softmax distribution.",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-19,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#4ff55f78-0cbc-483a-b379-96a2b0fba1c3,"This isn’t quite what we want, though, because in many cases the teacher will assign a high probability to one class, with all other class probabilities close to zero. When that happens, the teacher doesn’t provide much additional information beyond the ground truth labels, so instead we “soften” the probabilities by scaling the logits with a temperature hyperparameter T before applying the softmax:7",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-19,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#0d35e192-cc5a-4927-a302-9c166d0442e0,"1024)
    # Delete temporary file
    tmp_path.unlink()",
"Natural Language Processing with Transformers, Revised Edition",8. Making Transformers Efficient in Production,2022-07-19,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html,https://learning.oreilly.com/library/view/-/9781098136789/ch08.html#bd04bd19-f352-49d0-a962-358b5cc70f28,"1024)
    # Delete temporary file
    tmp_path.unlink()",
"Natural Language Processing with Transformers, Revised Edition",7. Question Answering,2022-07-17,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html#e0f56ba1-a8cb-4fc7-a56a-3f738bb02bfd,"One reason for the performance drop is that customer reviews are quite different from the Wikipedia articles the SQuAD 2.0 dataset is generated from, and the language they use is often informal. Another factor is likely the inherent subjectivity of our dataset, where both questions and answers differ from the factual information contained in Wikipedia.",
"Natural Language Processing with Transformers, Revised Edition",7. Question Answering,2022-07-17,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html#6db395e6-13e7-4ed0-8962-63ff03a402c7,"Relying on just the F1-score is thus misleading, and tracking both metrics is a good strategy to balance the trade-off between underestimating (EM) and overestimating (F1-score) model performance.",
"Natural Language Processing with Transformers, Revised Edition",7. Question Answering,2022-07-17,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html#98fb769a-ca12-4c12-a654-56f6125b6ac2,"Under the hood, these functions first normalize the prediction and label by removing punctuation, fixing whitespace, and converting to lowercase. The normalized strings are then tokenized as a bag-of-words, before finally computing the metric at the token level. From this simple example we can see that EM is a much stricter metric than the F1-score: adding a single token to the prediction gives an EM of zero. On the other hand, the F1-score can fail to catch truly incorrect answers.",
"Natural Language Processing with Transformers, Revised Edition",7. Question Answering,2022-07-14,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html#599f5f43-1a79-4748-ad07-eeb3025480ec,"The TransformersReader sometimes predicts the same answer twice, but with different scores. This can happen in long contexts if the answer lies across two overlapping windows. In FARM, these duplicates are removed.",
"Natural Language Processing with Transformers, Revised Edition",7. Question Answering,2022-07-14,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html#7e639322-91b9-4958-bdd4-1ca1f22302f7,"In  Transformers, the QA pipeline normalizes the start and end logits with a softmax in each passage. This means that it is only meaningful to compare answer scores between answers extracted from the same passage, where the probabilities sum to 1. For example, an answer score of 0.9 from one passage is not necessarily better than a score of 0.8 in another. In FARM, the logits are not normalized, so inter-passage answers can be compared more easily.",
"Natural Language Processing with Transformers, Revised Edition",7. Question Answering,2022-07-14,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html#821effe1-e70e-4f91-a4fd-89f9056c4a8a,"However, this heuristic can produce out-of-scope answers by selecting tokens that belong to the question instead of the context. In practice, the pipeline computes the best combination of start and end indices subject to various constraints such as being in-scope, requiring the start indices to precede the end indices, and so on.",
"Natural Language Processing with Transformers, Revised Edition",7. Question Answering,2022-07-12,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html,https://learning.oreilly.com/library/view/-/9781098136789/ch07.html#835b4dff-5cce-4cb4-a97e-aea5eac9fd0e,"The two-stage process of first retrieving relevant documents and then extracting answers from them is also the basis for many modern QA systems, including semantic search engines, intelligent assistants, and automated information extractors.",
"Natural Language Processing with Transformers, Revised Edition",6. Summarization,2022-07-09,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch06.html,https://learning.oreilly.com/library/view/-/9781098136789/ch06.html#ed6186a1-8165-4762-bb03-2824b17a0699,"A common question when working with summarization models is how we can summarize documents where the texts are longer than the model’s context length. Unfortunately, there is no single strategy to solve this problem, and to date this is still an open and active research question. For example, recent work by OpenAI showed how to scale summarization by applying it recursively to long documents and using human feedback in the loop.6",
"Natural Language Processing with Transformers, Revised Edition",6. Summarization,2022-07-09,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch06.html,https://learning.oreilly.com/library/view/-/9781098136789/ch06.html#9fe66041-0f59-4db9-91b7-bd3560bc2912,"Obviously there could be important information for the summary toward the end of the text, but for now we need to live with this limitation of the model architectures.",
"Natural Language Processing with Transformers, Revised Edition",4. Multilingual Named Entity Recognition,2022-07-06,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch04.html,https://learning.oreilly.com/library/view/-/9781098136789/ch04.html#5a4e85ff-80b7-4e85-b2ed-2e04cb82e9c2,"In general, the size of the performance drop is related to how “far away” the languages are from each other. Although German and French are grouped as Indo-European languages, they technically belong to different language families: Germanic and Romance, respectively.",
"Natural Language Processing with Transformers, Revised Edition",4. Multilingual Named Entity Recognition,2022-07-06,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch04.html,https://learning.oreilly.com/library/view/-/9781098136789/ch04.html#3c9f3841-e6ba-4926-85b4-f4f31bef71d9,"In the BERT paper,5 the authors assigned this label to the first subword (“Chr” in our example) and ignored the following subword (“##ista”). This is the convention we’ll adopt here, and we’ll indicate the ignored subwords with IGN. We can later easily propagate the predicted label of the first subword to the subsequent subwords in the postprocessing step.",
"Natural Language Processing with Transformers, Revised Edition",3. Transformer Anatomy,2022-07-05,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html#29ad594c-1ecd-4019-b8e8-f64f7c5a89f1,"While learnable position embeddings are easy to implement and widely used, there are some alternatives:

Absolute positional representations
Transformer models can use static patterns consisting of modulated sine and cosine signals to encode the positions of the tokens. This works especially well when there are not large volumes of data available.

Relative positional representations
Although absolute positions are important, one can argue that when computing an embedding, the surrounding tokens are most important. Relative positional representations follow that intuition and encode the relative positions between tokens. This cannot be set up by just introducing a new relative embedding layer at the beginning, since the relative embedding changes for each token depending on where from the sequence we are attending to it. Instead, the attention mechanism itself is modified with additional terms that take the relative position between tokens into account. Models such as DeBERTa use such representations.5",
"Natural Language Processing with Transformers, Revised Edition",3. Transformer Anatomy,2022-07-05,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html#b16c7ab3-fb43-4327-974a-934e5ef49f34,"When it comes to placing the layer normalization in the encoder or decoder layers of a transformer, there are two main choices adopted in the literature:

Post layer normalization
This is the arrangement used in the Transformer paper; it places layer normalization in between the skip connections. This arrangement is tricky to train from scratch as the gradients can diverge. For this reason, you will often see a concept known as learning rate warm-up, where the learning rate is gradually increased from a small value to some maximum value during training.

Pre layer normalization
This is the most common arrangement found in the literature; it places layer normalization within the span of the skip connections. This tends to be much more stable during training, and it does not usually require any learning rate warm-up.",
"Natural Language Processing with Transformers, Revised Edition",3. Transformer Anatomy,2022-07-05,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html#2b34a18d-9ecf-4fe0-83e5-9407f53f6e31,"A rule of thumb from the literature is for the hidden size of the first layer to be four times the size of the embeddings, and a GELU activation function is most commonly used. This is where most of the capacity and memorization is hypothesized to happen, and it’s the part that is most often scaled when scaling up the models.",
"Natural Language Processing with Transformers, Revised Edition",3. Transformer Anatomy,2022-07-03,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html#2472a4e2-336d-4c0f-bcd0-bcb8ccd52086,"But why do we need more than one attention head? The reason is that the softmax of one head tends to focus on mostly one aspect of similarity. Having several heads allows the model to focus on several aspects at once. For instance, one head can focus on subject-verb interaction, whereas another finds nearby adjectives.",
"Natural Language Processing with Transformers, Revised Edition",3. Transformer Anatomy,2022-07-03,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html,https://learning.oreilly.com/library/view/-/9781098136789/ch03.html#42c85ca9-d196-4255-b878-a2788e970a17,"In scaled dot-product attention, the dot products are scaled by the size of the embedding vectors so that we don’t get too many large numbers during training that can cause the softmax we will apply next to saturate.",
"Natural Language Processing with Transformers, Revised Edition",2. Text Classification,2022-07-03,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html#d2863706-39d2-4bf1-9693-fb8c18bea980,"For this reason, it is also worth investing time into looking at the examples that the model is most confident about, so that we can be confident that the model does not improperly exploit certain features of the text.",
"Natural Language Processing with Transformers, Revised Edition",2. Text Classification,2022-07-03,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html#9f2481e9-bafa-40c0-8dba-c085856ae0c3,"Thus, it is normal that there are some wrongly labeled examples. With this approach, we can quickly find and correct them.",
"Natural Language Processing with Transformers, Revised Edition",2. Text Classification,2022-07-03,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html#096cfe1e-bd70-481e-a9f4-59e9e834d75f,"Inspecting the model’s weakest predictions can help identify such features, and cleaning the data or injecting similar examples can make the model more robust.",
"Natural Language Processing with Transformers, Revised Edition",2. Text Classification,2022-07-03,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html#5d87c98d-8dca-491e-ac9b-3d25cf29ea57,A simple yet powerful technique is to sort the validation samples by the model loss.,
"Natural Language Processing with Transformers, Revised Edition",2. Text Classification,2022-07-03,https://learning.oreilly.com/library/view/-/9781098136789/,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html,https://learning.oreilly.com/library/view/-/9781098136789/ch02.html#15ad56d5-cde6-4150-b7e6-8b9c3ac4b9e9,"There are several ways to deal with imbalanced data, including:

Randomly oversample the minority class.

Randomly undersample the majority class.

Gather more labeled data from the underrepresented classes.",
