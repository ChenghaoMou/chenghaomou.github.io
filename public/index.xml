<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
    <channel>
      <title>Sleepless in Debugging</title>
      <link>https://sleepless-in-debugging.statichost.eu</link>
      <description>Last 10 notes on Sleepless in Debugging</description>
      <generator>Quartz -- quartz.jzhao.xyz</generator>
      <item>
    <title>202205231928 Cost Equivalent Curve</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205231928-Cost-Equivalent-Curve</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205231928-Cost-Equivalent-Curve</guid>
    <description>Definitions Cost: the number of training examples in the training set used. Performance: the performance given a model and a cost according to the task metric.</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205231931 Transfer Learning Approaches</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205231931-Transfer-Learning-Approaches</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205231931-Transfer-Learning-Approaches</guid>
    <description>Multitask Training: Train the model on all tasks/datasets all at once; Sequential Multitask Training: Train the model on some tasks/datasets first before continue training it with the target tasks/datasets; Multitask Training and Fine-tuning: Train the model on some tasks first and fine-tuning it with target tasks/datasets; Findings from @lourie_2021 that ^3f3b27 almost always outperform other methods.</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205241111 Code Review Pyramid</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241111-Code-Review-Pyramid</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241111-Code-Review-Pyramid</guid>
    <description> Style and most tests should be automated (in the commit hooks, CI tools or in a bash script) so you don’t have to write a PR comment about this. One should have a trace of written records regarding design decisions and implementation choices, ranging from technical specifications to your API documentations.</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205241112 Deep Learning First Principles for Efficiency</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241112-Deep-Learning-First-Principles-for-Efficiency</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241112-Deep-Learning-First-Principles-for-Efficiency</guid>
    <description>Three Areas of Optimization 1. Compute Compute refers to the arithmetic operations done by CPU or GPU, typically measured in floating point operations per second (FLOPS).</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205241113 Hotelling&#039;s Two-sample T-squared Test</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241113-Hotelling's-Two-sample-T-squared-Test</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241113-Hotelling's-Two-sample-T-squared-Test</guid>
    <description>Multivariate (multiple features) two-sample test. Assumptions Both samples are independent; Both samples are multivariate normal; Both samples have equal variance-covariance matrices; H0​: Samples drawn from populations with the same multivariate mean.</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205241114 How to Remember Names</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241114-How-to-Remember-Names</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241114-How-to-Remember-Names</guid>
    <description>Takeaways: Initially, a name and a face are not associated until your hippocampus puts them together into a single memory; Instead of focusing on introducing yourself, pay more attention to learning them.</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205241115 How to Work Hard</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241115-How-to-Work-Hard</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241115-How-to-Work-Hard</guid>
    <description>Why Work Hard? We can not change our talents, so work hard is, in most cases, the only choice. How to Work Hard? Define your goals and foster disciplines Two mindsets: Enjoy achievement Dislike idleness Learn what work is What matches your talent might not be your interest Know what is important and interesting Plan ahead Avoid diminishing returns and burnout Work towards the hard-core center problems .</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205241116 Kernel Two-sample Test</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241116-Kernel-Two-sample-Test</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205241116-Kernel-Two-sample-Test</guid>
    <description>Maximum Mean Discrepancy (MMD) Multivariate two-sample test. Formula MMD(P,Q)=n21​i=1∑n​j=1∑n​k(xi​,xj​)+m21​i=1∑m​j=1∑m​k(yi​,yj​)−nm2​i=1∑n​j=1∑m​k(xi​,yj​) k refers to the kernel function.</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>202205281322 Four Things That Make Things Memorable</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205281322-Four-Things-That-Make-Things-Memorable</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Permanent-Notes/202205281322-Four-Things-That-Make-Things-Memorable</guid>
    <description> Novelty: something new Repetition: reinforcement Association: connections Emotional Resonance .</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item><item>
    <title>01 Intro</title>
    <link>https://sleepless-in-debugging.statichost.eu/4archives/Privacy-AI/01-Intro</link>
    <guid>https://sleepless-in-debugging.statichost.eu/4archives/Privacy-AI/01-Intro</guid>
    <description>Information Flow: a flow of bits from sender to receiver with some probability. Privacy VS. Transparency Leaky information flow: aka. privacy violation.</description>
    <pubDate>Mon, 04 Mar 2024 14:18:16 GMT</pubDate>
  </item>
    </channel>
  </rss>